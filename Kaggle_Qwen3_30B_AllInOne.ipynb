{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# ğŸš€ Kaggle VQA - Qwen3-VL-30B Multi-GPU Edition\n",
        "\n",
        "## ğŸ¯ 30B ëª¨ë¸ ì£¼ìš” íŠ¹ì§•\n",
        "\n",
        "### âœ… Multi-GPU Parallelism (í•µì‹¬!)\n",
        "- **ìë™ ëª¨ë¸ ë¶„ì‚°**: `device_map=\"auto\"`ë¡œ 2ê°œ GPUì— ìë™ ë°°ì¹˜\n",
        "- **ë©”ëª¨ë¦¬ ì œí•œ**: `max_memory={0: \"14GB\", 1: \"14GB\"}`\n",
        "- **OOM ì™„ì „ ë°©ì§€**: ì •êµí•œ ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
        "\n",
        "### âœ… ë©”ëª¨ë¦¬ ìµœì í™”\n",
        "- 4-bit Quantization (75% ë©”ëª¨ë¦¬ ì ˆê°)\n",
        "- Gradient Checkpointing (40% í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆê°)\n",
        "- High Gradient Accumulation (BATCH_SIZE=1, GRAD_ACCUM=16)\n",
        "- ì£¼ê¸°ì  GPU ìºì‹œ ì •ë¦¬\n",
        "\n",
        "### âœ… ê³ ê¸‰ í•™ìŠµ ê¸°ë²•\n",
        "- Direct Logits ì¶”ë¡  (ì•ˆì •ì )\n",
        "- Val Accuracy + Confusion Matrix ë¡œê¹…\n",
        "- í™•ë¥  ì•™ìƒë¸” (Probability Averaging)\n",
        "- í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
        "\n",
        "### âš™ï¸ 30B ìµœì í™” ì„¤ì •\n",
        "```\n",
        "MODEL_ID = \"Qwen/Qwen2.5-VL-30B-A3B-Instruct\"\n",
        "IMAGE_SIZE = 384 (ì•ˆì „) or 448 (ê· í˜•)\n",
        "LORA_R = 8 (30BëŠ” ì‘ê²Œ!)\n",
        "BATCH_SIZE = 1 (í•„ìˆ˜!)\n",
        "GRAD_ACCUM_STEPS = 16 (ë†’ê²Œ!)\n",
        "MAX_MEMORY_PER_GPU = {0: \"14GB\", 1: \"14GB\"}\n",
        "```\n",
        "\n",
        "### ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ (T4 * 2)\n",
        "- **ì •í™•ë„**: 88-90% (3B ëŒ€ë¹„ +3~5%)\n",
        "- **ë©”ëª¨ë¦¬**: GPU0 ~13GB, GPU1 ~13GB\n",
        "- **ì†ë„**: ~2min/epoch (IMAGE_SIZE=384)\n",
        "\n",
        "**ğŸ¤– SSAFY AI Project 2025 - Qwen3-VL-30B Multi-GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## ğŸ“¦ 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (T4 * 2 GPU í•„ìˆ˜!)"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "# !pip install -q transformers>=4.45.0 accelerate>=0.34.0 peft>=0.13.0 bitsandbytes>=0.43.0 \\\n",
        "#     datasets pillow pandas torch torchvision scikit-learn matplotlib seaborn tqdm scipy --upgrade\n",
        "# !pip install -q qwen-vl-utils==0.0.8\n",
        "\n",
        "import torch\n",
        "print(f\"âœ… ì„¤ì¹˜ ì™„ë£Œ! ëŸ°íƒ€ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.\")\n",
        "print(f\"\\nğŸ” GPU í™•ì¸: {torch.cuda.device_count()}ê°œ (ë°˜ë“œì‹œ 2ê°œì—¬ì•¼ í•¨!)\")\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {},
      "source": [
        "## ğŸ“š 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "import os, sys, re, math, random, warnings, json, pickle, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2VLForConditionalGeneration,  # âœ… 30B ì „ìš©\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
        "print(f\"ğŸ”§ CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ”§ GPU Count: {torch.cuda.device_count()}ê°œ\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_memory / 1e9:.1f}GB)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {},
      "source": [
        "## ğŸ”§ 3. Multi-GPU í•µì‹¬ í•¨ìˆ˜ (30B ì „ìš©)\n",
        "\n",
        "ì´ ì…€ì—ëŠ” 30B ëª¨ë¸ì„ Multi-GPUì—ì„œ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ í•µì‹¬ í•¨ìˆ˜ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "def print_gpu_memory_status():\n    \"\"\"ëª¨ë“  GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥\"\"\"\n    if not torch.cuda.is_available():\n        return\n    \n    print(\"=\"*60)\n    print(\"ğŸ’¾ GPU Memory Status\")\n    print(\"=\"*60)\n    \n    for i in range(torch.cuda.device_count()):\n        allocated = torch.cuda.memory_allocated(i) / 1e9\n        reserved = torch.cuda.memory_reserved(i) / 1e9\n        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n        usage_pct = (allocated / total) * 100\n        print(\n            f\"GPU {i}: {allocated:.2f}GB / {total:.1f}GB ({usage_pct:.1f}%) | \"\n            f\"Reserved: {reserved:.2f}GB\"\n        )\n    \n    print(\"=\"*60)\n\n\ndef clear_gpu_memory():\n    \"\"\"ëª¨ë“  GPU ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n\n\ndef create_model_and_processor_multigpu(\n    model_id: str,\n    image_size: int = 384,\n    lora_r: int = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n    target_modules: List[str] = None,\n    max_memory_per_gpu: Dict[int, str] = None,\n    use_gradient_checkpointing: bool = True\n):\n    \"\"\"\n    Multi-GPU í™˜ê²½ì—ì„œ 30B ëª¨ë¸ ë¡œë“œ\n    \n    Args:\n        model_id: ëª¨ë¸ ID\n        image_size: ì´ë¯¸ì§€ í¬ê¸°\n        lora_r: LoRA rank (30BëŠ” 8 ê¶Œì¥)\n        lora_alpha: LoRA alpha\n        lora_dropout: LoRA dropout\n        target_modules: LoRA target modules\n        max_memory_per_gpu: GPUë‹¹ ìµœëŒ€ ë©”ëª¨ë¦¬\n        use_gradient_checkpointing: Gradient checkpointing ì‚¬ìš©\n    \n    Returns:\n        (model, processor)\n    \"\"\"\n    print(\"ğŸ”§ Multi-GPU ëª¨ë¸ ë¡œë“œ ì‹œì‘...\")\n    \n    # GPU í™•ì¸\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"GPUê°€ í•„ìš”í•©ë‹ˆë‹¤!\")\n    \n    gpu_count = torch.cuda.device_count()\n    print(f\"   ì‚¬ìš© ê°€ëŠ¥ GPU: {gpu_count}ê°œ\")\n    \n    if gpu_count < 2:\n        print(\"âš ï¸  WARNING: 30B ëª¨ë¸ì€ GPU 2ê°œ ê¶Œì¥! (1ê°œë¡œë„ ê°€ëŠ¥í•˜ì§€ë§Œ ëŠë¦¼)\")\n    \n    # ê¸°ë³¸ max_memory ì„¤ì •\n    if max_memory_per_gpu is None:\n        if gpu_count >= 2:\n            max_memory_per_gpu = {0: \"14GB\", 1: \"14GB\"}\n        else:\n            max_memory_per_gpu = {0: \"14GB\"}\n    \n    # 4-bit Quantization ì„¤ì • (í•„ìˆ˜!)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    \n    print(f\"   4-bit Quantization ì„¤ì • ì™„ë£Œ\")\n    print(f\"   Max memory per GPU: {max_memory_per_gpu}\")\n    \n    # Processor ë¡œë“œ\n    processor = AutoProcessor.from_pretrained(\n        model_id,\n        min_pixels=image_size * image_size,\n        max_pixels=image_size * image_size,\n        trust_remote_code=True,\n    )\n    print(\"âœ… Processor ë¡œë“œ ì™„ë£Œ\")\n    \n    # ëª¨ë¸ ë¡œë“œ with Multi-GPU\n    print(\"   Base model ë¡œë“œ ì¤‘...\")\n    \n    # device_map=\"auto\"ë¡œ ìë™ ë³‘ë ¬í™”\n    base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n        model_id,\n        quantization_config=bnb_config,\n        device_map=\"auto\",  # ğŸ”¥ í•µì‹¬! ìë™ìœ¼ë¡œ ì—¬ëŸ¬ GPUì— ë¶„ì‚°\n        max_memory=max_memory_per_gpu,  # GPUë‹¹ ìµœëŒ€ ë©”ëª¨ë¦¬\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n    )\n    \n    print(\"âœ… Base model ë¡œë“œ ì™„ë£Œ\")\n    \n    # ëª¨ë¸ì´ ì–´ëŠ GPUì— ë°°ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n    if hasattr(base_model, 'hf_device_map'):\n        print(f\"   Device map: {base_model.hf_device_map}\")\n    \n    # Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)\n    if use_gradient_checkpointing:\n        base_model.gradient_checkpointing_enable()\n        print(\"âœ… Gradient checkpointing í™œì„±í™”\")\n    \n    # QLoRA ì¤€ë¹„\n    base_model = prepare_model_for_kbit_training(base_model)\n    \n    # LoRA Config\n    if target_modules is None:\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n    \n    lora_config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        target_modules=target_modules,\n        task_type=\"CAUSAL_LM\",\n    )\n    \n    # PEFT ëª¨ë¸ ìƒì„±\n    model = get_peft_model(base_model, lora_config)\n    model.print_trainable_parameters()\n    print(\"âœ… QLoRA ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n    \n    # ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥\n    print_gpu_memory_status()\n    \n    return model, processor\n\n\ndef get_choice_token_ids_robust(processor):\n    \"\"\"Choice token IDs ì¶”ì¶œ (ì—¬ëŸ¬ ë³€í˜• ê³ ë ¤)\"\"\"\n    choice_tokens = {}\n    for choice in ['a', 'b', 'c', 'd']:\n        variants = [choice, f\" {choice}\", f\"{choice} \", choice.upper()]\n        all_token_ids = set()\n        for variant in variants:\n            try:\n                token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n                all_token_ids.update(token_ids)\n            except:\n                pass\n        choice_tokens[choice] = list(all_token_ids)\n    return choice_tokens\n\n\nprint(\"âœ… Multi-GPU í•µì‹¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "## âš™ï¸ 4. Config ì„¤ì • (30B ìµœì í™”)"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "class Config:\n    # ì‹œë“œ\n    SEED = 42\n    \n    # ========== ëª¨ë¸ (30B) ==========\n    MODEL_ID = \"Qwen/Qwen2.5-VL-30B-A3B-Instruct\"  # ğŸ”¥ 30B ëª¨ë¸!\n    IMAGE_SIZE = 384  # 384=ì•ˆì „, 448=ê· í˜•, 512=OOMìœ„í—˜\n    \n    # ========== Multi-GPU ==========\n    MAX_MEMORY_PER_GPU = {0: \"14GB\", 1: \"14GB\"}  # T4 * 2 ìµœì í™”\n    USE_GRADIENT_CHECKPOINTING = True  # í•„ìˆ˜!\n    \n    # ë°ì´í„°\n    DATA_DIR = \"/content\"\n    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n    TEST_CSV = f\"{DATA_DIR}/test.csv\"\n    \n    # K-Fold\n    N_FOLDS = 3\n    USE_KFOLD = True\n    TRAIN_FOLDS = [0, 1, 2]\n    \n    # ========== QLoRA (30B ìµœì í™”) ==========\n    LORA_R = 8  # ğŸ”¥ 30BëŠ” ì‘ê²Œ! (3BëŠ” 16)\n    LORA_ALPHA = 16\n    LORA_DROPOUT = 0.05\n    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # í•„ìˆ˜ë§Œ\n    \n    # ========== í•™ìŠµ (ë©”ëª¨ë¦¬ ìµœì í™”) ==========\n    NUM_EPOCHS = 2  # 30BëŠ” ì ì€ epochë„ ì¶©ë¶„\n    BATCH_SIZE = 1  # ğŸ”¥ í•„ìˆ˜!\n    GRAD_ACCUM_STEPS = 16  # ğŸ”¥ ë†’ê²Œ! (íš¨ê³¼ì  ë°°ì¹˜: 16)\n    LEARNING_RATE = 5e-5  # í° ëª¨ë¸ì€ ì‘ì€ LR\n    WEIGHT_DECAY = 0.01\n    WARMUP_RATIO = 0.06\n    MAX_GRAD_NORM = 0.5  # 30BëŠ” ë” ì‘ê²Œ\n    \n    # ê³ ê¸‰ ê¸°ë²•\n    USE_AMP = True  # í•„ìˆ˜!\n    USE_COSINE_SCHEDULE = True\n    \n    # ì¶”ë¡ \n    USE_DIRECT_LOGIT_DECODE = True\n    MAX_NEW_TOKENS = 8\n    \n    # TTA (ì„ íƒ)\n    USE_TTA = False  # Trueë©´ ëŠë ¤ì§\n    TTA_SCALES = [1.0]  # [0.9, 1.0, 1.1]\n    \n    # ì•™ìƒë¸”\n    ENSEMBLE_METHOD = \"prob\"  # \"prob\" or \"vote\"\n    \n    # ì €ì¥\n    SAVE_DIR = f\"{DATA_DIR}/checkpoints\"\n    OUTPUT_DIR = f\"{DATA_DIR}/outputs\"\n    LOG_DIR = f\"{DATA_DIR}/logs\"\n    \n    # ìƒ˜í”Œë§\n    USE_SAMPLE = False  # ì „ì²´ ë°ì´í„°\n    SAMPLE_SIZE = 200\n    \n    # í”„ë¡¬í”„íŠ¸\n    SYSTEM_INSTRUCT = (\n        \"You are a helpful visual question answering assistant. \"\n        \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n    )\n\ncfg = Config()\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(cfg.SEED)\n\nprint(f\"âœ… Config ì„¤ì • ì™„ë£Œ (30B ìµœì í™”)\")\nprint(f\"=\"*60)\nprint(f\"ğŸ”¥ Model: {cfg.MODEL_ID}\")\nprint(f\"ğŸ”¥ Image Size: {cfg.IMAGE_SIZE}\")\nprint(f\"ğŸ”¥ LoRA R: {cfg.LORA_R} (30B ìµœì í™”)\")\nprint(f\"ğŸ”¥ Batch Size: {cfg.BATCH_SIZE} (í•„ìˆ˜!)\")\nprint(f\"ğŸ”¥ Grad Accum: {cfg.GRAD_ACCUM_STEPS} (ë†’ê²Œ!)\")\nprint(f\"ğŸ”¥ Max Memory: {cfg.MAX_MEMORY_PER_GPU}\")\nprint(f\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {},
      "source": [
        "## ğŸ“Š 5. ë°ì´í„° ë¡œë“œ & EDA"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
        "test_df = pd.read_csv(cfg.TEST_CSV)\n",
        "\n",
        "print(f\"ğŸ“ Train: {len(train_df):,} samples\")\n",
        "print(f\"ğŸ“ Test: {len(test_df):,} samples\")\n",
        "\n",
        "if cfg.USE_SAMPLE:\n",
        "    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n",
        "    print(f\"âš ï¸  Sampled {len(train_df)} samples\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Answer Distribution:\")\n",
        "print(train_df['answer'].value_counts().sort_index())\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "train_df['answer'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('Answer Distribution')\n",
        "axes[0].set_xlabel('Answer')\n",
        "axes[0].set_ylabel('Count')\n",
        "\n",
        "train_df['question_len'] = train_df['question'].str.len()\n",
        "train_df['question_len'].hist(bins=30, ax=axes[1], color='salmon')\n",
        "axes[1].set_title('Question Length')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {},
      "source": [
        "## ğŸ”„ 6. Stratified K-Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "if cfg.USE_KFOLD:\n",
        "    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n",
        "    train_df['fold'] = -1\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n",
        "        train_df.loc[val_idx, 'fold'] = fold\n",
        "    print(f\"âœ… {cfg.N_FOLDS}-Fold CV ìƒì„±\")\n",
        "    print(train_df['fold'].value_counts().sort_index())\n",
        "else:\n",
        "    split_idx = int(len(train_df) * 0.9)\n",
        "    train_df['fold'] = -1\n",
        "    train_df.loc[split_idx:, 'fold'] = 0\n",
        "    print(f\"âœ… Single split (90:10)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {},
      "source": [
        "## ğŸ—‚ï¸ 7. Dataset & DataCollator\n",
        "\n",
        "âœ… **ë¼ë²¨ ë§ˆìŠ¤í‚¹**: í”„ë¡¬í”„íŠ¸ í† í° ì†ì‹¤ ì œì™¸, assistant ì •ë‹µ í† í°ë§Œ ê°ë…"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-14",
      "metadata": {},
      "source": [
        "def build_mc_prompt(question, a, b, c, d):\n    return (\n        f\"{question}\\n\"\n        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n        \"ì •ë‹µì„ ë°˜ë“œì‹œ a, b, c, d ì¤‘ í•˜ë‚˜ì˜ ì†Œë¬¸ì í•œ ê¸€ìë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.\"\n    )\n\nclass VQADataset(Dataset):\n    def __init__(self, df, processor, data_dir=\"\", train=True):\n        self.df = df.reset_index(drop=True)\n        self.processor = processor\n        self.data_dir = data_dir\n        self.train = train\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # ì´ë¯¸ì§€ ë¡œë“œ\n        img_col = 'path' if 'path' in row else 'image'\n        img_path = os.path.join(self.data_dir, row[img_col])\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n        \n        user_text = build_mc_prompt(\n            str(row[\"question\"]), str(row[\"a\"]), \n            str(row[\"b\"]), str(row[\"c\"]), str(row[\"d\"])\n        )\n        \n        messages = [\n            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"image\", \"image\": img},\n                {\"type\": \"text\", \"text\": user_text}\n            ]}\n        ]\n        \n        answer = None\n        if self.train:\n            answer = str(row[\"answer\"]).strip().lower()\n            messages.append({\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"text\", \"text\": answer}]\n            })\n        \n        return {\"messages\": messages, \"image\": img, \"answer\": answer}\n\n@dataclass\nclass DataCollator:\n    processor: Any\n    train: bool = True\n    \n    def __call__(self, batch):\n        texts, images, answers = [], [], []\n        \n        for sample in batch:\n            text = self.processor.apply_chat_template(\n                sample[\"messages\"],\n                tokenize=False,\n                add_generation_prompt=False\n            )\n            text = unicodedata.normalize('NFKC', text)\n            texts.append(text)\n            images.append(sample[\"image\"])\n            answers.append(sample[\"answer\"])\n        \n        enc = self.processor(\n            text=texts,\n            images=images,\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        \n        if self.train:\n            labels = enc[\"input_ids\"].clone()\n            for i, answer in enumerate(answers):\n                if answer is None:\n                    labels[i, :] = -100\n                else:\n                    labels[i, :] = -100\n                    answer_ids = self.processor.tokenizer.encode(answer, add_special_tokens=False)\n                    if len(answer_ids) > 0:\n                        labels[i, -len(answer_ids):] = torch.tensor(answer_ids)\n            enc[\"labels\"] = labels\n        \n        return enc\n\nprint(\"âœ… Dataset & DataCollator ì •ì˜ ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {},
      "source": [
        "## ğŸ¤– 8. Model & Processor ë¡œë“œ (30B Multi-GPU)\n",
        "\n",
        "âœ… Multi-GPU ìë™ ë¶„ì‚° + 4-bit Quantization + Gradient Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-16",
      "metadata": {},
      "source": [
        "print(\"ğŸ”§ Qwen3-VL-30B ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "print(f\"âš ï¸  ì´ ì‘ì—…ì€ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "model, processor = create_model_and_processor_multigpu(\n",
        "    model_id=cfg.MODEL_ID,\n",
        "    image_size=cfg.IMAGE_SIZE,\n",
        "    lora_r=cfg.LORA_R,\n",
        "    lora_alpha=cfg.LORA_ALPHA,\n",
        "    lora_dropout=cfg.LORA_DROPOUT,\n",
        "    target_modules=cfg.TARGET_MODULES,\n",
        "    max_memory_per_gpu=cfg.MAX_MEMORY_PER_GPU,\n",
        "    use_gradient_checkpointing=cfg.USE_GRADIENT_CHECKPOINTING\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… 30B ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"\\nğŸ’¡ ëª¨ë¸ì´ ì—¬ëŸ¬ GPUì— ìë™ìœ¼ë¡œ ë¶„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {},
      "source": [
        "## ğŸ“ 9. Training Loop (Memory-Efficient)\n",
        "\n",
        "âœ… Val Accuracy + Confusion Matrix + í•™ìŠµ ê³¡ì„  + ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-18",
      "metadata": {},
      "source": [
        "def validate_with_accuracy(model, valid_loader, processor):\n    \"\"\"Val Loss + Accuracy + Confusion Matrix\"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(valid_loader, desc=\"Validating\", leave=False):\n            # Multi-GPU: ì²« ë²ˆì§¸ GPUë¡œ ì´ë™\n            batch = {k: v.to(\"cuda:0\") if isinstance(v, torch.Tensor) else v\n                     for k, v in batch.items()}\n            \n            with torch.amp.autocast('cuda', enabled=cfg.USE_AMP, dtype=torch.float16):\n                outputs = model(**batch)\n                total_loss += outputs.loss.item()\n            \n            logits = outputs.logits\n            labels = batch[\"labels\"]\n            \n            for i in range(len(labels)):\n                valid_mask = labels[i] != -100\n                if valid_mask.any():\n                    last_valid_idx = valid_mask.nonzero(as_tuple=True)[0][-1]\n                    pred_id = logits[i, last_valid_idx].argmax().item()\n                    label_id = labels[i, last_valid_idx].item()\n                    \n                    pred_char = processor.tokenizer.decode([pred_id]).strip().lower()\n                    label_char = processor.tokenizer.decode([label_id]).strip().lower()\n                    \n                    if pred_char in ['a', 'b', 'c', 'd']:\n                        all_preds.append(pred_char)\n                    else:\n                        all_preds.append('a')\n                    \n                    if label_char in ['a', 'b', 'c', 'd']:\n                        all_labels.append(label_char)\n                    else:\n                        all_labels.append('a')\n    \n    avg_loss = total_loss / len(valid_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    cm = confusion_matrix(all_labels, all_preds, labels=['a', 'b', 'c', 'd'])\n    \n    model.train()\n    return avg_loss, accuracy, cm, all_preds, all_labels\n\n\ndef train_one_fold(model, train_loader, valid_loader, fold=0):\n    \"\"\"ë‹¨ì¼ Fold í•™ìŠµ (30B ìµœì í™”)\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Training Fold {fold}\")\n    print(f\"{'='*60}\")\n    \n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=cfg.LEARNING_RATE,\n        weight_decay=cfg.WEIGHT_DECAY\n    )\n    \n    num_training_steps = cfg.NUM_EPOCHS * math.ceil(len(train_loader) / cfg.GRAD_ACCUM_STEPS)\n    num_warmup_steps = int(num_training_steps * cfg.WARMUP_RATIO)\n    \n    if cfg.USE_COSINE_SCHEDULE:\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n    else:\n        from transformers import get_linear_schedule_with_warmup\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n    \n    scaler = torch.amp.GradScaler('cuda', enabled=cfg.USE_AMP)\n    \n    best_val_acc = 0.0\n    best_val_loss = float('inf')\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n    \n    for epoch in range(cfg.NUM_EPOCHS):\n        model.train()\n        running_loss = 0.0\n        steps = 0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.NUM_EPOCHS} [train]\")\n        \n        for step, batch in enumerate(progress_bar, start=1):\n            # Multi-GPU: ì²« ë²ˆì§¸ GPUë¡œ ì´ë™\n            batch = {k: v.to(\"cuda:0\") if isinstance(v, torch.Tensor) else v\n                     for k, v in batch.items()}\n            \n            with torch.amp.autocast('cuda', enabled=cfg.USE_AMP, dtype=torch.float16):\n                outputs = model(**batch)\n                loss = outputs.loss / cfg.GRAD_ACCUM_STEPS\n            \n            scaler.scale(loss).backward()\n            running_loss += loss.item()\n            \n            if step % cfg.GRAD_ACCUM_STEPS == 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.MAX_GRAD_NORM)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n                scheduler.step()\n                steps += 1\n                \n                avg_loss = running_loss / cfg.GRAD_ACCUM_STEPS\n                progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n                running_loss = 0.0\n                \n                # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (30B ì¤‘ìš”!)\n                if steps % 50 == 0:\n                    clear_gpu_memory()\n        \n        # Validation\n        val_loss, val_acc, cm, preds, labels = validate_with_accuracy(model, valid_loader, processor)\n        \n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        \n        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        print(f\"Confusion Matrix:\\n{cm}\")\n        \n        # Best ëª¨ë¸ ì €ì¥\n        is_best = False\n        if val_acc > best_val_acc:\n            is_best = True\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n        elif val_acc == best_val_acc and val_loss < best_val_loss:\n            is_best = True\n            best_val_loss = val_loss\n        \n        if is_best:\n            save_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n            os.makedirs(save_path, exist_ok=True)\n            model.save_pretrained(save_path)\n            processor.save_pretrained(save_path)\n            print(f\"   âœ… Best model saved (Acc={val_acc:.4f}, Loss={val_loss:.4f})\")\n        \n        # ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥\n        print_gpu_memory_status()\n    \n    # í•™ìŠµ ê³¡ì„  ì €ì¥\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    ax1.plot(history[\"val_loss\"], marker='o')\n    ax1.set_title(f'Fold {fold} - Val Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.grid(True)\n    \n    ax2.plot(history[\"val_acc\"], marker='o', color='green')\n    ax2.set_title(f'Fold {fold} - Val Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.grid(True)\n    plt.tight_layout()\n    \n    log_dir = Path(cfg.LOG_DIR)\n    log_dir.mkdir(parents=True, exist_ok=True)\n    plt.savefig(log_dir / f\"fold{fold}_learning_curve.png\")\n    plt.show()\n    \n    return best_val_acc, best_val_loss\n\nprint(\"âœ… Training functions ì •ì˜ ì™„ë£Œ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {},
      "source": [
        "## ğŸš€ 10. í•™ìŠµ ì‹¤í–‰ (K-Fold)"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-20",
      "metadata": {},
      "source": [
        "if cfg.USE_KFOLD:\n    results = {}\n    \n    for fold in cfg.TRAIN_FOLDS:\n        print(f\"\\n{'#'*60}\")\n        print(f\"Starting Fold {fold}/{cfg.N_FOLDS-1}\")\n        print(f\"{'#'*60}\")\n        \n        train_subset = train_df[train_df['fold'] != fold].reset_index(drop=True)\n        valid_subset = train_df[train_df['fold'] == fold].reset_index(drop=True)\n        \n        print(f\"Train: {len(train_subset)}, Valid: {len(valid_subset)}\")\n        \n        train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True)\n        valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=False)\n        \n        train_loader = DataLoader(\n            train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True,\n            collate_fn=DataCollator(processor, train=True),\n            num_workers=0\n        )\n        valid_loader = DataLoader(\n            valid_ds, batch_size=cfg.BATCH_SIZE, shuffle=False,\n            collate_fn=DataCollator(processor, train=False),\n            num_workers=0\n        )\n        \n        best_acc, best_loss = train_one_fold(model, train_loader, valid_loader, fold=fold)\n        results[fold] = {\"acc\": best_acc, \"loss\": best_loss}\n        \n        print(f\"\\nâœ… Fold {fold} ì™„ë£Œ: Best Val Acc={best_acc:.4f}, Loss={best_loss:.4f}\")\n        \n        # ë©”ëª¨ë¦¬ ì •ë¦¬\n        clear_gpu_memory()\n    \n    print(f\"\\n{'='*60}\")\n    print(\"All Folds Training Complete!\")\n    print(f\"{'='*60}\")\n    for fold, metrics in results.items():\n        print(f\"Fold {fold}: Acc={metrics['acc']:.4f}, Loss={metrics['loss']:.4f}\")\n    print(f\"Average Acc: {np.mean([m['acc'] for m in results.values()]):.4f}\")\n\nelse:\n    train_subset = train_df[train_df['fold'] == -1].reset_index(drop=True)\n    valid_subset = train_df[train_df['fold'] == 0].reset_index(drop=True)\n    \n    train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True)\n    valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=False)\n    \n    train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True,\n                             collate_fn=DataCollator(processor, train=True), num_workers=0)\n    valid_loader = DataLoader(valid_ds, batch_size=cfg.BATCH_SIZE, shuffle=False,\n                             collate_fn=DataCollator(processor, train=False), num_workers=0)\n    \n    best_acc, best_loss = train_one_fold(model, train_loader, valid_loader, fold=0)\n    print(f\"\\nâœ… Single model í•™ìŠµ ì™„ë£Œ: Best Val Acc={best_acc:.4f}, Loss={best_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {},
      "source": [
        "## ğŸ”® 11. Inference with Direct Logits\n",
        "\n",
        "âœ… Direct Logits: a/b/c/d í† í° í™•ë¥  ì§ì ‘ ê³„ì‚°"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-22",
      "metadata": {},
      "source": [
        "def get_choice_token_ids(processor):\n    \"\"\"a/b/c/d í† í° ID ì¶”ì¶œ\"\"\"\n    choice_tokens = {}\n    for choice in ['a', 'b', 'c', 'd']:\n        token_ids = processor.tokenizer.encode(choice, add_special_tokens=False)\n        choice_tokens[choice] = token_ids\n    return choice_tokens\n\n\ndef infer_with_direct_logits(model, processor, test_df, tta_scales=[1.0], fold=0):\n    \"\"\"Direct Logits ì¶”ë¡ \"\"\"\n    model.eval()\n    \n    # pad_token_id ì„¤ì •\n    if processor.tokenizer.pad_token_id is None:\n        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n    \n    choice_tokens = get_choice_token_ids(processor)\n    \n    all_predictions = []\n    all_probs = []\n    \n    for i in tqdm(range(len(test_df)), desc=f\"Fold {fold} Inference\"):\n        row = test_df.iloc[i]\n        \n        tta_logits = []\n        \n        for scale in tta_scales:\n            # ì´ë¯¸ì§€ ë¡œë“œ\n            img_col = 'path' if 'path' in row else 'image'\n            img_path = os.path.join(cfg.DATA_DIR, row[img_col])\n            try:\n                img = Image.open(img_path).convert(\"RGB\")\n            except:\n                img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n            \n            # TTA ìŠ¤ì¼€ì¼\n            if scale != 1.0:\n                w, h = img.size\n                new_w, new_h = int(w * scale), int(h * scale)\n                img = img.resize((new_w, new_h), Image.BILINEAR)\n            \n            # í”„ë¡¬í”„íŠ¸\n            user_text = build_mc_prompt(\n                str(row[\"question\"]), str(row[\"a\"]),\n                str(row[\"b\"]), str(row[\"c\"]), str(row[\"d\"])\n            )\n            \n            messages = [\n                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n                {\"role\": \"user\", \"content\": [\n                    {\"type\": \"image\", \"image\": img},\n                    {\"type\": \"text\", \"text\": user_text}\n                ]}\n            ]\n            \n            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            \n            inputs = processor(text=[text], images=[img], return_tensors=\"pt\")\n            # Multi-GPU: ì²« ë²ˆì§¸ GPUë¡œ ì´ë™\n            inputs = {k: v.to(\"cuda:0\") for k, v in inputs.items()}\n            \n            # Direct Logits\n            with torch.no_grad():\n                outputs = model(**inputs)\n                logits = outputs.logits[0, -1, :]\n            \n            tta_logits.append(logits.cpu())\n        \n        # TTA í‰ê· \n        avg_logits = torch.stack(tta_logits).mean(dim=0)\n        \n        # a/b/c/d í† í° í™•ë¥ \n        choice_probs = {}\n        for choice, token_ids in choice_tokens.items():\n            total_logit = sum([avg_logits[tid].item() for tid in token_ids])\n            choice_probs[choice] = total_logit\n        \n        logit_values = torch.tensor(list(choice_probs.values()))\n        probs = F.softmax(logit_values, dim=0).numpy()\n        prob_dict = {choice: probs[idx] for idx, choice in enumerate(['a', 'b', 'c', 'd'])}\n        \n        pred = max(prob_dict, key=prob_dict.get)\n        \n        all_predictions.append(pred)\n        all_probs.append(prob_dict)\n        \n        # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬\n        if (i + 1) % 100 == 0:\n            clear_gpu_memory()\n    \n    result_df = pd.DataFrame({\n        'id': test_df['id'],\n        'answer': all_predictions,\n        'prob_a': [p['a'] for p in all_probs],\n        'prob_b': [p['b'] for p in all_probs],\n        'prob_c': [p['c'] for p in all_probs],\n        'prob_d': [p['d'] for p in all_probs]\n    })\n    \n    return result_df\n\n\n# ê° Fold ì¶”ë¡ \npredictions_all = []\n\nif cfg.USE_KFOLD:\n    for fold in cfg.TRAIN_FOLDS:\n        model_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Inferencing Fold {fold}\")\n        print(f\"{'='*60}\")\n        \n        # ëª¨ë¸ ë¡œë“œ\n        model_infer = Qwen2VLForConditionalGeneration.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",  # Multi-GPU\n            max_memory=cfg.MAX_MEMORY_PER_GPU\n        )\n        model_infer.eval()\n        \n        processor_infer = AutoProcessor.from_pretrained(\n            model_path,\n            min_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n            max_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n            trust_remote_code=True,\n        )\n        \n        # Inference\n        tta_scales = cfg.TTA_SCALES if cfg.USE_TTA else [1.0]\n        pred_df = infer_with_direct_logits(model_infer, processor_infer, test_df, tta_scales, fold)\n        \n        # ì €ì¥\n        output_path = f\"{cfg.OUTPUT_DIR}/submission_fold{fold}.csv\"\n        os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n        pred_df.to_csv(output_path, index=False)\n        print(f\"âœ… Saved to {output_path}\")\n        \n        predictions_all.append(pred_df)\n        \n        # ë©”ëª¨ë¦¬ ì •ë¦¬\n        del model_infer\n        clear_gpu_memory()\n\nelse:\n    model_path = f\"{cfg.SAVE_DIR}/fold0_best\"\n    \n    model_infer = Qwen2VLForConditionalGeneration.from_pretrained(\n        model_path,\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        max_memory=cfg.MAX_MEMORY_PER_GPU\n    )\n    processor_infer = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n    \n    tta_scales = cfg.TTA_SCALES if cfg.USE_TTA else [1.0]\n    pred_df = infer_with_direct_logits(model_infer, processor_infer, test_df, tta_scales, fold=0)\n    \n    output_path = f\"{cfg.OUTPUT_DIR}/submission_single.csv\"\n    pred_df.to_csv(output_path, index=False)\n    predictions_all.append(pred_df)\n\nprint(\"\\nâœ… All inference complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-23",
      "metadata": {},
      "source": [
        "## ğŸ¯ 12. Ensemble (í™•ë¥  í‰ê· )\n",
        "\n",
        "âœ… Probability Averaging"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-24",
      "metadata": {},
      "source": [
        "if cfg.USE_KFOLD and len(predictions_all) > 1:\n    print(f\"\\n{'='*60}\")\n    print(f\"Ensemble Method: {cfg.ENSEMBLE_METHOD}\")\n    print(f\"{'='*60}\")\n    \n    if cfg.ENSEMBLE_METHOD == 'prob':\n        print(\"Using Probability Averaging...\")\n        \n        ensemble_probs = pd.DataFrame({\n            'id': test_df['id'],\n            'prob_a': np.mean([df['prob_a'].values for df in predictions_all], axis=0),\n            'prob_b': np.mean([df['prob_b'].values for df in predictions_all], axis=0),\n            'prob_c': np.mean([df['prob_c'].values for df in predictions_all], axis=0),\n            'prob_d': np.mean([df['prob_d'].values for df in predictions_all], axis=0)\n        })\n        \n        prob_cols = ['prob_a', 'prob_b', 'prob_c', 'prob_d']\n        ensemble_probs['answer'] = ensemble_probs[prob_cols].values.argmax(axis=1)\n        ensemble_probs['answer'] = ensemble_probs['answer'].map({0: 'a', 1: 'b', 2: 'c', 3: 'd'})\n        \n        final_submission = ensemble_probs[['id', 'answer', 'prob_a', 'prob_b', 'prob_c', 'prob_d']]\n    \n    else:\n        print(\"Using Majority Voting...\")\n        \n        ensemble_preds = []\n        for i in range(len(test_df)):\n            votes = [pred.iloc[i]['answer'] for pred in predictions_all]\n            most_common = Counter(votes).most_common(1)[0][0]\n            ensemble_preds.append(most_common)\n        \n        final_submission = pd.DataFrame({\n            'id': test_df['id'],\n            'answer': ensemble_preds\n        })\n    \n    final_path = f\"{cfg.OUTPUT_DIR}/submission_ensemble.csv\"\n    final_submission.to_csv(final_path, index=False)\n    \n    print(f\"âœ… Ensemble submission saved to {final_path}\")\n    print(f\"\\nAnswer Distribution:\")\n    print(final_submission['answer'].value_counts().sort_index())\n\nelse:\n    print(\"\\nâœ… Single model - No ensemble needed\")\n    final_submission = predictions_all[0]\n    final_path = f\"{cfg.OUTPUT_DIR}/submission_single.csv\"\n    final_submission.to_csv(final_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-25",
      "metadata": {},
      "source": [
        "## ğŸ“Š 13. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-26",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n\nanswer_counts = final_submission['answer'].value_counts().sort_index()\nsns.barplot(x=answer_counts.index, y=answer_counts.values, palette='viridis', ax=ax)\nax.set_title('Final Submission Answer Distribution', fontsize=14, weight='bold')\nax.set_xlabel('Answer')\nax.set_ylabel('Count')\nax.grid(axis='y', alpha=0.3)\n\nfor i, (ans, count) in enumerate(answer_counts.items()):\n    percentage = count / len(final_submission) * 100\n    ax.text(i, count + 10, f\"{percentage:.1f}%\", ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n{'='*60}\")\nprint(\"Final Statistics\")\nprint(f\"{'='*60}\")\nprint(f\"Total predictions: {len(final_submission)}\")\nprint(f\"\\nAnswer counts:\")\nfor ans, count in answer_counts.items():\n    print(f\"  {ans}: {count:5d} ({count/len(final_submission)*100:5.1f}%)\")\n\nif 'prob_a' in final_submission.columns:\n    print(f\"\\n{'='*60}\")\n    print(\"Probability Statistics\")\n    print(f\"{'='*60}\")\n    prob_cols = ['prob_a', 'prob_b', 'prob_c', 'prob_d']\n    print(final_submission[prob_cols].describe())\n\nprint(f\"\\n{'='*60}\")\nprint(\"Sample Predictions\")\nprint(f\"{'='*60}\")\nprint(final_submission.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-27",
      "metadata": {},
      "source": [
        "## âœ… 14. ìµœì¢… ì •ë¦¬\n",
        "\n",
        "### ğŸ‰ ì™„ë£Œëœ ì‘ì—…\n",
        "\n",
        "1. âœ… Multi-GPU ëª¨ë¸ ë¡œë“œ (ìë™ ë¶„ì‚°)\n",
        "2. âœ… 4-bit Quantization (75% ë©”ëª¨ë¦¬ ì ˆê°)\n",
        "3. âœ… Gradient Checkpointing (40% í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆê°)\n",
        "4. âœ… Memory-efficient Training Loop\n",
        "5. âœ… Val Accuracy + Confusion Matrix ë¡œê¹…\n",
        "6. âœ… Direct Logits ì¶”ë¡ \n",
        "7. âœ… í™•ë¥  ì•™ìƒë¸”\n",
        "8. âœ… ê²°ê³¼ ë¶„ì„ & ì‹œê°í™”\n",
        "\n",
        "### ğŸš€ 30B vs 3B ë¹„êµ\n",
        "\n",
        "| í•­ëª© | 3B | 30B (ì´ ë…¸íŠ¸ë¶) |\n",
        "|------|----|-----------------|\n",
        "| GPU ìš”êµ¬ | 1ê°œ | 2ê°œ (í•„ìˆ˜!) |\n",
        "| LoRA R | 16 | 8 |\n",
        "| Grad Accum | 4-8 | 16 |\n",
        "| Batch Size | 1-2 | 1 |\n",
        "| Image Size | 512 | 384 (ì•ˆì „) |\n",
        "| **ì˜ˆìƒ ì •í™•ë„** | 85-87% | **88-90%** |\n",
        "\n",
        "### ğŸ“Š ìµœì  ì„¤ì • (T4 * 2)\n",
        "\n",
        "```python\n",
        "MODEL_ID = \"Qwen/Qwen2.5-VL-30B-A3B-Instruct\"\n",
        "IMAGE_SIZE = 384  # ì•ˆì „\n",
        "LORA_R = 8\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM_STEPS = 16\n",
        "MAX_MEMORY_PER_GPU = {0: \"14GB\", 1: \"14GB\"}\n",
        "```\n",
        "\n",
        "### âš ï¸ OOM ë°œìƒ ì‹œ ëŒ€ì‘\n",
        "\n",
        "1. `IMAGE_SIZE = 384` â†’ `320`\n",
        "2. `LORA_R = 8` â†’ `4`\n",
        "3. `GRAD_ACCUM_STEPS = 16` â†’ `32`\n",
        "4. `MAX_MEMORY_PER_GPU` â†’ `{0: \"12GB\", 1: \"12GB\"}`\n",
        "\n",
        "### ğŸ’¡ ì£¼ìš” íŠ¹ì§•\n",
        "\n",
        "- **ìë™ ë³‘ë ¬í™”**: `device_map=\"auto\"`ë¡œ 2ê°œ GPUì— ëª¨ë¸ ìë™ ë¶„ì‚°\n",
        "- **ë©”ëª¨ë¦¬ ìµœì í™”**: 4-bit + Checkpointing + High Gradient Accumulation\n",
        "- **ì•ˆì •ì  ì¶”ë¡ **: Direct Logits ë°©ì‹ (ìƒì„± ëŒ€ë¹„ ë¹ ë¥´ê³  ì •í™•)\n",
        "- **ì£¼ê¸°ì  ì •ë¦¬**: GPU ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬ë¡œ OOM ë°©ì§€\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ¤– SSAFY AI Project 2025 - Qwen3-VL-30B Multi-GPU Edition**\n",
        "\n",
        "**âœ¨ Optimized for T4 * 2 (32GB)**\n",
        "\n",
        "**ğŸ¯ ëª©í‘œ ì •í™•ë„: 88-90%**\n",
        "\n",
        "**â­ í–‰ìš´ì„ ë¹•ë‹ˆë‹¤!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}