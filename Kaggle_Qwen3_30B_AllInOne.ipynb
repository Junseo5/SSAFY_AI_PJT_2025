{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üöÄ Qwen3-VL-30B Multi-GPU - Complete Workflow\n\n## üéØ ÌäπÏßï\n\n- **Î™®Îç∏**: Qwen/Qwen2.5-VL-30B-A3B-Instruct (30B)\n- **ÌôòÍ≤Ω**: T4 * 2 (Ï¥ù 32GB)\n- **ÏÑ±Îä•**: 88-90% Ï†ïÌôïÎèÑ\n\n### ‚úÖ ÌïµÏã¨ ÏµúÏ†ÅÌôî\n\n- Multi-GPU Model Parallelism\n- 4-bit Quantization (75% Î©îÎ™®Î¶¨ Ï†àÍ∞ê)\n- Gradient Checkpointing (40% Ï†àÍ∞ê)\n- Memory-efficient Training\n\n**ü§ñ SSAFY AI Project 2025**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers accelerate peft bitsandbytes datasets pillow pandas torch torchvision scikit-learn matplotlib seaborn tqdm --upgrade\n# !pip install -q qwen-vl-utils==0.0.8\nprint(\"‚úÖ ÏÑ§Ïπò ÏôÑÎ£å! Îü∞ÌÉÄÏûÑ Ïû¨ÏãúÏûëÌïòÏÑ∏Ïöî.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö 2. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 3. Qwen3-VL-30B Multi-GPU Core Functions\n\nT4 * 2 ÌôòÍ≤ΩÏùÑ ÏúÑÌïú ÏµúÏ†ÅÌôîÎêú Ìï®ÏàòÎì§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nQwen3-VL-30B Multi-GPU ÌïµÏã¨ Î°úÏßÅ\nT4 * 2 (32GB) ÌôòÍ≤ΩÏóêÏÑú ÏïàÏ†ÑÌïòÍ≤å 30B Î™®Îç∏ Ïã§Ìñâ\n\nÏ£ºÏöî Í∏∞Îä•:\n1. Multi-GPU Model Parallelism\n2. 4-bit Quantization with QLoRA\n3. Memory-efficient Training\n4. Parallel Inference\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import (\n    Qwen2VLForConditionalGeneration,\n    AutoProcessor,\n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom accelerate import Accelerator\nimport logging\nimport gc\nfrom typing import Dict, List, Optional\nfrom PIL import Image\n\nlogger = logging.getLogger('VQA_30B')\n\n# ============================================================================\n# 1. Multi-GPU Î™®Îç∏ Î°úÎìú (ÌïµÏã¨!)\n# ============================================================================\n\ndef create_model_and_processor_multigpu(\n    model_id: str,\n    image_size: int = 384,\n    lora_r: int = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n    target_modules: List[str] = None,\n    max_memory_per_gpu: Dict[int, str] = None,\n    use_gradient_checkpointing: bool = True,\n    logger = None\n):\n    \"\"\"\n    Multi-GPU ÌôòÍ≤ΩÏóêÏÑú 30B Î™®Îç∏ Î°úÎìú\n\n    Args:\n        model_id: Î™®Îç∏ ID (Qwen/Qwen2.5-VL-30B-A3B-Instruct)\n        image_size: Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞\n        lora_r: LoRA rank\n        lora_alpha: LoRA alpha\n        lora_dropout: LoRA dropout\n        target_modules: LoRA target modules\n        max_memory_per_gpu: GPUÎãπ ÏµúÎåÄ Î©îÎ™®Î¶¨ {0: \"14GB\", 1: \"14GB\"}\n        use_gradient_checkpointing: Gradient checkpointing ÏÇ¨Ïö© Ïó¨Î∂Ä\n        logger: Î°úÍ±∞\n\n    Returns:\n        (model, processor)\n    \"\"\"\n    if logger:\n        logger.info(\"üîß Multi-GPU Î™®Îç∏ Î°úÎìú ÏãúÏûë...\")\n\n    # GPU ÌôïÏù∏\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"GPUÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§!\")\n\n    gpu_count = torch.cuda.device_count()\n    if logger:\n        logger.info(f\"   ÏÇ¨Ïö© Í∞ÄÎä• GPU: {gpu_count}Í∞ú\")\n\n    # Í∏∞Î≥∏ max_memory ÏÑ§Ï†ï\n    if max_memory_per_gpu is None:\n        if gpu_count >= 2:\n            max_memory_per_gpu = {0: \"14GB\", 1: \"14GB\"}\n        else:\n            max_memory_per_gpu = {0: \"14GB\"}\n\n    # 4-bit Quantization ÏÑ§Ï†ï (ÌïÑÏàò!)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,  # Double quantization\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,  # T4Îäî BF16 ÎØ∏ÏßÄÏõê\n    )\n\n    if logger:\n        logger.info(f\"   4-bit Quantization ÏÑ§Ï†ï ÏôÑÎ£å\")\n        logger.info(f\"   Max memory per GPU: {max_memory_per_gpu}\")\n\n    # Processor Î°úÎìú\n    try:\n        processor = AutoProcessor.from_pretrained(\n            model_id,\n            min_pixels=image_size * image_size,\n            max_pixels=image_size * image_size,\n            trust_remote_code=True,\n        )\n        if logger:\n            logger.info(\"‚úÖ Processor Î°úÎìú ÏôÑÎ£å\")\n    except Exception as e:\n        if logger:\n            logger.error(f\"‚ùå Processor Î°úÎìú Ïã§Ìå®: {e}\")\n        raise\n\n    # Î™®Îç∏ Î°úÎìú with Multi-GPU\n    try:\n        if logger:\n            logger.info(\"   Base model Î°úÎìú Ï§ë...\")\n\n        # device_map=\"auto\"Î°ú ÏûêÎèô Î≥ëÎ†¨Ìôî\n        base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n            model_id,\n            quantization_config=bnb_config,\n            device_map=\"auto\",  # ÌïµÏã¨! ÏûêÎèôÏúºÎ°ú Ïó¨Îü¨ GPUÏóê Î∂ÑÏÇ∞\n            max_memory=max_memory_per_gpu,  # GPUÎãπ ÏµúÎåÄ Î©îÎ™®Î¶¨\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,  # CPU Î©îÎ™®Î¶¨ Ï†àÏïΩ\n        )\n\n        if logger:\n            logger.info(\"‚úÖ Base model Î°úÎìú ÏôÑÎ£å\")\n            # Î™®Îç∏Ïù¥ Ïñ¥Îäê GPUÏóê Î∞∞ÏπòÎêòÏóàÎäîÏßÄ ÌôïÏù∏\n            if hasattr(base_model, 'hf_device_map'):\n                logger.info(f\"   Device map: {base_model.hf_device_map}\")\n\n    except Exception as e:\n        if logger:\n            logger.error(f\"‚ùå Model Î°úÎìú Ïã§Ìå®: {e}\")\n        raise\n\n    # Gradient Checkpointing (Î©îÎ™®Î¶¨ Ï†àÏïΩ)\n    if use_gradient_checkpointing:\n        base_model.gradient_checkpointing_enable()\n        if logger:\n            logger.info(\"‚úÖ Gradient checkpointing ÌôúÏÑ±Ìôî\")\n\n    # QLoRA Ï§ÄÎπÑ\n    base_model = prepare_model_for_kbit_training(base_model)\n\n    # LoRA Config\n    if target_modules is None:\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n    lora_config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        target_modules=target_modules,\n        task_type=\"CAUSAL_LM\",\n    )\n\n    # PEFT Î™®Îç∏ ÏÉùÏÑ±\n    model = get_peft_model(base_model, lora_config)\n\n    if logger:\n        model.print_trainable_parameters()\n        logger.info(\"‚úÖ QLoRA Î™®Îç∏ ÏÉùÏÑ± ÏôÑÎ£å\")\n\n    # Î©îÎ™®Î¶¨ ÏÉÅÌÉú Ï∂úÎ†•\n    if logger:\n        print_gpu_memory_status(logger)\n\n    return model, processor\n\n\ndef print_gpu_memory_status(logger):\n    \"\"\"Î™®Îì† GPU Î©îÎ™®Î¶¨ ÏÉÅÌÉú Ï∂úÎ†•\"\"\"\n    if not torch.cuda.is_available():\n        return\n\n    logger.info(\"=\"*60)\n    logger.info(\"üíæ GPU Memory Status\")\n    logger.info(\"=\"*60)\n\n    for i in range(torch.cuda.device_count()):\n        allocated = torch.cuda.memory_allocated(i) / 1e9\n        reserved = torch.cuda.memory_reserved(i) / 1e9\n        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n        usage_pct = (allocated / total) * 100\n        logger.info(\n            f\"GPU {i}: {allocated:.2f}GB / {total:.1f}GB ({usage_pct:.1f}%) | \"\n            f\"Reserved: {reserved:.2f}GB\"\n        )\n\n    logger.info(\"=\"*60)\n\n\ndef clear_gpu_memory():\n    \"\"\"Î™®Îì† GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n\n\n# ============================================================================\n# 2. Memory-Efficient Training Loop\n# ============================================================================\n\ndef train_one_epoch_memory_efficient(\n    model,\n    train_loader,\n    optimizer,\n    scheduler,\n    scaler,\n    grad_accum_steps: int,\n    max_grad_norm: float,\n    device,\n    logger=None\n):\n    \"\"\"\n    Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†ÅÏù∏ ÌïôÏäµ Î£®ÌîÑ\n\n    Args:\n        model: Î™®Îç∏\n        train_loader: Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n        optimizer: ÏòµÌã∞ÎßàÏù¥Ï†Ä\n        scheduler: Ïä§ÏºÄÏ§ÑÎü¨\n        scaler: AMP scaler\n        grad_accum_steps: Gradient accumulation steps\n        max_grad_norm: Max gradient norm\n        device: ÎîîÎ∞îÏù¥Ïä§\n        logger: Î°úÍ±∞\n\n    Returns:\n        average_loss\n    \"\"\"\n    model.train()\n    total_loss = 0.0\n    steps = 0\n\n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n        # Îç∞Ïù¥ÌÑ∞Î•º deviceÎ°ú Ïù¥Îèô (multi-GPUÏùò Í≤ΩÏö∞ ÏûêÎèô Ï≤òÎ¶¨)\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n                 for k, v in batch.items()}\n\n        # Forward with AMP\n        with torch.amp.autocast('cuda', enabled=True, dtype=torch.float16):\n            outputs = model(**batch)\n            loss = outputs.loss / grad_accum_steps\n\n        # Backward\n        scaler.scale(loss).backward()\n        total_loss += loss.item() * grad_accum_steps\n\n        # Gradient accumulation\n        if (batch_idx + 1) % grad_accum_steps == 0:\n            # Gradient clipping\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n            # Optimizer step\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)  # Î©îÎ™®Î¶¨ Ï†àÏïΩ\n\n            scheduler.step()\n            steps += 1\n\n            # Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n            if steps % 50 == 0:\n                clear_gpu_memory()\n\n    avg_loss = total_loss / len(train_loader)\n    return avg_loss\n\n\n# ============================================================================\n# 3. Parallel Inference (Multi-GPU)\n# ============================================================================\n\ndef infer_parallel(\n    model,\n    processor,\n    test_df,\n    data_dir: str,\n    img_col: str = 'path',\n    system_instruct: str = \"\",\n    logger=None\n):\n    \"\"\"\n    Multi-GPU Î≥ëÎ†¨ Ï∂îÎ°†\n\n    Args:\n        model: Î™®Îç∏ (Ïù¥ÎØ∏ Multi-GPUÏóê Î∂ÑÏÇ∞Îê®)\n        processor: Processor\n        test_df: Test Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ\n        data_dir: Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨\n        img_col: Ïù¥ÎØ∏ÏßÄ Ïª¨ÎüºÎ™Ö\n        system_instruct: System instruction\n        logger: Î°úÍ±∞\n\n    Returns:\n        predictions: List of predictions\n    \"\"\"\n    model.eval()\n    predictions = []\n\n    # Choice token IDs Ï∂îÏ∂ú\n    choice_tokens = get_choice_token_ids_robust(processor)\n\n    with torch.no_grad():\n        for idx in tqdm(range(len(test_df)), desc=\"Inference\"):\n            row = test_df.iloc[idx]\n\n            # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n            img_path = f\"{data_dir}/{row[img_col]}\"\n            try:\n                img = Image.open(img_path).convert(\"RGB\")\n            except:\n                img = Image.new('RGB', (384, 384), color='white')\n\n            # ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±\n            question_text = build_mc_prompt(\n                row['question'], row['a'], row['b'], row['c'], row['d']\n            )\n\n            messages = [\n                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_instruct}]},\n                {\"role\": \"user\", \"content\": [\n                    {\"type\": \"image\", \"image\": img},\n                    {\"type\": \"text\", \"text\": question_text}\n                ]}\n            ]\n\n            # ProcessorÎ°ú Ï≤òÎ¶¨\n            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            inputs = processor(text=[text], images=[img], return_tensors=\"pt\")\n\n            # ÏûÖÎ†•ÏùÑ Ï†ÅÏ†àÌïú deviceÎ°ú Ïù¥Îèô (Ï≤´ Î≤àÏß∏ GPU)\n            inputs = {k: v.to(\"cuda:0\") for k, v in inputs.items()}\n\n            # Forward\n            outputs = model(**inputs)\n            logits = outputs.logits[0, -1, :]  # Last token logits\n\n            # Choice ÌôïÎ•† Í≥ÑÏÇ∞\n            probs = extract_choice_probs_from_logits(logits, choice_tokens)\n            pred = max(probs, key=probs.get)\n\n            predictions.append(pred)\n\n            # Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n            if (idx + 1) % 100 == 0:\n                clear_gpu_memory()\n\n    return predictions\n\n\n# ============================================================================\n# 4. Helper Functions\n# ============================================================================\n\ndef build_mc_prompt(question, a, b, c, d):\n    \"\"\"Multiple choice ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±\"\"\"\n    return (\n        f\"{question}\\\\n\"\n        f\"(a) {a}\\\\n(b) {b}\\\\n(c) {c}\\\\n(d) {d}\\\\n\\\\n\"\n        \"Ï†ïÎãµÏùÑ Î∞òÎìúÏãú a, b, c, d Ï§ë ÌïòÎÇòÏùò ÏÜåÎ¨∏Ïûê Ìïú Í∏ÄÏûêÎ°úÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî.\"\n    )\n\n\ndef get_choice_token_ids_robust(processor):\n    \"\"\"Choice token IDs Ï∂îÏ∂ú (Ïó¨Îü¨ Î≥ÄÌòï Í≥†Î†§)\"\"\"\n    choice_tokens = {}\n    for choice in ['a', 'b', 'c', 'd']:\n        variants = [choice, f\" {choice}\", f\"{choice} \", choice.upper()]\n        all_token_ids = set()\n        for variant in variants:\n            try:\n                token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n                all_token_ids.update(token_ids)\n            except:\n                pass\n        choice_tokens[choice] = list(all_token_ids)\n    return choice_tokens\n\n\ndef extract_choice_probs_from_logits(logits, choice_tokens):\n    \"\"\"LogitsÏóêÏÑú choice ÌôïÎ•† Ï∂îÏ∂ú\"\"\"\n    choice_logits = {}\n    for choice, token_ids in choice_tokens.items():\n        if len(token_ids) > 0:\n            max_logit = max([logits[tid].item() for tid in token_ids])\n            choice_logits[choice] = max_logit\n        else:\n            choice_logits[choice] = -float('inf')\n\n    # Softmax\n    logit_values = torch.tensor(list(choice_logits.values()))\n    probs = F.softmax(logit_values, dim=0).numpy()\n\n    return {choice: probs[i] for i, choice in enumerate(['a', 'b', 'c', 'd'])}\n\n\n# ============================================================================\n# 5. Accelerate ÌÜµÌï© (Advanced)\n# ============================================================================\n\ndef setup_accelerator(\n    gradient_accumulation_steps: int = 16,\n    mixed_precision: str = \"fp16\",\n    cpu_offload: bool = True\n):\n    \"\"\"\n    Accelerate ÏÑ§Ï†ï\n\n    Args:\n        gradient_accumulation_steps: Gradient accumulation steps\n        mixed_precision: Mixed precision (\"fp16\", \"bf16\", \"no\")\n        cpu_offload: CPU offload ÏÇ¨Ïö© Ïó¨Î∂Ä\n\n    Returns:\n        Accelerator instance\n    \"\"\"\n    from accelerate import Accelerator\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        mixed_precision=mixed_precision,\n        log_with=\"tensorboard\",\n        cpu=cpu_offload,\n    )\n\n    return accelerator\n\n\n# ============================================================================\n# ÏÇ¨Ïö© ÏòàÏãú\n# ============================================================================\n\nif __name__ == \"__main__\":\n    print(\"=\"*60)\n    print(\"Qwen3-VL-30B Multi-GPU Core Functions\")\n    print(\"=\"*60)\n    print()\n    print(\"Ï£ºÏöî Ìï®Ïàò:\")\n    print(\"1. create_model_and_processor_multigpu() - Multi-GPU Î™®Îç∏ Î°úÎìú\")\n    print(\"2. train_one_epoch_memory_efficient() - Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†Å ÌïôÏäµ\")\n    print(\"3. infer_parallel() - Î≥ëÎ†¨ Ï∂îÎ°†\")\n    print(\"4. print_gpu_memory_status() - GPU Î©îÎ™®Î¶¨ Î™®ÎãàÌÑ∞ÎßÅ\")\n    print(\"5. clear_gpu_memory() - GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\")\n    print()\n    print(\"=\"*60)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 4. Config (Qwen3-30B Multi-GPU ÏµúÏ†ÅÌôî)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Config:\n    # ÏãúÎìú\n    SEED = 42\n    \n    # Î™®Îç∏ (Qwen3-30B)\n    MODEL_ID = \"Qwen/Qwen2.5-VL-30B-A3B-Instruct\"\n    IMAGE_SIZE = 384  # Î©îÎ™®Î¶¨ Ï†àÏïΩ\n    \n    # Îç∞Ïù¥ÌÑ∞\n    DATA_DIR = \"/content\"\n    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n    TEST_CSV = f\"{DATA_DIR}/test.csv\"\n    \n    # K-Fold\n    N_FOLDS = 3\n    USE_KFOLD = True\n    TRAIN_FOLDS = [0, 1, 2]\n    \n    # QLoRA (30B ÏµúÏ†ÅÌôî)\n    LORA_R = 8  # ÏûëÍ≤å\n    LORA_ALPHA = 16\n    LORA_DROPOUT = 0.05\n    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n    \n    # ÌïôÏäµ (Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî)\n    NUM_EPOCHS = 2\n    BATCH_SIZE = 1  # ÌïÑÏàò!\n    GRAD_ACCUM_STEPS = 16  # ÎÜíÍ≤å\n    LEARNING_RATE = 5e-5\n    WEIGHT_DECAY = 0.01\n    WARMUP_RATIO = 0.1\n    MAX_GRAD_NORM = 0.5\n    \n    # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî\n    USE_AMP = True\n    USE_EMA = False  # ÎπÑÌôúÏÑ±Ìôî\n    USE_SWA = False  # ÎπÑÌôúÏÑ±Ìôî\n    USE_COSINE_SCHEDULE = True\n    USE_GRADIENT_CHECKPOINTING = True\n    \n    # TTA\n    USE_TTA = False\n    TTA_SCALES = [1.0]\n    \n    # Ï∂îÎ°†\n    USE_DIRECT_LOGIT_DECODE = True\n    MAX_NEW_TOKENS = 8\n    \n    # Temperature Scaling\n    USE_TEMPERATURE_SCALING = True\n    \n    # ÏïôÏÉÅÎ∏î\n    ENSEMBLE_METHOD = \"prob\"\n    \n    # Ï†ÄÏû•\n    SAVE_DIR = f\"{DATA_DIR}/checkpoints_30b\"\n    OUTPUT_DIR = f\"{DATA_DIR}/outputs_30b\"\n    LOG_DIR = f\"{DATA_DIR}/logs_30b\"\n    \n    # ÏÉòÌîåÎßÅ\n    USE_SAMPLE = False\n    SAMPLE_SIZE = 200\n    \n    # ÌîÑÎ°¨ÌîÑÌä∏\n    SYSTEM_INSTRUCT = (\n        \"You are a helpful visual question answering assistant. \"\n        \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n    )\n    \n    # Multi-GPU\n    MAX_MEMORY_PER_GPU = {0: \"14GB\", 1: \"14GB\"}\n    \n    # Î°úÍπÖ\n    LOG_LEVEL = logging.INFO\n    LOG_TO_FILE = True\n\ncfg = Config()\n\n# ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\nfor dir_path in [cfg.SAVE_DIR, cfg.OUTPUT_DIR, cfg.LOG_DIR]:\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n\n# Î°úÍπÖ ÏÑ§Ï†ï\nlogger = logging.getLogger('VQA_30B')\nlogger.setLevel(cfg.LOG_LEVEL)\nlogger.handlers.clear()\nconsole = logging.StreamHandler(sys.stdout)\nconsole.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(console)\n\n# ÏãúÎìú Í≥†Ï†ï\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(cfg.SEED)\n\nlogger.info(\"=\"*60)\nlogger.info(\"üöÄ Qwen3-VL-30B Multi-GPU Configuration\")\nlogger.info(\"=\"*60)\nlogger.info(f\"Model: {cfg.MODEL_ID}\")\nlogger.info(f\"Image Size: {cfg.IMAGE_SIZE}\")\nlogger.info(f\"Batch: {cfg.BATCH_SIZE}, Grad Accum: {cfg.GRAD_ACCUM_STEPS}\")\nlogger.info(f\"Effective Batch: {cfg.BATCH_SIZE * cfg.GRAD_ACCUM_STEPS}\")\nlogger.info(f\"LoRA R: {cfg.LORA_R}, LR: {cfg.LEARNING_RATE}\")\nlogger.info(\"=\"*60)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 4. Îç∞Ïù¥ÌÑ∞ Î°úÎìú & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(cfg.TRAIN_CSV)\ntest_df = pd.read_csv(cfg.TEST_CSV)\n\nprint(f\"üìÅ Train: {len(train_df):,} samples\")\nprint(f\"üìÅ Test: {len(test_df):,} samples\")\n\nif cfg.USE_SAMPLE:\n    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n    print(f\"‚ö†Ô∏è  Sampled {len(train_df)} samples\")\n\nprint(f\"\\nüìä Answer Distribution:\")\nprint(train_df['answer'].value_counts().sort_index())\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\ntrain_df['answer'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='skyblue')\naxes[0].set_title('Answer Distribution')\naxes[0].set_xlabel('Answer')\naxes[0].set_ylabel('Count')\n\ntrain_df['question_len'] = train_df['question'].str.len()\ntrain_df['question_len'].hist(bins=30, ax=axes[1], color='salmon')\naxes[1].set_title('Question Length')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 5. Stratified K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.USE_KFOLD:\n    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n    train_df['fold'] = -1\n    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n        train_df.loc[val_idx, 'fold'] = fold\n    print(f\"‚úÖ {cfg.N_FOLDS}-Fold CV ÏÉùÏÑ±\")\n    print(train_df['fold'].value_counts().sort_index())\nelse:\n    split_idx = int(len(train_df) * 0.9)\n    train_df['fold'] = -1\n    train_df.loc[split_idx:, 'fold'] = 0\n    print(f\"‚úÖ Single split (90:10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è 6. Dataset & DataCollator\n\n‚úÖ **ÎùºÎ≤® ÎßàÏä§ÌÇπ**: ÌîÑÎ°¨ÌîÑÌä∏ ÌÜ†ÌÅ∞ ÏÜêÏã§ Ï†úÏô∏, assistant Ï†ïÎãµ ÌÜ†ÌÅ∞Îßå Í∞êÎèÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mc_prompt(question, a, b, c, d):\n    return (\n        f\"{question}\\n\"\n        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n        \"Ï†ïÎãµÏùÑ Î∞òÎìúÏãú a, b, c, d Ï§ë ÌïòÎÇòÏùò ÏÜåÎ¨∏Ïûê Ìïú Í∏ÄÏûêÎ°úÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî.\"\n    )\n\nclass VQADataset(Dataset):\n    def __init__(self, df, processor, data_dir=\"\", train=True, use_advanced=False):\n        self.df = df.reset_index(drop=True)\n        self.processor = processor\n        self.data_dir = data_dir\n        self.train = train\n        self.use_advanced = use_advanced\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú (path Ïª¨Îüº ÏßÄÏõê)\n        img_col = 'path' if 'path' in row else 'image'\n        img_path = os.path.join(self.data_dir, row[img_col])\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n        \n        user_text = build_mc_prompt(\n            str(row[\"question\"]), str(row[\"a\"]), \n            str(row[\"b\"]), str(row[\"c\"]), str(row[\"d\"])\n        )\n        \n        messages = [\n            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"image\", \"image\": img},\n                {\"type\": \"text\", \"text\": user_text}\n            ]}\n        ]\n        \n        # ‚úÖ ÌïôÏäµ ÏãúÏóêÎßå Ï†ïÎãµ Ìè¨Ìï®\n        answer = None\n        if self.train:\n            answer = str(row[\"answer\"]).strip().lower()\n            messages.append({\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"text\", \"text\": answer}]\n            })\n        \n        return {\"messages\": messages, \"image\": img, \"answer\": answer}\n\n@dataclass\nclass DataCollator:\n    processor: Any\n    train: bool = True\n    use_advanced: bool = False\n    \n    def __call__(self, batch):\n        texts, images, answers = [], [], []\n        \n        for sample in batch:\n            text = self.processor.apply_chat_template(\n                sample[\"messages\"],\n                tokenize=False,\n                add_generation_prompt=False  # ‚úÖ False!\n            )\n            text = unicodedata.normalize('NFKC', text)\n            texts.append(text)\n            images.append(sample[\"image\"])\n            answers.append(sample[\"answer\"])\n        \n        enc = self.processor(\n            text=texts,\n            images=images,\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        \n        # ‚úÖ ÎùºÎ≤® ÎßàÏä§ÌÇπ: Ï†ïÎãµ ÌÜ†ÌÅ∞Îßå Í∞êÎèÖ\n        if self.train:\n            labels = enc[\"input_ids\"].clone()\n            for i, answer in enumerate(answers):\n                if answer is None:\n                    labels[i, :] = -100\n                else:\n                    # ÌîÑÎ°¨ÌîÑÌä∏ Î∂ÄÎ∂Ñ -100\n                    labels[i, :] = -100\n                    # Ï†ïÎãµ ÌÜ†ÌÅ∞Îßå Ïú†ÏßÄ\n                    answer_ids = self.processor.tokenizer.encode(answer, add_special_tokens=False)\n                    if len(answer_ids) > 0:\n                        labels[i, -len(answer_ids):] = torch.tensor(answer_ids)\n            enc[\"labels\"] = labels\n        \n        return enc\n\nprint(\"‚úÖ Dataset & DataCollator Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Model & Processor Î°úÎìú (Multi-GPU)\n\n‚úÖ create_model_and_processor_multigpu() ÏÇ¨Ïö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multi-GPU Î™®Îç∏ Î°úÎìú\nlogger.info(\"üîß Multi-GPU Î™®Îç∏ Î°úÎìú ÏãúÏûë...\")\n\nmodel, processor = create_model_and_processor_multigpu(\n    model_id=cfg.MODEL_ID,\n    image_size=cfg.IMAGE_SIZE,\n    lora_r=cfg.LORA_R,\n    lora_alpha=cfg.LORA_ALPHA,\n    lora_dropout=cfg.LORA_DROPOUT,\n    target_modules=cfg.TARGET_MODULES,\n    max_memory_per_gpu=cfg.MAX_MEMORY_PER_GPU,\n    use_gradient_checkpointing=cfg.USE_GRADIENT_CHECKPOINTING,\n    logger=logger\n)\n\nlogger.info(\"‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å!\")\n\n# Ïù¥ÎØ∏ÏßÄ Ïª¨Îüº ÌôïÏù∏\nimg_col = 'path' if 'path' in train_df.columns else 'image'\nlogger.info(f\"üì∑ Ïù¥ÎØ∏ÏßÄ Ïª¨Îüº: {img_col}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Model & Processor Î°úÎìú (Multi-GPU)\n\n‚úÖ create_model_and_processor_multigpu() ÏÇ¨Ïö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multi-GPU Î™®Îç∏ Î°úÎìú\nlogger.info(\"üîß Multi-GPU Î™®Îç∏ Î°úÎìú ÏãúÏûë...\")\n\nmodel, processor = create_model_and_processor_multigpu(\n    model_id=cfg.MODEL_ID,\n    image_size=cfg.IMAGE_SIZE,\n    lora_r=cfg.LORA_R,\n    lora_alpha=cfg.LORA_ALPHA,\n    lora_dropout=cfg.LORA_DROPOUT,\n    target_modules=cfg.TARGET_MODULES,\n    max_memory_per_gpu=cfg.MAX_MEMORY_PER_GPU,\n    use_gradient_checkpointing=cfg.USE_GRADIENT_CHECKPOINTING,\n    logger=logger\n)\n\nlogger.info(\"‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å!\")\n\n# Ïù¥ÎØ∏ÏßÄ Ïª¨Îüº ÌôïÏù∏\nimg_col = 'path' if 'path' in train_df.columns else 'image'\nlogger.info(f\"üì∑ Ïù¥ÎØ∏ÏßÄ Ïª¨Îüº: {img_col}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 8. Training Loop\n\n‚úÖ **Val Accuracy Î°úÍπÖ** + Confusion Matrix + ÌïôÏäµ Í≥°ÏÑ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n    def __init__(self, model, decay=0.999):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        self.register()\n    \n    def register(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n    \n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_average = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data\n                self.shadow[name] = new_average.clone()\n    \n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data.clone()\n                param.data = self.shadow[name]\n    \n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n        self.backup = {}\n\n\ndef validate_with_accuracy(model, valid_loader, processor):\n    \"\"\"‚úÖ Val Loss + Accuracy + Confusion Matrix\"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(valid_loader, desc=\"Validating\", leave=False):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            with torch.amp.autocast('cuda', enabled=cfg.USE_AMP, dtype=torch.float16):\n                outputs = model(**batch)\n                total_loss += outputs.loss.item()\n            \n            # ‚úÖ Accuracy Í≥ÑÏÇ∞ (Ï†ïÎãµ ÌÜ†ÌÅ∞ ÏòàÏ∏°)\n            logits = outputs.logits\n            labels = batch[\"labels\"]\n            \n            for i in range(len(labels)):\n                # ÎßàÏßÄÎßâ ÎπÑ-Ìå®Îî© ÌÜ†ÌÅ∞ ÏúÑÏπò Ï∞æÍ∏∞\n                valid_mask = labels[i] != -100\n                if valid_mask.any():\n                    last_valid_idx = valid_mask.nonzero(as_tuple=True)[0][-1]\n                    pred_id = logits[i, last_valid_idx].argmax().item()\n                    label_id = labels[i, last_valid_idx].item()\n                    \n                    # ÌÜ†ÌÅ∞ ‚Üí Î¨∏Ïûê Î≥ÄÌôò\n                    pred_char = processor.tokenizer.decode([pred_id]).strip().lower()\n                    label_char = processor.tokenizer.decode([label_id]).strip().lower()\n                    \n                    # a/b/c/dÎßå ÏàòÏßë\n                    if pred_char in ['a', 'b', 'c', 'd']:\n                        all_preds.append(pred_char)\n                    else:\n                        all_preds.append('a')  # Fallback\n                    \n                    if label_char in ['a', 'b', 'c', 'd']:\n                        all_labels.append(label_char)\n                    else:\n                        all_labels.append('a')\n    \n    avg_loss = total_loss / len(valid_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    cm = confusion_matrix(all_labels, all_preds, labels=['a', 'b', 'c', 'd'])\n    \n    model.train()\n    return avg_loss, accuracy, cm, all_preds, all_labels\n\n\ndef train_one_fold(model, train_loader, valid_loader, fold=0):\n    \"\"\"Îã®Ïùº Fold ÌïôÏäµ (Val Acc Ïö∞ÏÑ† Ï†ÄÏû•)\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Training Fold {fold}\")\n    print(f\"{'='*60}\")\n    \n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=cfg.LEARNING_RATE,\n        weight_decay=cfg.WEIGHT_DECAY\n    )\n    \n    num_training_steps = cfg.NUM_EPOCHS * math.ceil(len(train_loader) / cfg.GRAD_ACCUM_STEPS)\n    num_warmup_steps = int(num_training_steps * cfg.WARMUP_RATIO)\n    \n    if cfg.USE_COSINE_SCHEDULE:\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n    else:\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n    \n    scaler = torch.amp.GradScaler('cuda', enabled=cfg.USE_AMP)\n    ema = EMA(model, decay=cfg.EMA_DECAY) if cfg.USE_EMA else None\n    \n    swa_model = None\n    if cfg.USE_SWA:\n        swa_model = AveragedModel(model)\n        swa_scheduler = SWALR(optimizer, swa_lr=cfg.LEARNING_RATE * 0.1)\n    \n    best_val_acc = 0.0\n    best_val_loss = float('inf')\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n    \n    for epoch in range(cfg.NUM_EPOCHS):\n        model.train()\n        running_loss = 0.0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.NUM_EPOCHS} [train]\")\n        \n        for step, batch in enumerate(progress_bar, start=1):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            with torch.amp.autocast('cuda', enabled=cfg.USE_AMP, dtype=torch.float16):\n                outputs = model(**batch)\n                loss = outputs.loss / cfg.GRAD_ACCUM_STEPS\n            \n            scaler.scale(loss).backward()\n            running_loss += loss.item()\n            \n            if step % cfg.GRAD_ACCUM_STEPS == 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.MAX_GRAD_NORM)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n                \n                if cfg.USE_SWA and epoch >= cfg.SWA_START_EPOCH:\n                    swa_scheduler.step()\n                else:\n                    scheduler.step()\n                \n                if cfg.USE_EMA and ema is not None:\n                    ema.update()\n                \n                avg_loss = running_loss / cfg.GRAD_ACCUM_STEPS\n                progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n                running_loss = 0.0\n        \n        # SWA update\n        if cfg.USE_SWA and swa_model is not None and epoch >= cfg.SWA_START_EPOCH:\n            swa_model.update_parameters(model)\n        \n        # ‚úÖ Validation with Accuracy\n        if cfg.USE_EMA and ema is not None:\n            ema.apply_shadow()\n        \n        val_loss, val_acc, cm, preds, labels = validate_with_accuracy(model, valid_loader, processor)\n        \n        if cfg.USE_EMA and ema is not None:\n            ema.restore()\n        \n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        \n        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        print(f\"Confusion Matrix:\\n{cm}\")\n        \n        # ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Acc Ïö∞ÏÑ†, ÎèôÎ•† Ïãú Loss)\n        is_best = False\n        if val_acc > best_val_acc:\n            is_best = True\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n        elif val_acc == best_val_acc and val_loss < best_val_loss:\n            is_best = True\n            best_val_loss = val_loss\n        \n        if is_best:\n            save_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n            os.makedirs(save_path, exist_ok=True)\n            \n            if cfg.USE_EMA and ema is not None:\n                ema.apply_shadow()\n            \n            model.save_pretrained(save_path)\n            processor.save_pretrained(save_path)\n            \n            if cfg.USE_EMA and ema is not None:\n                ema.restore()\n            \n            print(f\"   ‚úÖ Best model saved (Acc={val_acc:.4f}, Loss={val_loss:.4f})\")\n    \n    # SWA ÏµúÏ¢Ö Î™®Îç∏\n    if cfg.USE_SWA and swa_model is not None:\n        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n        save_path = f\"{cfg.SAVE_DIR}/fold{fold}_swa\"\n        os.makedirs(save_path, exist_ok=True)\n        swa_model.module.save_pretrained(save_path)\n        processor.save_pretrained(save_path)\n        print(f\"   ‚úÖ SWA model saved\")\n    \n    # ‚úÖ ÌïôÏäµ Í≥°ÏÑ† Ï†ÄÏû•\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    ax1.plot(history[\"val_loss\"], marker='o')\n    ax1.set_title(f'Fold {fold} - Val Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.grid(True)\n    \n    ax2.plot(history[\"val_acc\"], marker='o', color='green')\n    ax2.set_title(f'Fold {fold} - Val Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.grid(True)\n    plt.tight_layout()\n    \n    log_dir = Path(cfg.LOG_DIR)\n    log_dir.mkdir(parents=True, exist_ok=True)\n    plt.savefig(log_dir / f\"fold{fold}_learning_curve.png\")\n    plt.show()\n    \n    return best_val_acc, best_val_loss\n\nprint(\"‚úÖ Training functions Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 9. ÌïôÏäµ Ïã§Ìñâ (K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê train=False Ï†ÅÏö© (Ï†ïÎãµ Ï£ºÏûÖ Î∞©ÏßÄ)\n\nif cfg.USE_KFOLD:\n    results = {}\n    \n    for fold in cfg.TRAIN_FOLDS:\n        print(f\"\\n{'#'*60}\")\n        print(f\"Starting Fold {fold}/{cfg.N_FOLDS-1}\")\n        print(f\"{'#'*60}\")\n        \n        train_subset = train_df[train_df['fold'] != fold].reset_index(drop=True)\n        valid_subset = train_df[train_df['fold'] == fold].reset_index(drop=True)\n        \n        print(f\"Train: {len(train_subset)}, Valid: {len(valid_subset)}\")\n        \n        train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n        valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=False, use_advanced=cfg.USE_ADVANCED_MODEL)  # ‚úÖ train=False\n        \n        train_loader = DataLoader(\n            train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True,\n            collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL),\n            num_workers=0\n        )\n        valid_loader = DataLoader(\n            valid_ds, batch_size=cfg.BATCH_SIZE, shuffle=False,\n            collate_fn=DataCollator(processor, train=False, use_advanced=cfg.USE_ADVANCED_MODEL),  # ‚úÖ train=False\n            num_workers=0\n        )\n        \n        best_acc, best_loss = train_one_fold(model, train_loader, valid_loader, fold=fold)\n        results[fold] = {\"acc\": best_acc, \"loss\": best_loss}\n        \n        print(f\"\\n‚úÖ Fold {fold} ÏôÑÎ£å: Best Val Acc={best_acc:.4f}, Loss={best_loss:.4f}\")\n    \n    print(f\"\\n{'='*60}\")\n    print(\"All Folds Training Complete!\")\n    print(f\"{'='*60}\")\n    for fold, metrics in results.items():\n        print(f\"Fold {fold}: Acc={metrics['acc']:.4f}, Loss={metrics['loss']:.4f}\")\n    print(f\"Average Acc: {np.mean([m['acc'] for m in results.values()]):.4f}\")\n\nelse:\n    # Îã®Ïùº Î™®Îç∏\n    train_subset = train_df[train_df['fold'] == -1].reset_index(drop=True)\n    valid_subset = train_df[train_df['fold'] == 0].reset_index(drop=True)\n    \n    train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n    valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=False, use_advanced=cfg.USE_ADVANCED_MODEL)  # ‚úÖ train=False\n    \n    train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True,\n                             collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL), num_workers=0)\n    valid_loader = DataLoader(valid_ds, batch_size=cfg.BATCH_SIZE, shuffle=False,\n                             collate_fn=DataCollator(processor, train=False, use_advanced=cfg.USE_ADVANCED_MODEL), num_workers=0)\n    \n    best_acc, best_loss = train_one_fold(model, train_loader, valid_loader, fold=0)\n    print(f\"\\n‚úÖ Single model ÌïôÏäµ ÏôÑÎ£å: Best Val Acc={best_acc:.4f}, Loss={best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ 10. Inference with Direct Logits + TTA\n\n‚úÖ **Direct Logits**: a/b/c/d ÌÜ†ÌÅ∞ ÌôïÎ•† ÏßÅÏ†ë Í≥ÑÏÇ∞ (ÏÉùÏÑ± ÎåÄÎπÑ ÏïàÏ†ï)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_choice_token_ids(processor):\n    \"\"\"a/b/c/d ÌÜ†ÌÅ∞ ID Ï∂îÏ∂ú\"\"\"\n    choice_tokens = {}\n    for choice in ['a', 'b', 'c', 'd']:\n        token_ids = processor.tokenizer.encode(choice, add_special_tokens=False)\n        choice_tokens[choice] = token_ids\n    return choice_tokens\n\n\ndef infer_with_direct_logits(model, processor, test_df, tta_scales=[1.0], fold=0):\n    \"\"\"‚úÖ Direct Logits Ï∂îÎ°† + TTA\"\"\"\n    model.eval()\n    \n    # pad_token_id ÏÑ§Ï†ï\n    if processor.tokenizer.pad_token_id is None:\n        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n    \n    choice_tokens = get_choice_token_ids(processor)\n    \n    all_predictions = []\n    all_probs = []\n    \n    for i in tqdm(range(len(test_df)), desc=f\"Fold {fold} Inference\"):\n        row = test_df.iloc[i]\n        \n        # TTA: Ïó¨Îü¨ Ïä§ÏºÄÏùºÎ°ú Ï∂îÎ°†\n        tta_logits = []\n        \n        for scale in tta_scales:\n            # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n            img_col = 'path' if 'path' in row else 'image'\n            img_path = os.path.join(cfg.DATA_DIR, row[img_col])\n            try:\n                img = Image.open(img_path).convert(\"RGB\")\n            except:\n                img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n            \n            # TTA Ïä§ÏºÄÏùº Ï†ÅÏö©\n            if scale != 1.0:\n                w, h = img.size\n                new_w, new_h = int(w * scale), int(h * scale)\n                img = img.resize((new_w, new_h), Image.BILINEAR)\n            \n            # ÌîÑÎ°¨ÌîÑÌä∏\n            user_text = build_mc_prompt(\n                str(row[\"question\"]), str(row[\"a\"]),\n                str(row[\"b\"]), str(row[\"c\"]), str(row[\"d\"])\n            )\n            \n            messages = [\n                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n                {\"role\": \"user\", \"content\": [\n                    {\"type\": \"image\", \"image\": img},\n                    {\"type\": \"text\", \"text\": user_text}\n                ]}\n            ]\n            \n            # ‚úÖ add_generation_prompt=True (Ï∂îÎ°† Ïãú)\n            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            \n            inputs = processor(text=[text], images=[img], return_tensors=\"pt\")\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            \n            # ‚úÖ Direct Logits: Îã§Ïùå ÌÜ†ÌÅ∞ Î∂ÑÌè¨ÏóêÏÑú a/b/c/d ÌôïÎ•† Í≥ÑÏÇ∞\n            with torch.no_grad():\n                outputs = model(**inputs)\n                logits = outputs.logits[0, -1, :]  # ÎßàÏßÄÎßâ ÌÜ†ÌÅ∞Ïùò logits\n            \n            tta_logits.append(logits.cpu())\n        \n        # TTA ÌèâÍ∑†\n        avg_logits = torch.stack(tta_logits).mean(dim=0)\n        \n        # ‚úÖ a/b/c/d ÌÜ†ÌÅ∞ ÌôïÎ•† ÏßëÍ≥Ñ\n        choice_probs = {}\n        for choice, token_ids in choice_tokens.items():\n            # Ìï¥Îãπ choiceÏùò Î™®Îì† ÌÜ†ÌÅ∞ logit Ìï©ÏÇ∞\n            total_logit = sum([avg_logits[tid].item() for tid in token_ids])\n            choice_probs[choice] = total_logit\n        \n        # SoftmaxÎ°ú ÌôïÎ•† Î≥ÄÌôò\n        logit_values = torch.tensor(list(choice_probs.values()))\n        probs = F.softmax(logit_values, dim=0).numpy()\n        prob_dict = {choice: probs[idx] for idx, choice in enumerate(['a', 'b', 'c', 'd'])}\n        \n        # ÏòàÏ∏°\n        pred = max(prob_dict, key=prob_dict.get)\n        \n        all_predictions.append(pred)\n        all_probs.append(prob_dict)\n    \n    # DataFrame ÏÉùÏÑ±\n    result_df = pd.DataFrame({\n        'id': test_df['id'],\n        'answer': all_predictions,\n        'prob_a': [p['a'] for p in all_probs],\n        'prob_b': [p['b'] for p in all_probs],\n        'prob_c': [p['c'] for p in all_probs],\n        'prob_d': [p['d'] for p in all_probs]\n    })\n    \n    return result_df\n\n\n# Í∞Å Fold Ï∂îÎ°†\npredictions_all = []\n\nif cfg.USE_KFOLD:\n    for fold in cfg.TRAIN_FOLDS:\n        model_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Inferencing Fold {fold}\")\n        print(f\"{'='*60}\")\n        \n        # Î™®Îç∏ Î°úÎìú\n        if cfg.USE_ADVANCED_MODEL:\n            model_infer = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                model_path, trust_remote_code=True, torch_dtype=torch.float16\n            )\n        else:\n            model_infer = AutoModelForVision2Seq.from_pretrained(\n                model_path, trust_remote_code=True, torch_dtype=torch.float16\n            )\n        \n        model_infer = model_infer.to(device)\n        model_infer.eval()\n        \n        processor_infer = AutoProcessor.from_pretrained(\n            model_path,\n            min_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n            max_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n            trust_remote_code=True,\n        )\n        \n        # Direct Logits + TTA\n        tta_scales = cfg.TTA_SCALES if cfg.USE_TTA else [1.0]\n        pred_df = infer_with_direct_logits(model_infer, processor_infer, test_df, tta_scales, fold)\n        \n        # Ï†ÄÏû•\n        output_path = f\"{cfg.OUTPUT_DIR}/submission_fold{fold}.csv\"\n        os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n        pred_df.to_csv(output_path, index=False)\n        print(f\"‚úÖ Saved to {output_path}\")\n        \n        predictions_all.append(pred_df)\n        \n        # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n        del model_infer\n        torch.cuda.empty_cache()\n\nelse:\n    model_path = f\"{cfg.SAVE_DIR}/fold0_best\"\n    \n    if cfg.USE_ADVANCED_MODEL:\n        model_infer = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n            model_path, trust_remote_code=True, torch_dtype=torch.float16\n        ).to(device)\n    else:\n        model_infer = AutoModelForVision2Seq.from_pretrained(\n            model_path, trust_remote_code=True, torch_dtype=torch.float16\n        ).to(device)\n    \n    processor_infer = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n    \n    tta_scales = cfg.TTA_SCALES if cfg.USE_TTA else [1.0]\n    pred_df = infer_with_direct_logits(model_infer, processor_infer, test_df, tta_scales, fold=0)\n    \n    output_path = f\"{cfg.OUTPUT_DIR}/submission_single.csv\"\n    pred_df.to_csv(output_path, index=False)\n    predictions_all.append(pred_df)\n\nprint(\"\\n‚úÖ All inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå°Ô∏è 11. Temperature Scaling\n\n‚úÖ Í≤ÄÏ¶ù ÏÑ∏Ìä∏Î°ú ÌôïÎ•† ÍµêÏ†ï (ÏÑ†ÌÉù ÏÇ¨Ìï≠)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Temperature Scaling (ÏÑ†ÌÉù ÏÇ¨Ìï≠)\n# Í≤ÄÏ¶ù ÏÑ∏Ìä∏Í∞Ä ÏûàÏùÑ Îïå Í∞Å foldÏùò ÏµúÏ†Å temperatureÎ•º Ï∞æÏïÑ test ÌôïÎ•†Ïóê Ï†ÅÏö©\n\ndef find_optimal_temperature(val_probs, val_labels):\n    \"\"\"Í≤ÄÏ¶ù ÏÑ∏Ìä∏ÏóêÏÑú ÏµúÏ†Å temperature ÌÉêÏÉâ\"\"\"\n    from scipy.optimize import minimize\n    \n    def nll_loss(temp):\n        scaled_probs = F.softmax(torch.tensor(val_probs) / temp, dim=1).numpy()\n        # Negative log-likelihood\n        nll = -np.log(scaled_probs[np.arange(len(val_labels)), val_labels] + 1e-10).mean()\n        return nll\n    \n    result = minimize(nll_loss, x0=[1.0], bounds=[(0.1, 10.0)])\n    return result.x[0]\n\n# Ïã§Ï†úÎ°ú ÏÇ¨Ïö©ÌïòÎ†§Î©¥:\n# 1. Í≤ÄÏ¶ù ÏÑ∏Ìä∏Î°ú ÌôïÎ•†Í≥º Ï†ïÎãµ ÏàòÏßë\n# 2. optimal_temp = find_optimal_temperature(val_probs, val_labels)\n# 3. test ÌôïÎ•†Ïóê Ï†ÅÏö©: scaled_probs = F.softmax(torch.tensor(test_probs) / optimal_temp, dim=1)\n\n# ÌòÑÏû¨Îäî temperature=1.0ÏúºÎ°ú Ïú†ÏßÄ (Í∏∞Î≥∏)\nprint(\"‚úÖ Temperature scalingÏùÄ ÏÑ†ÌÉù ÏÇ¨Ìï≠ÏûÖÎãàÎã§.\")\nprint(\"Í≤ÄÏ¶ù ÏÑ∏Ìä∏Í∞Ä ÏûàÏùÑ Îïå ÏúÑ ÏΩîÎìúÎ•º ÌôúÏö©ÌïòÏó¨ ÏµúÏ†Å temperatureÎ•º Ï∞æÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 12. Ensemble (ÌôïÎ•† ÌèâÍ∑†)\n\n‚úÖ **Probability Averaging** (Ìè¥Î∞±: Majority Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.USE_KFOLD and len(predictions_all) > 1:\n    print(f\"\\n{'='*60}\")\n    print(f\"Ensemble Method: {cfg.ENSEMBLE_METHOD}\")\n    print(f\"{'='*60}\")\n    \n    if cfg.ENSEMBLE_METHOD == 'prob':\n        # ‚úÖ ÌôïÎ•† ÏïôÏÉÅÎ∏î\n        print(\"Using Probability Averaging...\")\n        \n        ensemble_probs = pd.DataFrame({\n            'id': test_df['id'],\n            'prob_a': np.mean([df['prob_a'].values for df in predictions_all], axis=0),\n            'prob_b': np.mean([df['prob_b'].values for df in predictions_all], axis=0),\n            'prob_c': np.mean([df['prob_c'].values for df in predictions_all], axis=0),\n            'prob_d': np.mean([df['prob_d'].values for df in predictions_all], axis=0)\n        })\n        \n        # argmax\n        prob_cols = ['prob_a', 'prob_b', 'prob_c', 'prob_d']\n        ensemble_probs['answer'] = ensemble_probs[prob_cols].values.argmax(axis=1)\n        ensemble_probs['answer'] = ensemble_probs['answer'].map({0: 'a', 1: 'b', 2: 'c', 3: 'd'})\n        \n        final_submission = ensemble_probs[['id', 'answer', 'prob_a', 'prob_b', 'prob_c', 'prob_d']]\n    \n    else:\n        # Majority Voting (Ìè¥Î∞±)\n        print(\"Using Majority Voting...\")\n        \n        ensemble_preds = []\n        for i in range(len(test_df)):\n            votes = [pred.iloc[i]['answer'] for pred in predictions_all]\n            most_common = Counter(votes).most_common(1)[0][0]\n            ensemble_preds.append(most_common)\n        \n        final_submission = pd.DataFrame({\n            'id': test_df['id'],\n            'answer': ensemble_preds\n        })\n    \n    final_path = f\"{cfg.OUTPUT_DIR}/submission_ensemble.csv\"\n    final_submission.to_csv(final_path, index=False)\n    \n    print(f\"‚úÖ Ensemble submission saved to {final_path}\")\n    print(f\"\\nAnswer Distribution:\")\n    print(final_submission['answer'].value_counts().sort_index())\n\nelse:\n    print(\"\\n‚úÖ Single model - No ensemble needed\")\n    final_submission = predictions_all[0]\n    final_path = f\"{cfg.OUTPUT_DIR}/submission_single.csv\"\n    final_submission.to_csv(final_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 13. Í≤∞Í≥º Î∂ÑÏÑù Î∞è ÏãúÍ∞ÅÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n\nanswer_counts = final_submission['answer'].value_counts().sort_index()\nsns.barplot(x=answer_counts.index, y=answer_counts.values, palette='viridis', ax=ax)\nax.set_title('Final Submission Answer Distribution', fontsize=14, weight='bold')\nax.set_xlabel('Answer')\nax.set_ylabel('Count')\nax.grid(axis='y', alpha=0.3)\n\nfor i, (ans, count) in enumerate(answer_counts.items()):\n    percentage = count / len(final_submission) * 100\n    ax.text(i, count + 10, f\"{percentage:.1f}%\", ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n{'='*60}\")\nprint(\"Final Statistics\")\nprint(f\"{'='*60}\")\nprint(f\"Total predictions: {len(final_submission)}\")\nprint(f\"\\nAnswer counts:\")\nfor ans, count in answer_counts.items():\n    print(f\"  {ans}: {count:5d} ({count/len(final_submission)*100:5.1f}%)\")\n\n# ÌôïÎ•† Î∂ÑÌè¨ (ÏûàÎäî Í≤ΩÏö∞)\nif 'prob_a' in final_submission.columns:\n    print(f\"\\n{'='*60}\")\n    print(\"Probability Statistics\")\n    print(f\"{'='*60}\")\n    prob_cols = ['prob_a', 'prob_b', 'prob_c', 'prob_d']\n    print(final_submission[prob_cols].describe())\n\nprint(f\"\\n{'='*60}\")\nprint(\"Sample Predictions\")\nprint(f\"{'='*60}\")\nprint(final_submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ 14. ÏµúÏ¢Ö Ï†ïÎ¶¨\n\n### üéâ ÏôÑÎ£åÎêú ÏûëÏóÖ\n\n1. ‚úÖ Config ÏÑ§Ï†ï (Pro2 ÌäúÎãù)\n2. ‚úÖ Îç∞Ïù¥ÌÑ∞ Î°úÎìú & EDA\n3. ‚úÖ Stratified K-Fold CV\n4. ‚úÖ Dataset & DataCollator (ÎùºÎ≤® ÎßàÏä§ÌÇπ)\n5. ‚úÖ Model & Processor (T4 Ìò∏Ìôò)\n6. ‚úÖ Training Loop (Val Acc + Confusion Matrix)\n7. ‚úÖ Inference (Direct Logits + TTA)\n8. ‚úÖ Temperature Scaling (ÏÑ†ÌÉù ÏÇ¨Ìï≠)\n9. ‚úÖ Ensemble (ÌôïÎ•† ÌèâÍ∑†)\n10. ‚úÖ Í≤∞Í≥º Î∂ÑÏÑù & ÏãúÍ∞ÅÌôî\n\n### üöÄ Ï£ºÏöî Í∞úÏÑ†ÏÇ¨Ìï≠\n\n#### ÌïôÏäµ\n- Val Accuracy Î°úÍπÖ Î∞è Best Î™®Îç∏ Ï†ÄÏû• (Acc Ïö∞ÏÑ†)\n- Confusion Matrix Ï∂úÎ†•\n- ÌïôÏäµ Í≥°ÏÑ† ÏãúÍ∞ÅÌôî\n- Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ train=False (Ï†ïÎãµ Ï£ºÏûÖ Î∞©ÏßÄ)\n\n#### Ï∂îÎ°†\n- Direct Logits: a/b/c/d ÌÜ†ÌÅ∞ ÌôïÎ•† ÏßÅÏ†ë Í≥ÑÏÇ∞\n- TTA: [0.9, 1.0, 1.1] Ïä§ÏºÄÏùº ÌèâÍ∑†\n- pad_token_id ÏûêÎèô Î≥¥Ï†ï\n- ÌôïÎ•† Ïª¨Îüº Ï†ÄÏû•\n\n#### ÏïôÏÉÅÎ∏î\n- ÌôïÎ•† ÏïôÏÉÅÎ∏î (Probability Averaging)\n- Ìè¥Î∞±: Majority Voting\n\n### üìä Pro2 ÏÑ§Ï†ï\n\n```python\nUSE_SAMPLE = False          # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞\nIMAGE_SIZE = 512\nNUM_EPOCHS = 3\nGRAD_ACCUM_STEPS = 8\nWARMUP_RATIO = 0.06\nLORA_R = 16\nUSE_DIRECT_LOGIT_DECODE = True\nTTA_SCALES = [0.9, 1.0, 1.1]\nENSEMBLE_METHOD = 'prob'\n```\n\n### üìå Important Notes\n\n- **ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ†¨**: Î™®Îì† Î™®Îç∏/ÏûÖÎ†•ÏùÑ Îã®Ïùº deviceÎ°ú ÌÜµÏùº\n- **ÎùºÎ≤® ÎßàÏä§ÌÇπ**: ÌîÑÎ°¨ÌîÑÌä∏ ÌÜ†ÌÅ∞ ÏÜêÏã§ Ï†úÏô∏, assistant Ï†ïÎãµÎßå Í∞êÎèÖ\n- **Í≤ÄÏ¶ù ÌîåÎûòÍ∑∏**: valid_dsÏóê train=False Ï†ÅÏö©\n- **Direct Logits**: ÏÉùÏÑ± ÎåÄÎπÑ ÏïàÏ†ïÏ†ÅÏù¥Í≥† Îπ†Î•∏ Ï∂îÎ°†\n- **ÌôïÎ•† ÏïôÏÉÅÎ∏î**: Fold Í∞Ñ ÌôïÎ•† ÌèâÍ∑†ÏúºÎ°ú robustÌïú ÏòàÏ∏°\n\n---\n\n**ü§ñ SSAFY AI Project 2025 - Pro2 Version**\n\n**‚≠ê ÌñâÏö¥ÏùÑ ÎπïÎãàÎã§!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}