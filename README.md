# π“’ Kaggle VQA Challenge - ν†µν•© λ…ΈνΈλ¶ λ²„μ „

## π― ν”„λ΅μ νΈ κ°μ”

Visual Question Answering (VQA) μ±λ¦°μ§€λ¥Ό μ„ν• **λ‹¨μΌ ν†µν•© λ…ΈνΈλ¶** ν”„λ΅μ νΈμ…λ‹λ‹¤.

- **λ¨λΈ**: Qwen2.5-VL (3B/7B) + QLoRA
- **λ©ν‘ μ •ν™•λ„**: 85-88% (Top 10%)
- **ν™κ²½**: T4 GPU μ™„λ²½ νΈν™
- **νΉμ§•**: λ¨λ“  κΈ°λ¥μ΄ ν•λ‚μ λ…ΈνΈλ¶μ— ν†µν•©

## π€ λΉ λ¥Έ μ‹μ‘

### π“’ λ©”μΈ λ…ΈνΈλ¶

**`Kaggle_AllInOne_Pro.ipynb`** - μ „μ²΄ νμ΄ν”„λΌμΈ ν†µν•© λ…ΈνΈλ¶

μ΄ λ…ΈνΈλ¶ ν•λ‚λ΅ λ¨λ“  κ²ƒμ΄ κ°€λ¥ν•©λ‹λ‹¤:
- β… ν™κ²½ μ„¤μ • λ° ν¨ν‚¤μ§€ μ„¤μΉ
- β… Config ν†µν•© κ΄€λ¦¬
- β… λ°μ΄ν„° λ΅λ“ λ° EDA
- β… Stratified K-Fold CV
- β… κ³ κΈ‰ ν•™μµ λ£¨ν”„ (AMP, EMA, SWA, Cosine Warmup)
- β… TTA μ¶”λ΅ 
- β… μ•™μƒλΈ”
- β… μ μ¶ νμΌ μƒμ„±

### π”µ λ² μ΄μ¤λΌμΈ μ°Έκ³ 

**`251023_Baseline.ipynb`** - κ²½μ λ² μ΄μ¤λΌμΈ μ½”λ“ (μ°Έκ³ μ©)

## β¨ μ£Όμ” κΈ°λ¥

### 1. T4 GPU μ™„λ²½ νΈν™
- β… Float16 (BFloat16 λ€μ‹ )
- β… SDPA Attention (FlashAttention μ κ±°)
- β… 4-bit QLoRA
- β… Gradient Checkpointing

### 2. λΌλ²¨ μ •λ ¬ κµμ • (ν•µμ‹¬!)
- β… Assistant λ©”μ‹μ§€μ— μ •λ‹µ ν¬ν•¨
- β… `add_generation_prompt=False` μ‚¬μ©
- β… μ •λ‹µ ν† ν° μ„μΉ μ •ν™•ν• ν•™μµ

### 3. κ³ κΈ‰ ν•™μµ κΈ°λ²•
- β… **AMP** (Automatic Mixed Precision)
- β… **EMA** (Exponential Moving Average)
- β… **SWA** (Stochastic Weight Averaging)
- β… **Cosine Warmup Scheduler**
- β… **Gradient Clipping**

### 4. K-Fold Cross-Validation
- β… Stratified K-Fold (λ‹µλ³€ λ¶„ν¬ μ μ§€)
- β… 3-Fold κΈ°λ³Έ μ„¤μ •
- β… Foldλ³„ λ…λ¦½ ν•™μµ

### 5. TTA & Ensemble
- β… Test-Time Augmentation μ§€μ›
- β… Majority Voting μ•™μƒλΈ”
- β… Weighted Ensemble μµμ…

## π“ μμƒ μ„±λ¥

| μ„¤μ • | μ •ν™•λ„ | ν•™μµ μ‹κ°„ | λ…ΈνΈ |
|------|--------|-----------|------|
| Baseline (200 samples) | 60-65% | ~20min | λΉ λ¥Έ ν…μ¤νΈ |
| Single Fold (3B, full data) | 75-78% | ~2h | λ‹¨μΌ λ¨λΈ |
| 3-Fold Ensemble (3B) | 79-82% | ~6h | μ•™μƒλΈ” |
| 3-Fold Ensemble (7B) | 83-85% | ~12h | κ³ μ„±λ¥ |
| + TTA + Optimization (7B) | 85-88% | ~15h | μµκ³  μ„±λ¥ |

## π—‚οΈ ν”„λ΅μ νΈ κµ¬μ΅°

```
SSAFY_AI_PJT_2025/
β”β”€β”€ π“’ Kaggle_AllInOne_Pro.ipynb    β­ λ©”μΈ ν†µν•© λ…ΈνΈλ¶
β”β”€β”€ π“’ 251023_Baseline.ipynb         μ°Έκ³ μ© λ² μ΄μ¤λΌμΈ
β”β”€β”€ README.md                         μ΄ νμΌ
β”β”€β”€ PROJECT_SUMMARY.md                ν”„λ΅μ νΈ μ”μ•½
β”β”€β”€ requirements.txt                  ν¨ν‚¤μ§€ λ©λ΅
β”β”€β”€ install.sh                        μλ™ μ„¤μΉ μ¤ν¬λ¦½νΈ
β”β”€β”€ data/                             λ°μ΄ν„° ν΄λ”
β”‚   β”β”€β”€ train.csv
β”‚   β”β”€β”€ test.csv
β”‚   β””β”€β”€ sample_submission.csv
β”β”€β”€ experiments/                      μ‹¤ν— κ²°κ³Ό μ €μ¥
β”‚   β””β”€β”€ README.md
β”β”€β”€ checkpoints/                      λ¨λΈ μ²΄ν¬ν¬μΈνΈ (ν•™μµ ν›„ μƒμ„±)
β”β”€β”€ outputs/                          μ μ¶ νμΌ (μ¶”λ΅  ν›„ μƒμ„±)
β””β”€β”€ logs/                             ν•™μµ λ΅κ·Έ (μ„ νƒ)
```

## π“ μ‚¬μ© λ°©λ²•

### 1. ν™κ²½ μ¤€λΉ„ (Colab/Kaggle)

```python
# Kaggle_AllInOne_Pro.ipynbμ μ²« λ²μ§Έ μ½”λ“ μ…€ μ‹¤ν–‰
!pip install -q "transformers>=4.44.2" "accelerate>=0.34.2" "peft>=0.13.2" \
    "bitsandbytes>=0.43.1" datasets pillow pandas torch torchvision \
    scikit-learn matplotlib seaborn tqdm --upgrade
!pip install -q qwen-vl-utils==0.0.8

# λ°νƒ€μ„ μ¬μ‹μ‘
```

### 2. λ°μ΄ν„° μ—…λ΅λ“

Colabμ κ²½μ°:
```python
from google.colab import drive
drive.mount('/content/drive')

# λ°μ΄ν„° μ••μ¶• ν•΄μ 
!unzip "/content/drive/My Drive/data.zip" -d "/content/"
```

Kaggleμ κ²½μ°:
- Add Data β†’ Upload Dataset

### 3. Config μ„¤μ •

λ…ΈνΈλ¶μ Config μ…€μ—μ„ ν•μ΄νΌνλΌλ―Έν„° μ΅°μ •:

```python
class Config:
    # λ¨λΈ μ„¤μ •
    MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"  # λλ” 7B
    IMAGE_SIZE = 384  # 384, 512, 768

    # K-Fold μ„¤μ •
    N_FOLDS = 3
    USE_KFOLD = True

    # ν•™μµ μ„¤μ •
    NUM_EPOCHS = 1  # μ‹¤μ „: 3~5
    BATCH_SIZE = 1
    GRAD_ACCUM_STEPS = 4
    LEARNING_RATE = 1e-4

    # κ³ κΈ‰ κΈ°λ²•
    USE_AMP = True
    USE_EMA = True
    USE_SWA = False
    USE_TTA = False

    # μƒν”λ§ (λ””λ²„κΉ…)
    USE_SAMPLE = True  # False: μ „μ²΄ λ°μ΄ν„°
    SAMPLE_SIZE = 200
```

### 4. μμ°¨ μ‹¤ν–‰

λ…ΈνΈλ¶μ λ¨λ“  μ…€μ„ μμ„λ€λ΅ μ‹¤ν–‰:
1. ν™κ²½ μ„¤μ •
2. Config
3. λ°μ΄ν„° λ΅λ“ & EDA
4. K-Fold μƒμ„±
5. Dataset μ •μ
6. λ¨λΈ λ΅λ“
7. ν•™μµ
8. μ¶”λ΅ 
9. μ•™μƒλΈ”
10. κ²°κ³Ό λ¶„μ„

### 5. μ μ¶

`outputs/submission_ensemble.csv` (λλ” `submission_single.csv`) νμΌμ„ λ‹¤μ΄λ΅λ“ν•μ—¬ μ μ¶

## π”§ ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ°€μ΄λ“

### λΉ λ¥Έ ν…μ¤νΈ (20λ¶„)
```python
USE_SAMPLE = True
SAMPLE_SIZE = 200
NUM_EPOCHS = 1
USE_KFOLD = False
```

### λ‹¨μΌ λ¨λΈ μ‹¤ν— (2μ‹κ°„)
```python
USE_SAMPLE = False
NUM_EPOCHS = 3
USE_KFOLD = False
MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"
```

### 3-Fold μ•™μƒλΈ” (6-12μ‹κ°„)
```python
USE_SAMPLE = False
NUM_EPOCHS = 3
USE_KFOLD = True
N_FOLDS = 3
MODEL_ID = "Qwen/Qwen2.5-VL-7B-Instruct"  # κ³ μ„±λ¥
```

### μµκ³  μ„±λ¥ (15μ‹κ°„)
```python
USE_SAMPLE = False
NUM_EPOCHS = 5
USE_KFOLD = True
N_FOLDS = 3
MODEL_ID = "Qwen/Qwen2.5-VL-7B-Instruct"
IMAGE_SIZE = 512  # λλ” 768
USE_EMA = True
USE_SWA = True
USE_TTA = True
TTA_SCALES = [0.9, 1.0, 1.1]
```

## β οΈ μ¤‘μ” μ‚¬ν•­

### T4 GPU νΈν™μ„±
- **Float16 μ‚¬μ©** (BFloat16 μ•„λ‹) - T4λ” BF16 λ―Έμ§€μ›
- **SDPA Attention** (FlashAttention μ κ±°) - T4 μµμ ν™” λ¶κ°€
- **4-bit Quantization** - λ©”λ¨λ¦¬ ν¨μ¨

### λΌλ²¨ μ •λ ¬ κµμ •
μ΄κ²ƒμ΄ κ°€μ¥ μ¤‘μ”ν• μμ • μ‚¬ν•­μ…λ‹λ‹¤!

β **μλ»λ λ°©λ²•** (ν•™μµ/μ¶”λ΅  λ¶μΌμΉ):
```python
# ν•™μµ μ‹ μ •λ‹µ μ—†μ΄ ν•™μµ
messages = [
    {"role": "user", "content": [...]},
]
text = processor.apply_chat_template(messages, add_generation_prompt=True)
```

β… **μ¬λ°”λ¥Έ λ°©λ²•** (λΌλ²¨ μ •λ ¬):
```python
# ν•™μµ μ‹ μ •λ‹µ ν¬ν•¨
messages = [
    {"role": "user", "content": [...]},
    {"role": "assistant", "content": [{"type": "text", "text": "a"}]}  # μ •λ‹µ!
]
text = processor.apply_chat_template(messages, add_generation_prompt=False)  # False!
```

### μ¬ν„μ„±
- Seed 42λ΅ κ³ μ •
- `torch.backends.cudnn.deterministic = True`

### λ©”λ¨λ¦¬ κ΄€λ¦¬
- Gradient Checkpointing ν™μ„±ν™”
- Batch Size 1 + Gradient Accumulation 4

## π“ FAQ

### Q1: OOM (Out of Memory) μ—λ¬κ°€ λ°μƒν•΄μ”
**A**: λ‹¤μμ„ μ‹λ„ν•μ„Έμ”:
- `BATCH_SIZE = 1`λ΅ κ°μ†
- `IMAGE_SIZE = 384`λ΅ κ°μ†
- `MODEL_ID`λ¥Ό 3Bλ΅ λ³€κ²½
- `USE_EMA = False`, `USE_SWA = False`

### Q2: ν•™μµμ΄ λ„λ¬΄ λλ ¤μ”
**A**:
- `USE_SAMPLE = True`, `SAMPLE_SIZE = 200`μΌλ΅ λΉ λ¥Έ ν…μ¤νΈ
- `NUM_EPOCHS = 1`λ΅ κ°μ†
- `USE_KFOLD = False`λ΅ λ‹¨μΌ λ¨λΈ ν•™μµ

### Q3: μ •ν™•λ„κ°€ λ‚®μ•„μ”
**A**:
- `NUM_EPOCHS` μ¦κ°€ (3~5)
- `MODEL_ID`λ¥Ό 7Bλ΅ λ³€κ²½
- `IMAGE_SIZE` μ¦κ°€ (512, 768)
- `USE_KFOLD = True`λ΅ μ•™μƒλΈ”
- `USE_EMA = True`, `USE_TTA = True`

### Q4: scripts/ ν΄λ”κ°€ μ—†μ–΄μ”
**A**: λ¨λ“  μ½”λ“κ°€ `Kaggle_AllInOne_Pro.ipynb` λ…ΈνΈλ¶μ— ν†µν•©λμ–΄ μμµλ‹λ‹¤. λ³„λ„ μ¤ν¬λ¦½νΈ νμΌμ΄ ν•„μ” μ—†μµλ‹λ‹¤.

## π“ μ°Έκ³  μλ£

- **Qwen2.5-VL κ³µμ‹ λ¬Έμ„**: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct
- **PEFT (LoRA)**: https://huggingface.co/docs/peft
- **Transformers**: https://huggingface.co/docs/transformers

## π“ λ³€κ²½ μ‚¬ν•­ (μ΄μ „ λ²„μ „ λ€λΉ„)

### β… ν†µν•© μ™„λ£
- β `scripts/` ν΄λ” β†’ β… λ…ΈνΈλ¶μ— ν†µν•©
- β `config/` ν΄λ” β†’ β… Config ν΄λμ¤λ΅ ν†µν•©
- β `notebooks/VQA_Training_Complete.ipynb` β†’ β… `Kaggle_AllInOne_Pro.ipynb`λ΅ λ€μ²΄

### β… μ¶”κ°€λ κΈ°λ¥
- β… EMA (Exponential Moving Average)
- β… SWA (Stochastic Weight Averaging)
- β… Cosine Warmup Scheduler
- β… TTA (Test-Time Augmentation)
- β… ν†µν•© Config κ΄€λ¦¬
- β… μλ™ EDA & μ‹κ°ν™”

### β… μ μ§€λ κΈ°λ¥
- β… T4 νΈν™μ„± (Float16, SDPA)
- β… λΌλ²¨ μ •λ ¬ κµμ •
- β… Stratified K-Fold
- β… QLoRA (4-bit)
- β… Gradient Checkpointing

## π― λ‹¤μ λ‹¨κ³„

1. **μ‹¤ν— κ΄€λ¦¬**: `experiments/` ν΄λ”μ— μ‹¤ν— λ΅κ·Έ μ €μ¥
2. **ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”**: Optuna λ“± ν™μ©
3. **μ•™μƒλΈ” κ°μ„ **: Weighted Voting, Stacking
4. **λ°μ΄ν„° μ¦κ°•**: Choice Shuffle, Paraphrase
5. **μ—λ¬ λ¶„μ„**: μμΈ΅ μ‹¤ν¨ μƒν” λ¶„μ„

## π“§ λ¬Έμ

- **GitHub Issues**: ν”„λ΅μ νΈ κ΄€λ ¨ μ§λ¬Έ

---

**π¤– SSAFY AI Project 2025**

**β­ ν–‰μ΄μ„ λΉ•λ‹λ‹¤!**
