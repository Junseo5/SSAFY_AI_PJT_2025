# Kaggle VQA Challenge Solution

## ğŸ¯ Project Overview

Visual Question Answering (VQA) ì±Œë¦°ì§€ ì†”ë£¨ì…˜ìœ¼ë¡œ, ì´ë¯¸ì§€, ì§ˆë¬¸, 4ê°œì˜ ì„ íƒì§€ë¥¼ ì…ë ¥ë°›ì•„ ì •ë‹µ(a/b/c/d) í•˜ë‚˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

- **Target Accuracy**: 85-88% (Top 10%)
- **Model**: Qwen2.5-VL-7B-Instruct (QLoRA 4-bit)
- **Hardware**: T4 GPU Ã— 2 (30GB VRAM)
- **Strategy**: 3-fold Cross-Validation + Ensemble

## âš ï¸ Critical T4 GPU Compatibility Notes

ë³¸ í”„ë¡œì íŠ¸ëŠ” T4 GPU í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ìµœì í™”ë˜ì—ˆìœ¼ë©°, ë‹¤ìŒ ì‚¬í•­ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤:

### 1. BFloat16 â†’ Float16
- **ë¬¸ì œ**: T4ëŠ” BFloat16 ë¯¸ì§€ì› (Ampere SM80+ í•„ìš”)
- **í•´ê²°**: ëª¨ë“  í•™ìŠµ/ì¶”ë¡ ì—ì„œ Float16 ì‚¬ìš©
- **ì˜í–¥**: `bf16=False`, `torch.float16` ì‚¬ìš©

### 2. FlashAttention 2 ì œê±°
- **ë¬¸ì œ**: FA2ëŠ” Ampere ì´ìƒì—ì„œë§Œ ìµœì í™”
- **í•´ê²°**: `attn_implementation="sdpa"` ì‚¬ìš© (ê¸°ë³¸ SDPA)
- **ì˜í–¥**: requirements.txtì—ì„œ flash-attn ì œê±°

### 3. Transformers Git Install
- **ë¬¸ì œ**: PyPI ë²„ì „ì€ Qwen2.5-VL ì§€ì› ë¶€ì¡±
- **í•´ê²°**: Gitì—ì„œ ì§ì ‘ ì„¤ì¹˜
```bash
pip install git+https://github.com/huggingface/transformers.git
```

### 4. Qwen VL Utils í•„ìˆ˜
```bash
pip install qwen-vl-utils[decord]==0.0.8
```

### 5. ë¼ë²¨ ì •ë ¬ êµì • (ê°€ì¥ ì¤‘ìš”!)
- **ë¬¸ì œ**: í•™ìŠµ/ì¶”ë¡  í† í° ìœ„ì¹˜ ë¶ˆì¼ì¹˜
- **í•´ê²°**: Assistant ë©”ì‹œì§€ì— ì •ë‹µ 1ê¸€ì í¬í•¨
```python
messages.append({
    "role": "assistant",
    "content": [{"type": "text", "text": answer}]  # 'a', 'b', 'c', 'd'
})
```

### 6. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í†µì¼
- **ë°©ë²•**: `apply_chat_template` + `process_vision_info` ì‚¬ìš©
- **ì¥ì **: ë²„ì „ í˜¸í™˜ì„±, ì•ˆì •ì„±

## ğŸ“ Project Structure

```
SSAFY_AI_PJT_2025/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt          # T4 í˜¸í™˜ íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ install.sh               # ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ train_config.yaml    # í•™ìŠµ ì„¤ì •
â”‚   â”œâ”€â”€ inference_config.yaml
â”‚   â”œâ”€â”€ prompt_templates.yaml # ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸
â”‚   â””â”€â”€ normalize.yaml       # ì •ê·œí™” ê·œì¹™
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â”œâ”€â”€ sample_submission.csv
â”‚   â””â”€â”€ images/              # ì´ë¯¸ì§€ íŒŒì¼ (ì¶”ê°€ í•„ìš”)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ eda.py               # íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
â”‚   â”œâ”€â”€ normalize.py         # ë‹µë³€ ì •ê·œí™”
â”‚   â”œâ”€â”€ stratified_cv.py     # Stratified K-Fold
â”‚   â”œâ”€â”€ augment.py           # ë°ì´í„° ì¦ê°• (OCR ì œì™¸)
â”‚   â”œâ”€â”€ prompt_manager.py    # í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
â”‚   â”œâ”€â”€ error_handler.py     # ì—ëŸ¬ ì²˜ë¦¬
â”‚   â”œâ”€â”€ memory_optimizer.py  # GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
â”‚   â”œâ”€â”€ baseline_train.py    # ğŸ”µ Baseline í•™ìŠµ (ê°„ë‹¨/ë¹ ë¦„)
â”‚   â”œâ”€â”€ baseline_infer.py    # ğŸ”µ Baseline ì¶”ë¡ 
â”‚   â”œâ”€â”€ train_lora.py        # ğŸŸ¢ Advanced QLoRA í•™ìŠµ (ë¼ë²¨ ì •ë ¬ êµì •)
â”‚   â”œâ”€â”€ infer_forced_choice.py # ğŸŸ¢ Advanced Forced-choice ì¶”ë¡ 
â”‚   â”œâ”€â”€ ensemble.py          # ì•™ìƒë¸” (í™•ë¥  í‰ê· )
â”‚   â””â”€â”€ validate_submission.py # ì œì¶œ íŒŒì¼ ê²€ì¦
â”œâ”€â”€ checkpoints/             # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ outputs/                 # ì œì¶œ íŒŒì¼
â”œâ”€â”€ logs/                    # í•™ìŠµ ë¡œê·¸
â””â”€â”€ notebooks/
    â”œâ”€â”€ 01_eda.ipynb
    â””â”€â”€ 02_vqa_training.ipynb

```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Option 1: ìë™ ì„¤ì¹˜
bash install.sh

# Option 2: ìˆ˜ë™ ì„¤ì¹˜
pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0
pip install git+https://github.com/huggingface/transformers.git
pip install qwen-vl-utils[decord]==0.0.8
pip install -r requirements.txt
```

### 2. WandB Setup

```bash
wandb login
```

### 3. Data Preparation

```bash
# ì´ë¯¸ì§€ íŒŒì¼ì„ data/images/ í´ë”ì— ì¶”ê°€
# EDA ì‹¤í–‰
python scripts/eda.py

# Stratified CV splits ìƒì„±
python scripts/stratified_cv.py
```

### 4. Training (Day 2)

```bash
# 7B ëª¨ë¸ 3-fold í•™ìŠµ
for fold in 0 1 2; do
  python scripts/train_lora.py \
    --model_id Qwen/Qwen2.5-VL-7B-Instruct \
    --fold $fold \
    --output_dir checkpoints/qwen-7b-fold$fold \
    --device cuda:0 \
    --num_epochs 3 \
    --lr 2e-5
done
```

### 5. Inference & Submission (Day 3)

```bash
# ê° foldë³„ ì¶”ë¡ 
for fold in 0 1 2; do
  python scripts/infer_forced_choice.py \
    --model_path checkpoints/qwen-7b-fold$fold/final \
    --test_csv data/test.csv \
    --image_dir data/images \
    --output_csv outputs/submission_fold$fold.csv \
    --device cuda:0
done

# ì•™ìƒë¸”
python scripts/ensemble.py \
  --predictions outputs/submission_fold0.csv outputs/submission_fold1.csv outputs/submission_fold2.csv \
  --weights 0.35 0.35 0.30 \
  --output outputs/submission_ensemble.csv

# ì œì¶œ íŒŒì¼ ê²€ì¦
python scripts/validate_submission.py --file outputs/submission_ensemble.csv
```

## ğŸ¯ Two Workflows: Baseline vs Advanced

This project provides two workflows to suit different needs:

### ğŸ”µ Baseline Workflow (Simple & Fast)

Perfect for quick testing and prototyping. Based on the competition's baseline notebook.

**Features**:
- Simple and straightforward
- Uses `AutoModelForVision2Seq` (compatible with baseline)
- Direct `path` column support (`train/train_0001.jpg`)
- No complex preprocessing
- Fast iteration

**Usage**:
```bash
# Training
python scripts/baseline_train.py \
  --model_id Qwen/Qwen2.5-VL-3B-Instruct \
  --train_csv data/train.csv \
  --data_dir data \
  --output_dir checkpoints/baseline \
  --image_size 384 \
  --batch_size 1 \
  --epochs 1

# Inference
python scripts/baseline_infer.py \
  --model_path checkpoints/baseline \
  --test_csv data/test.csv \
  --data_dir data \
  --output_csv outputs/submission_baseline.csv

# Validation
python scripts/validate_submission.py --file outputs/submission_baseline.csv
```

**When to use**:
- Quick experimentation
- Baseline comparison
- Limited time/resources
- Testing new ideas quickly

### ğŸŸ¢ Advanced Workflow (Optimized for Competition)

Full-featured workflow with all optimizations and critical fixes for maximum performance.

**Features**:
- `Qwen2_5_VLForConditionalGeneration` (latest class)
- Label alignment fix (assistant message)
- Question type-specific prompts (7 types)
- Stratified K-Fold Cross-Validation
- Data augmentation (with OCR protection)
- Ensemble methods (probability averaging)
- Answer normalization
- Memory optimization

**Usage**:
```bash
# 1. EDA & CV Splits
python scripts/eda.py
python scripts/stratified_cv.py

# 2. Training (3-fold)
for fold in 0 1 2; do
  python scripts/train_lora.py \
    --model_id Qwen/Qwen2.5-VL-7B-Instruct \
    --fold $fold \
    --output_dir checkpoints/qwen-7b-fold$fold \
    --device cuda:0 \
    --num_epochs 3 \
    --lr 2e-5
done

# 3. Inference
for fold in 0 1 2; do
  python scripts/infer_forced_choice.py \
    --model_path checkpoints/qwen-7b-fold$fold/final \
    --test_csv data/test.csv \
    --image_dir data/images \
    --output_csv outputs/submission_fold$fold.csv \
    --device cuda:0
done

# 4. Ensemble
python scripts/ensemble.py \
  --predictions outputs/submission_fold*.csv \
  --method weighted \
  --val_accuracies 0.825 0.818 0.822 \
  --output outputs/submission_ensemble.csv

# 5. Validation
python scripts/validate_submission.py --file outputs/submission_ensemble.csv
```

**When to use**:
- Final competition submission
- Maximum accuracy required
- Multi-fold ensemble
- Full pipeline validation

### ğŸ“Š Expected Performance

| Workflow | Accuracy | Training Time | Notes |
|----------|----------|---------------|-------|
| Baseline (3B) | 75-78% | ~2h | Quick baseline |
| Advanced Single (7B) | 79-82% | ~4h/fold | QLoRA optimized |
| Advanced Ensemble (3-fold) | 83-85% | ~12h total | Full pipeline |
| Advanced + Optimization | 85-88% | ~15h total | HP tuning, TTA |

### ğŸ’¡ Data Structure Support

Both workflows support flexible data structures:

```python
# Option 1: 'path' column (baseline style)
# data/train.csv:
# id,path,question,a,b,c,d,answer
# 1,train/train_0001.jpg,ì§ˆë¬¸?,ë³´ê¸°1,ë³´ê¸°2,ë³´ê¸°3,ë³´ê¸°4,a

# Option 2: 'image' column (alternative)
# id,image,question,a,b,c,d,answer
# 1,images/train_0001.jpg,ì§ˆë¬¸?,ë³´ê¸°1,ë³´ê¸°2,ë³´ê¸°3,ë³´ê¸°4,a
```

All scripts automatically detect and handle both formats.

## ğŸ“Š Key Features

### 1. ì§ˆë¬¸ ìœ í˜•ë³„ ìµœì í™” í”„ë¡¬í”„íŠ¸
- **counting**: ê°ì²´ ì¹´ìš´íŒ… ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸
- **color**: ìƒ‰ìƒ ì¸ì‹ ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸
- **ocr**: OCR ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ (í•œê¸€/ì˜ì–´/ìˆ«ì)
- **yesno**: ì‹œê°ì  ì¶”ë¡  ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸
- **general**: ë²”ìš© VQA í”„ë¡¬í”„íŠ¸

### 2. ë°ì´í„° ì¦ê°•
- ë³´ê¸° ìˆœì„œ ì…”í”Œ + ì •ë‹µ ë¼ë²¨ ì—…ë°ì´íŠ¸
- í•œêµ­ì–´ ì§ˆë¬¸ ë³€í˜• (paraphrase)
- ì´ë¯¸ì§€ ì¦ê°• (ë°ê¸°, ëŒ€ë¹„)
- **OCR ì§ˆë¬¸ ì œì™¸**: ë¬¸ì ë°˜ì „ ë°©ì§€

### 3. Stratified K-Fold
- ì§ˆë¬¸ ìœ í˜• ë¹„ìœ¨ ìœ ì§€
- ì •ë‹µ ë¶„í¬ ê· ë“±í™”
- Seed ê³ ì • (ì¬í˜„ì„±)

### 4. QLoRA í•™ìŠµ
- 4-bit quantization (NF4)
- LoRA: r=24, alpha=48
- Language modelë§Œ í•™ìŠµ (Vision encoder ë™ê²°)
- Label smoothing: 0.05
- FP16 precision (T4 í˜¸í™˜)

### 5. Forced-Choice ì¶”ë¡ 
- Logit-based ì˜ˆì¸¡ (a/b/c/d í† í° í™•ë¥ )
- Confidence ê³„ì‚° (margin)
- ì•ˆì „í•œ íŒŒì‹± (fallback í¬í•¨)

### 6. ì•™ìƒë¸”
- 3-fold ê°€ì¤‘ íˆ¬í‘œ
- í™•ë¥  í‰ê·  ë°©ì‹ (ì•ˆì •ì )
- Validation ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •

## ğŸ”¬ Architecture Details

### Model Configuration
```yaml
model: Qwen/Qwen2.5-VL-7B-Instruct
quantization: 4-bit NF4
precision: Float16 (T4 compatible)
attention: SDPA (FlashAttention 2 removed)
lora:
  r: 24
  alpha: 48
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]
```

### Training Configuration
```yaml
batch_size: 4
gradient_accumulation_steps: 2
effective_batch_size: 8
learning_rate: 2e-5
lr_scheduler: cosine
warmup_ratio: 0.05
num_epochs: 3
weight_decay: 0.01
label_smoothing: 0.05
optimizer: paged_adamw_8bit
fp16: true
bf16: false  # T4 unsupported
gradient_checkpointing: true
seed: 42
```

### Inference Configuration
```yaml
resolution:
  min_pixels: 256 * 28 * 28
  max_pixels: 768 * 28 * 28
  high_res: 1024 * 28 * 28  # ì¬ì¶”ë¡ ìš©
generation:
  max_new_tokens: 1
  do_sample: false
  temperature: 0.0
```

## ğŸ“ˆ Expected Performance

| Stage | Accuracy | Notes |
|-------|----------|-------|
| Zero-shot Baseline | 65-68% | í”„ë¡¬í”„íŠ¸ë§Œ |
| Single Fold (7B) | 79-82% | QLoRA í•™ìŠµ |
| 3-Fold Ensemble | 83-85% | ê°€ì¤‘ í‰ê·  |
| + Optimization | 85-88% | HP tuning, TTA |

## ğŸ”§ Troubleshooting

### ë¬¸ì œ 1: ImportError: cannot import 'Qwen2_5_VLForConditionalGeneration'
```bash
pip uninstall transformers
pip install git+https://github.com/huggingface/transformers.git
```

### ë¬¸ì œ 2: ValueError: Bfloat16 is only supported...
- ëª¨ë“  ì½”ë“œì—ì„œ `torch.bfloat16` â†’ `torch.float16` ë³€ê²½
- `bf16=False` í™•ì¸

### ë¬¸ì œ 3: GPU Out of Memory
```python
# batch_size ì¤„ì´ê¸°
per_device_train_batch_size=2  # 4 â†’ 2
gradient_accumulation_steps=4  # 2 â†’ 4
```

### ë¬¸ì œ 4: Validation accuracyê°€ 25% ê·¼ì²˜ (random guess)
- **ì›ì¸**: ë¼ë²¨ ì •ë ¬ ì˜¤ë¥˜
- **í•´ê²°**: `train_lora.py`ì—ì„œ assistant ë©”ì‹œì§€ í¬í•¨ í™•ì¸

## ğŸ“ Reproducibility

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì™„ì „í•œ ì¬í˜„ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤:

- âœ… Random seed ê³ ì •: 42
- âœ… CUDNN deterministic: True
- âœ… Version-locked requirements.txt
- âœ… WandB experiment tracking
- âœ… Stratified CV with seed

## ğŸ“ Reference Documents

í”„ë¡œì íŠ¸ êµ¬í˜„ ì‹œ ì°¸ê³ í•œ ë¬¸ì„œ:
- `FINAL_CORRECTED_Implementation_Prompt.md`: ìµœì¢… ê²€ì¦ ë²„ì „ êµ¬í˜„ ê°€ì´ë“œ
- `VERIFICATION_SUMMARY.md`: 6ê°€ì§€ ì¹˜ëª…ì  ì´ìŠˆ ìˆ˜ì • ì‚¬í•­
- `VQA_Specification_Enhancement.md`: í”„ë¡¬í”„íŠ¸ ì „ëµ, ì—ëŸ¬ ì²˜ë¦¬ ë“±

## ğŸ“„ License

MIT License - see LICENSE file

## ğŸ™ Acknowledgments

- Qwen Team: Qwen2.5-VL ëª¨ë¸
- Hugging Face: Transformers, PEFT
- WandB: Experiment tracking

---

**Generated for SSAFY AI Project 2025**

**Last Updated**: 2025-10-23

**Contact**: GitHub Issues
