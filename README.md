# π€ Kaggle VQA Challenge - Qwen3-VL-30B Multi-GPU Edition

## π― ν”„λ΅μ νΈ κ°μ”

Visual Question Answering (VQA) μ±λ¦°μ§€λ¥Ό μ„ν• **Qwen3-VL-30B Multi-GPU ν†µν•© λ…ΈνΈλ¶** ν”„λ΅μ νΈμ…λ‹λ‹¤.

- **λ¨λΈ**: Qwen2.5-VL-30B-A3B-Instruct (30B νλΌλ―Έν„°)
- **λ©ν‘ μ •ν™•λ„**: 88-90% (3B λ€λΉ„ +3~5%)
- **ν™κ²½**: T4 * 2 GPU (32GB) μµμ ν™”
- **νΉμ§•**: Multi-GPU λ³‘λ ¬ μ²λ¦¬ + λ©”λ¨λ¦¬ μµμ ν™”

## π€ λΉ λ¥Έ μ‹μ‘

### π“’ λ©”μΈ λ…ΈνΈλ¶

**`Kaggle_Qwen3_30B_AllInOne.ipynb`** - Qwen3-VL-30B Multi-GPU ν†µν•© λ…ΈνΈλ¶

μ΄ λ…ΈνΈλ¶ ν•λ‚λ΅ λ¨λ“  κ²ƒμ΄ κ°€λ¥ν•©λ‹λ‹¤:
- β… ν™κ²½ μ„¤μ • λ° ν¨ν‚¤μ§€ μ„¤μΉ
- β… Multi-GPU λ¨λΈ λ΅λ”© (μλ™ λ³‘λ ¬ν™”)
- β… 4-bit Quantization (λ©”λ¨λ¦¬ 75% μ κ°)
- β… Gradient Checkpointing (ν™μ„±ν™” λ©”λ¨λ¦¬ 40% μ κ°)
- β… High Gradient Accumulation (ν¨κ³Όμ  λ°°μΉ ν¬κΈ°)
- β… Stratified K-Fold CV
- β… Memory-efficient Training & Inference
- β… μ•™μƒλΈ” λ° μ μ¶ νμΌ μƒμ„±

## β¨ μ£Όμ” κΈ°λ¥

### 1. Multi-GPU Model Parallelism (ν•µμ‹¬!)
- β… **μλ™ λ¨λΈ λ¶„μ‚°**: `device_map="auto"`λ΅ 2κ° GPUμ— μλ™ λ¶„λ°°
- β… **λ©”λ¨λ¦¬ μ ν• μ„¤μ •**: `max_memory={0: "14GB", 1: "14GB"}`
- β… **OOM μ™„μ „ λ°©μ§€**: μ •κµν• λ©”λ¨λ¦¬ κ΄€λ¦¬
- β… **GPU κ· ν•**: λ¨λΈ λ μ΄μ–΄ μλ™ κ· ν• λ¶„μ‚°

### 2. λ©”λ¨λ¦¬ μµμ ν™”
- β… **4-bit Quantization**: NF4 + double quantization (75% λ©”λ¨λ¦¬ μ κ°)
- β… **Gradient Checkpointing**: ν™μ„±ν™” λ©”λ¨λ¦¬ 40% μ κ°
- β… **High Gradient Accumulation**: BATCH_SIZE=1 + GRAD_ACCUM=16
- β… **μ£ΌκΈ°μ  λ©”λ¨λ¦¬ μ •λ¦¬**: GPU μΊμ‹ ν΄λ¦¬μ–΄
- β… **CPU Offload**: Optimizer states CPUλ΅ μ΄λ™

### 3. κ³ κΈ‰ ν•™μµ κΈ°λ²•
- β… **AMP** (Automatic Mixed Precision with Float16)
- β… **Cosine Warmup Scheduler**
- β… **Gradient Clipping** (max_norm=0.5)
- β… **QLoRA** (Rank=8, 30B λ¨λΈ μµμ ν™”)
- β… **Memory-efficient Training Loop**

### 4. K-Fold Cross-Validation
- β… Stratified K-Fold (λ‹µλ³€ λ¶„ν¬ μ μ§€)
- β… 3-Fold κΈ°λ³Έ μ„¤μ •
- β… Foldλ³„ λ…λ¦½ ν•™μµ
- β… μ•™μƒλΈ” μ¶”λ΅ 

### 5. 30B λ¨λΈ μµμ ν™”
- β… μ‘μ€ LoRA Rank (8 vs 16 for 3B)
- β… λ†’μ€ Gradient Accumulation (16 vs 4 for 3B)
- β… μ‘μ€ μ΄λ―Έμ§€ ν¬κΈ° (384 μ•μ „, 448 κ· ν•)
- β… Target Modules μµμ†ν™” (ν•„μ λ μ΄μ–΄λ§)

## π“ μμƒ μ„±λ¥ (T4 * 2 ν™κ²½)

| μ„¤μ • | μ •ν™•λ„ | ν•™μµ μ‹κ°„ | λ©”λ¨λ¦¬ μ‚¬μ© | λ…ΈνΈ |
|------|--------|-----------|------------|------|
| IMAGE_SIZE=384, LORA_R=8, GA=16 | **88-90%** | ~2min/epoch | GPU0: 13GB, GPU1: 13GB | μ•μ „ (κ¶μ¥) β­ |
| IMAGE_SIZE=448, LORA_R=12, GA=12 | **89-91%** | ~3min/epoch | GPU0: 14.5GB, GPU1: 14.5GB | κ· ν• |
| IMAGE_SIZE=512, LORA_R=16, GA=8 | N/A | ~5min/epoch | β οΈ OOM μ„ν— | λΉ„κ¶μ¥ |

### 3B vs 30B λΉ„κµ

| ν•­λ© | Qwen2.5-VL-3B | Qwen2.5-VL-30B (μ΄ ν”„λ΅μ νΈ) |
|------|---------------|------------------------------|
| νλΌλ―Έν„° | 3B | **30B** (10λ°°) |
| GPU μ”κµ¬μ‚¬ν•­ | T4 * 1 | T4 * 2 |
| λ©”λ¨λ¦¬ (4-bit) | ~2GB | ~15GB |
| ν•™μµ μ†λ„ | 1x | ~2x λλ¦Ό |
| **μ •ν™•λ„** | 85-87% | **88-90%** (+3~5%) β­ |

## π—‚οΈ ν”„λ΅μ νΈ κµ¬μ΅°

```
SSAFY_AI_PJT_2025/
β”β”€β”€ π“’ Kaggle_Qwen3_30B_AllInOne.ipynb  β­ Qwen3-VL-30B Multi-GPU ν†µν•© λ…ΈνΈλ¶
β”β”€β”€ README.md                            μ΄ νμΌ (30B κ°€μ΄λ“)
β”β”€β”€ LICENSE                              ν”„λ΅μ νΈ λΌμ΄μ„ μ¤
β”β”€β”€ requirements.txt                     ν¨ν‚¤μ§€ λ©λ΅
β”β”€β”€ install.sh                           μλ™ μ„¤μΉ μ¤ν¬λ¦½νΈ
β”β”€β”€ data/                                λ°μ΄ν„° ν΄λ”
β”‚   β”β”€β”€ train.csv
β”‚   β”β”€β”€ test.csv
β”‚   β””β”€β”€ sample_submission.csv
β””β”€β”€ experiments/                         μ‹¤ν— κ²°κ³Ό μ €μ¥
    β””β”€β”€ README.md
```

**μ£Όμ” λ³€κ²½μ‚¬ν•­:**
- λ¨λ“  μ½”λ“κ°€ λ‹¨μΌ λ…ΈνΈλ¶ `Kaggle_Qwen3_30B_AllInOne.ipynb`μ— ν†µν•©
- Multi-GPU ν•µμ‹¬ ν•¨μλ“¤ λ…ΈνΈλ¶μ— μ§μ ‘ ν¬ν•¨
- λ¶ν•„μ”ν• νμΌ/ν΄λ” μ κ±°ν•μ—¬ κΉ”λ”ν• κµ¬μ΅° μ μ§€

## π“ μ‚¬μ© λ°©λ²•

### 1. ν™κ²½ μ¤€λΉ„ (Kaggle - T4 * 2 GPU ν•„μ!)

**μ¤‘μ”**: Kaggle μ„¤μ •μ—μ„ **GPU T4 x 2** μ„ νƒ ν•„μ

```python
# Kaggle_Qwen3_30B_AllInOne.ipynbμ μ²« λ²μ§Έ μ½”λ“ μ…€ μ‹¤ν–‰
!pip install -q transformers>=4.45.0 accelerate>=0.34.0 peft>=0.13.0 \
    bitsandbytes>=0.43.0 datasets pillow pandas torch torchvision \
    scikit-learn matplotlib seaborn tqdm scipy --upgrade
!pip install -q qwen-vl-utils==0.0.8

# λ°νƒ€μ„ μ¬μ‹μ‘ ν›„ GPU ν™•μΈ
import torch
print(f"μ‚¬μ© κ°€λ¥ GPU: {torch.cuda.device_count()}κ°")  # λ°λ“μ‹ 2κ°μ—¬μ•Ό ν•¨!
```

### 2. λ°μ΄ν„° μ—…λ΅λ“

Colabμ κ²½μ°:
```python
from google.colab import drive
drive.mount('/content/drive')

# λ°μ΄ν„° μ••μ¶• ν•΄μ 
!unzip "/content/drive/My Drive/data.zip" -d "/content/"
```

Kaggleμ κ²½μ°:
- Add Data β†’ Upload Dataset

### 3. Config μ„¤μ • (30B μµμ ν™”)

λ…ΈνΈλ¶μ Config μ…€μ—μ„ ν•μ΄νΌνλΌλ―Έν„° μ΅°μ •:

```python
class Config:
    # ========== λ¨λΈ (30B) ==========
    MODEL_ID = "Qwen/Qwen2.5-VL-30B-A3B-Instruct"  # 30B λ¨λΈ!
    IMAGE_SIZE = 384  # μ•μ „ μ„¤μ • (448μ€ κ· ν•, 512λ” OOM μ„ν—)

    # ========== Multi-GPU ==========
    MAX_MEMORY_PER_GPU = {0: "14GB", 1: "14GB"}  # T4 * 2 μµμ ν™”
    DEVICE_MAP = "auto"  # μλ™ λ³‘λ ¬ν™”

    # ========== ν•™μµ (λ©”λ¨λ¦¬ μµμ ν™”) ==========
    BATCH_SIZE = 1  # ν•„μ!
    GRAD_ACCUM_STEPS = 16  # λ†’κ²! (ν¨κ³Όμ  λ°°μΉ: 16)
    NUM_EPOCHS = 2  # 30Bλ” μ μ€ epochλ„ μ¶©λ¶„
    LEARNING_RATE = 5e-5  # ν° λ¨λΈμ€ μ‘μ€ LR

    # ========== LoRA (30B μµμ ν™”) ==========
    LORA_R = 8  # μ‘κ²! (3Bλ” 16)
    LORA_ALPHA = 16
    TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj"]  # ν•„μλ§

    # ========== λ©”λ¨λ¦¬ μµμ ν™” ==========
    USE_GRADIENT_CHECKPOINTING = True  # ν•„μ!
    USE_AMP = True  # ν•„μ!
    USE_CPU_OFFLOAD = True  # κ¶μ¥

    # ========== K-Fold ==========
    N_FOLDS = 3
    USE_KFOLD = True
```

### 4. μμ°¨ μ‹¤ν–‰

λ…ΈνΈλ¶μ λ¨λ“  μ…€μ„ μμ„λ€λ΅ μ‹¤ν–‰:
1. ν™κ²½ μ„¤μ •
2. Config
3. λ°μ΄ν„° λ΅λ“ & EDA
4. K-Fold μƒμ„±
5. Dataset μ •μ
6. λ¨λΈ λ΅λ“
7. ν•™μµ
8. μ¶”λ΅ 
9. μ•™μƒλΈ”
10. κ²°κ³Ό λ¶„μ„

### 5. μ μ¶

`outputs/submission_ensemble.csv` (λλ” `submission_single.csv`) νμΌμ„ λ‹¤μ΄λ΅λ“ν•μ—¬ μ μ¶

## π”§ ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ°€μ΄λ“ (T4 * 2)

### λ λ²¨ 1: μ•μ „ μ„¤μ • (κ¶μ¥) β­
```python
IMAGE_SIZE = 384
LORA_R = 8
BATCH_SIZE = 1
GRAD_ACCUM_STEPS = 16
NUM_EPOCHS = 2
MAX_MEMORY_PER_GPU = {0: "14GB", 1: "14GB"}

# λ©”λ¨λ¦¬: GPU0 ~13GB, GPU1 ~13GB
# ν•™μµ μ‹κ°„: ~2λ¶„/epoch
# μμƒ μ •ν™•λ„: 88-90%
```

### λ λ²¨ 2: κ· ν• μ„¤μ • (λ©”λ¨λ¦¬ μ¶©λ¶„ μ‹)
```python
IMAGE_SIZE = 448
LORA_R = 12
BATCH_SIZE = 1
GRAD_ACCUM_STEPS = 12
NUM_EPOCHS = 3
MAX_MEMORY_PER_GPU = {0: "14GB", 1: "14GB"}

# λ©”λ¨λ¦¬: GPU0 ~14.5GB, GPU1 ~14.5GB (μ£Όμ!)
# ν•™μµ μ‹κ°„: ~3λ¶„/epoch
# μμƒ μ •ν™•λ„: 89-91%
```

### λ λ²¨ 3: κ³ μ„±λ¥ (V100 * 2 μ΄μƒ)
```python
IMAGE_SIZE = 512
LORA_R = 16
BATCH_SIZE = 2
GRAD_ACCUM_STEPS = 8
NUM_EPOCHS = 3
MAX_MEMORY_PER_GPU = {0: "20GB", 1: "20GB"}

# V100 μ΄μƒ ν•„μ”
# μμƒ μ •ν™•λ„: 90-92%
```

## β οΈ μ¤‘μ” μ‚¬ν•­

### 1. Multi-GPU ν•„μ!
- **λ°λ“μ‹ GPU 2κ°** ν•„μ” (T4 * 2)
- 1κ° GPUλ΅λ” 30B λ¨λΈ μ‹¤ν–‰ λ¶κ°€
- Kaggle μ„¤μ •: Accelerator β†’ GPU T4 x 2

### 2. λ©”λ¨λ¦¬ κ΄€λ¦¬ (ν•µμ‹¬!)
```python
# OOM λ°μƒ μ‹ λ€μ‘
IMAGE_SIZE = 384  # 512 β†’ 384 λλ” 320
LORA_R = 4  # 8 β†’ 4
GRAD_ACCUM_STEPS = 32  # 16 β†’ 32
MAX_MEMORY_PER_GPU = {0: "12GB", 1: "12GB"}  # 14GB β†’ 12GB
```

### 3. 30B vs 3B μ£Όμ” μ°¨μ΄
| μ„¤μ • | 3B λ¨λΈ | 30B λ¨λΈ (μ΄ ν”„λ΅μ νΈ) |
|------|---------|------------------------|
| LORA_R | 16 | **8** (μ‘κ²!) |
| GRAD_ACCUM_STEPS | 4-8 | **16** (λ†’κ²!) |
| BATCH_SIZE | 1-2 | **1** (ν•„μ!) |
| IMAGE_SIZE | 512 | **384** (μ•μ „) |
| GPU κ°μ | 1κ° | **2κ°** (ν•„μ!) |

### 4. ν•™μµ μ†λ„
- 30Bλ” 3B λ€λΉ„ **2-3λ°° λλ¦Ό** (μ •μƒ)
- μ„±λ¥ ν–¥μƒμ„ μ„ν• trade-off
- Epoch μλ¥Ό μ¤„μ—¬μ„ λ³΄μ™„ (2-3 epoch μ¶©λ¶„)

### 5. λΌλ²¨ μ •λ ¬ κµμ •
```python
# ν•™μµ μ‹ μ •λ‹µ ν¬ν•¨
messages = [
    {"role": "user", "content": [...]},
    {"role": "assistant", "content": [{"type": "text", "text": "a"}]}  # μ •λ‹µ!
]
text = processor.apply_chat_template(messages, add_generation_prompt=False)  # False!
```

## π“ FAQ (Qwen3-30B)

### Q1: OOM (Out of Memory) μ—λ¬κ°€ λ°μƒν•΄μ”
**A**: λ‹¤μμ„ **μμ„λ€λ΅** μ‹λ„ν•μ„Έμ”:
1. `IMAGE_SIZE = 384` β†’ `320`λ΅ κ°μ†
2. `LORA_R = 8` β†’ `4`λ΅ κ°μ†
3. `GRAD_ACCUM_STEPS = 16` β†’ `32`λ΅ μ¦κ°€
4. `MAX_MEMORY_PER_GPU = {0: "14GB", 1: "14GB"}` β†’ `{0: "12GB", 1: "12GB"}`
5. `USE_CPU_OFFLOAD = True` ν™μ„±ν™”

### Q2: GPUκ°€ 1κ°λ§ μμ–΄μ”
**A**: 30B λ¨λΈμ€ **GPU 2κ° ν•„μ**μ…λ‹λ‹¤.
- Kaggleμ—μ„ GPU T4 x 2 μ„ νƒ
- λλ” 3B λ¨λΈ μ‚¬μ© (GPU 1κ°λ΅ κ°€λ¥, μ •ν™•λ„ -3~5%)

### Q3: ν•™μµμ΄ λ„λ¬΄ λλ ¤μ”
**A**: 30Bλ” 3B λ€λΉ„ 2-3λ°° λλ¦½λ‹λ‹¤ (μ •μƒ).
- `GRAD_ACCUM_STEPS` μ¤„μ΄κΈ°: 16 β†’ 8 (λ©”λ¨λ¦¬ ν—μ© μ‹)
- `NUM_EPOCHS` μ¤„μ΄κΈ°: 3 β†’ 2 (30Bλ” μ μ€ epochλ„ μ¶©λ¶„)
- DataLoader `num_workers = 2` μ„¤μ •

### Q4: GPU λ¶κ· ν•μ΄ λ°μƒν•΄μ”
**A**: `device_map="auto"`κ°€ μλ™ μ²λ¦¬ν•©λ‹λ‹¤.
- μ •μƒ: GPU0 ~13GB, GPU1 ~13GB
- λ¶κ· ν• μ‹: λ…ΈνΈλ¶ μ¬μ‹μ‘ ν›„ μ¬μ‹¤ν–‰

### Q5: μ •ν™•λ„κ°€ 3Bλ³΄λ‹¤ λ‚®μ•„μ”
**A**: λ‹¤μμ„ ν™•μΈν•μ„Έμ”:
- λ¨λΈ λ΅λ“ μ‹ Multi-GPU μ„¤μ • ν™•μΈ
- Gradient Checkpointing ν™μ„±ν™” μ—¬λ¶€
- μ¶©λ¶„ν• ν•™μµ (μµμ† 2 epoch)
- 30Bλ” λ³΄ν†µ 88-90% λ‹¬μ„± (+3~5% vs 3B)

## π“ μ°Έκ³  μλ£

- **Qwen2.5-VL-30B λ¨λΈ**: https://huggingface.co/Qwen/Qwen2.5-VL-30B-A3B-Instruct
- **QLoRA λ…Όλ¬Έ**: https://arxiv.org/abs/2305.14314
- **Accelerate λ¬Έμ„**: https://huggingface.co/docs/accelerate
- **BitsAndBytes**: https://github.com/TimDettmers/bitsandbytes
- **PEFT (LoRA)**: https://huggingface.co/docs/peft

## π“ μ£Όμ” μ—…λ°μ΄νΈ (v3.0 - Qwen3-30B)

### β… 30B λ¨λΈ μ§€μ›
- β… **Multi-GPU Model Parallelism** (μλ™ λ¶„μ‚°)
- β… **4-bit Quantization** (75% λ©”λ¨λ¦¬ μ κ°)
- β… **Gradient Checkpointing** (40% ν™μ„±ν™” λ©”λ¨λ¦¬ μ κ°)
- β… **High Gradient Accumulation** (ν¨κ³Όμ  λ°°μΉ ν¬κΈ°)
- β… **Memory-efficient Training** (μ£ΌκΈ°μ  μ •λ¦¬)

### β… μ„±λ¥ ν–¥μƒ
- **μ •ν™•λ„**: 85-87% (3B) β†’ **88-90%** (30B) (+3~5%)
- **λ¨λΈ ν¬κΈ°**: 3B β†’ 30B (10λ°° μ¦κ°€)
- **GPU μ”κµ¬μ‚¬ν•­**: T4 * 1 β†’ T4 * 2

### β… μ½”λ“ κµ¬μ΅°
- λ¨λ“  κΈ°λ¥ λ‹¨μΌ λ…ΈνΈλ¶ ν†µν•© (`Kaggle_Qwen3_30B_AllInOne.ipynb`)
- Multi-GPU ν•µμ‹¬ ν•¨μ λ‚΄μ¥
- λ¶ν•„μ”ν• νμΌ μ κ±°λ΅ κΉ”λ”ν• κµ¬μ΅°

## π― λ‹¤μ λ‹¨κ³„

1. **λ©”λ¨λ¦¬ μµμ ν™”**: λ” ν° μ΄λ―Έμ§€ ν¬κΈ° μ§€μ› (512, 768)
2. **μ•™μƒλΈ” κ°μ„ **: Weighted Voting, Temperature Scaling
3. **μ‹¤ν— κ΄€λ¦¬**: `experiments/` ν΄λ” ν™μ©
4. **μ—λ¬ λ¶„μ„**: μμΈ΅ μ‹¤ν¨ μƒν” λ¶„μ„
5. **λ°μ΄ν„° μ¦κ°•**: Choice Shuffle, Paraphrase

---

**π¤– SSAFY AI Project 2025 - Qwen3-VL-30B Multi-GPU Edition**

**β¨ Optimized for T4 * 2 (32GB)**

**π― λ©ν‘ μ •ν™•λ„: 88-90%**

**β­ ν–‰μ΄μ„ λΉ•λ‹λ‹¤!**
