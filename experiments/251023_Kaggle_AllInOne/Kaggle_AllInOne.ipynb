{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment Snapshot\n",
        "- Name: 251023_Kaggle_AllInOne\n",
        "- Score (Public LB): 0.80452\n",
        "- Baseline: 251023_Baseline (0.76028) — Delta: +0.04424\n",
        "- Model: Qwen/Qwen2.5-VL-3B-Instruct\n",
        "- Image size: 384\n",
        "- Mode: Zero-shot inference (no training)\n",
        "- Date: 2025-10-23\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SSAFY VQA — All‑in‑One Kaggle Notebook\n",
        "\n",
        "This single notebook consolidates setup and inference so you can run on Kaggle with just one `.ipynb`.\n",
        "\n",
        "How to use:\n",
        "- Create a Kaggle Dataset for your data (CSV + images).\n",
        "- Open a Kaggle Notebook, set GPU + Internet On, and Add your data Dataset.\n",
        "- Edit `DATA_DIR` below to match the mount path shown in the left Files panel.\n",
        "- Run zero‑shot inference immediately (no training) to create `submission_baseline.csv`.\n",
        "\n",
        "Notes:\n",
        "- This notebook does not depend on `scripts/` or `config/`.\n",
        "- For meaningful results, your Kaggle Dataset must include the images referenced by the CSV `path` (or `image`) column.\n",
        "- Optional fine‑tuning on Kaggle T4 is resource‑heavy; a simple stub is included but disabled by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User Config\n",
        "import os\n",
        "\n",
        "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
        "\n",
        "# Set your dataset mount directory.\n",
        "# Check the left Files panel in Kaggle after adding your Dataset,\n",
        "# e.g., '/kaggle/input/ssafy-ai-pjt-data' or '/kaggle/input/<your-dataset-name>'.\n",
        "DATA_DIR = '/kaggle/input/ssafy-ai-pjt-data' if IN_KAGGLE else './SSAFY_AI_PJT_2025/data'\n",
        "\n",
        "# CSV paths (the notebook will try both flat and nested 'data/' layouts)\n",
        "TRAIN_CSV = f\"{DATA_DIR}/train.csv\" if os.path.exists(f\"{DATA_DIR}/train.csv\") else f\"{DATA_DIR}/data/train.csv\"\n",
        "TEST_CSV  = f\"{DATA_DIR}/test.csv\"  if os.path.exists(f\"{DATA_DIR}/test.csv\")  else f\"{DATA_DIR}/data/test.csv\"\n",
        "\n",
        "# Outputs\n",
        "CKPT_DIR = '/kaggle/working/checkpoints/baseline' if IN_KAGGLE else './checkpoints/baseline'\n",
        "SUB_CSV  = '/kaggle/working/submission_baseline.csv' if IN_KAGGLE else './submission_baseline.csv'\n",
        "\n",
        "# Model + image size\n",
        "BASE_MODEL_ID = 'Qwen/Qwen2.5-VL-3B-Instruct'\n",
        "IMAGE_SIZE = 384\n",
        "\n",
        "# Run mode\n",
        "DO_TRAIN = False  # set True to try fine-tuning (heavy). Zero‑shot inference is default.\n",
        "SAMPLE_TRAIN_SIZE = 200  # used only if DO_TRAIN\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM = 4\n",
        "LR = 1e-4\n",
        "SEED = 42\n",
        "\n",
        "print('IN_KAGGLE:', IN_KAGGLE)\n",
        "print('DATA_DIR:', DATA_DIR)\n",
        "print('TRAIN_CSV exists:', os.path.exists(TRAIN_CSV))\n",
        "print('TEST_CSV  exists:', os.path.exists(TEST_CSV))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install minimal dependencies (Torch is provided by Kaggle image; keep it)\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    if isinstance(pkgs, str):\n",
        "        pkgs = [pkgs]\n",
        "    cmd = [sys.executable, '-m', 'pip', 'install', '-q'] + pkgs\n",
        "    print('pip install:', ' '.join(pkgs))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Transformers latest for Qwen2.5-VL\n",
        "pip_install(['-U', 'git+https://github.com/huggingface/transformers.git'])\n",
        "# Core deps for I/O + VLM utils\n",
        "pip_install(['qwen-vl-utils[decord]==0.0.8', 'pillow==10.4.0', 'opencv-python==4.10.0',\n",
        "            'pandas==2.2.2', 'numpy==1.26.4', 'tqdm==4.66.4', 'pyyaml==6.0.1',\n",
        "            'ipywidgets==8.1.3', 'matplotlib==3.9.1', 'seaborn==0.13.2'])\n",
        "\n",
        "# Optional: only needed if you plan to try fine‑tuning in this single notebook\n",
        "if DO_TRAIN:\n",
        "    pip_install(['peft==0.12.0', 'bitsandbytes==0.43.3', 'accelerate==0.33.0'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports + environment\n",
        "import os, sys, math, random, re\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForVision2Seq, AutoProcessor, get_linear_schedule_with_warmup\n",
        "\n",
        "os.environ['WANDB_MODE'] = 'disabled'  # disable W&B by default\n",
        "print('CUDA available:', torch.cuda.is_available(), '| torch', torch.__version__)\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt + parsing helpers\n",
        "SYSTEM_INSTRUCT = (\n",
        "    'You are a helpful visual question answering assistant. ' +\n",
        "    'Answer using exactly one letter among a, b, c, or d. No explanation.'\n",
        ")\n",
        "\n",
        "def build_mc_prompt(question, a, b, c, d):\n",
        "    return (\n",
        "        f\"{question}\n\"\n",
        "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
        "        '정답은 반드시 a, b, c, d 중 하나의 소문자만 출력하세요.'\n",
        "    )\n",
        "\n",
        "def parse_answer(text: str) -> str:\n",
        "    t = text.lower()\n",
        "    # If the decoded text contains 'assistant', take the tail\n",
        "    if 'assistant' in t:\n",
        "        t = t.split('assistant')[-1]\n",
        "    m = re.findall(r'\\b([abcd])\\b', t)\n",
        "    return m[0] if m else 'a'  # fallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero‑shot inference (no training)\n",
        "def load_model_and_processor(model_id: str, image_size: int = 384):\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        model_id,\n",
        "        min_pixels=image_size*image_size,\n",
        "        max_pixels=image_size*image_size,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model = AutoModelForVision2Seq.from_pretrained(\n",
        "        model_id,\n",
        "        device_map='auto',\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
        "    )\n",
        "    model.eval()\n",
        "    return processor, model\n",
        "\n",
        "def predict_row(model, processor, row, data_root: str, device: str = 'cuda', image_size: int = 384):\n",
        "    # Build image path\n",
        "    if 'path' in row:\n",
        "        img_path = os.path.join(data_root, row['path'])\n",
        "    elif 'image' in row:\n",
        "        img_path = os.path.join(data_root, row['image'])\n",
        "    else:\n",
        "        raise ValueError(\"No 'path' or 'image' column found\")\n",
        "\n",
        "    try:\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "    except Exception:\n",
        "        # Fallback to a blank image if not found\n",
        "        img = Image.new('RGB', (image_size, image_size), color='white')\n",
        "\n",
        "    user_text = build_mc_prompt(str(row['question']), str(row['a']), str(row['b']), str(row['c']), str(row['d']))\n",
        "\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': [{'type': 'text', 'text': SYSTEM_INSTRUCT}]},\n",
        "        {'role': 'user', 'content': [\n",
        "            {'type': 'image', 'image': img},\n",
        "            {'type': 'text',  'text': user_text}\n",
        "        ]}\n",
        "    ]\n",
        "\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = processor(text=[text], images=[img], padding=True, return_tensors='pt')\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=10, do_sample=False, temperature=0.0)\n",
        "    generated = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    return parse_answer(generated)\n",
        "\n",
        "def run_zero_shot_inference(model_id: str, test_csv: str, data_root: str, output_csv: str, image_size: int = 384):\n",
        "    print('Loading test CSV:', test_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    print('Test samples:', len(test_df))\n",
        "\n",
        "    processor, model = load_model_and_processor(model_id, image_size=image_size)\n",
        "\n",
        "    results = []\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc='Predict'):\n",
        "        ans = predict_row(model, processor, row, data_root, image_size=image_size)\n",
        "        results.append({'id': row['id'], 'answer': ans})\n",
        "\n",
        "    sub = pd.DataFrame(results).sort_values('id').reset_index(drop=True)\n",
        "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "    sub.to_csv(output_csv, index=False)\n",
        "    print('Saved submission to:', output_csv)\n",
        "    return sub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) Very simple fine‑tune stub — disabled by default\n",
        "# This is a minimal baseline loop and may OOM on T4.\n",
        "# Prefer zero‑shot or LoRA training in a dedicated pipeline.\n",
        "def train_minimal(model_id: str, train_csv: str, data_root: str, output_dir: str, image_size: int = 384,\n",
        "                  epochs: int = 1, batch_size: int = 1, grad_accum: int = 4, lr: float = 1e-4,\n",
        "                  sample_n: int | None = None):\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "    class VQADataset(Dataset):\n",
        "        def __init__(self, df, processor, data_dir: str, train: bool = True):\n",
        "            self.df = df.reset_index(drop=True)\n",
        "            self.processor = processor\n",
        "            self.data_dir = data_dir\n",
        "            self.train = train\n",
        "        def __len__(self):\n",
        "            return len(self.df)\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.df.iloc[idx]\n",
        "            img_rel = row['path'] if 'path' in row else row.get('image', '')\n",
        "            img_path = os.path.join(self.data_dir, img_rel)\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "            except Exception:\n",
        "                img = Image.new('RGB', (image_size, image_size), color='white')\n",
        "            user_text = build_mc_prompt(str(row['question']), str(row['a']), str(row['b']), str(row['c']), str(row['d']))\n",
        "            messages = [\n",
        "                {'role': 'system', 'content': [{'type': 'text', 'text': SYSTEM_INSTRUCT}]},\n",
        "                {'role': 'user',   'content': [\n",
        "                    {'type': 'image', 'image': img},\n",
        "                    {'type': 'text',  'text': user_text}\n",
        "                ]}\n",
        "            ]\n",
        "            if self.train and 'answer' in row:\n",
        "                messages.append({'role': 'assistant', 'content': [{'type': 'text', 'text': str(row['answer']).strip().lower()}]})\n",
        "            return {'messages': messages, 'image': img}\n",
        "\n",
        "    class Collator:\n",
        "        def __init__(self, processor):\n",
        "            self.processor = processor\n",
        "        def __call__(self, batch):\n",
        "            texts, images = [], []\n",
        "            for sample in batch:\n",
        "                text = self.processor.apply_chat_template(sample['messages'], tokenize=False, add_generation_prompt=False)\n",
        "                texts.append(text)\n",
        "                images.append(sample['image'])\n",
        "            enc = self.processor(text=texts, images=images, padding=True, return_tensors='pt')\n",
        "            enc['labels'] = enc['input_ids'].clone()  # naive labels on full sequence\n",
        "            if torch.cuda.is_available():\n",
        "                enc = {k: v.to('cuda') for k, v in enc.items()}\n",
        "            return enc\n",
        "\n",
        "    print('Loading training CSV:', train_csv)\n",
        "    df = pd.read_csv(train_csv)\n",
        "    if sample_n is not None:\n",
        "        df = df.sample(n=min(sample_n, len(df)), random_state=SEED).reset_index(drop=True)\n",
        "    print('Train samples:', len(df))\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_id, min_pixels=image_size*image_size, max_pixels=image_size*image_size, trust_remote_code=True)\n",
        "    model = AutoModelForVision2Seq.from_pretrained(model_id, device_map='auto', trust_remote_code=True, torch_dtype=torch.float16 if torch.cuda.is_available() else None)\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to('cuda')\n",
        "    model.train()\n",
        "\n",
        "    ds = VQADataset(df, processor, data_root, train=True)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=Collator(processor))\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    steps = epochs * math.ceil(len(dl) / max(1, grad_accum))\n",
        "    sched = get_linear_schedule_with_warmup(optim, int(steps*0.03), steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    global_step = 0\n",
        "    for epoch in range(epochs):\n",
        "        running = 0.0\n",
        "        for i, batch in enumerate(tqdm(dl, desc=f'Epoch {epoch+1}')):\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.float16 if torch.cuda.is_available() else None):\n",
        "                out = model(**batch)\n",
        "                loss = out.loss / max(1, grad_accum)\n",
        "            scaler.scale(loss).backward()\n",
        "            running += float(loss.item())\n",
        "            if (i + 1) % max(1, grad_accum) == 0:\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "                optim.zero_grad(set_to_none=True)\n",
        "                sched.step()\n",
        "                global_step += 1\n",
        "        print(f'epoch {epoch+1} done')\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model.save_pretrained(output_dir)\n",
        "    processor.save_pretrained(output_dir)\n",
        "    print('Saved checkpoint to:', output_dir)\n",
        "    return output_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run — zero‑shot by default\n",
        "if DO_TRAIN:\n",
        "    print('Fine‑tuning is enabled — this may OOM on T4.\\nTry SAMPLE_TRAIN_SIZE small first or keep DO_TRAIN=False.')\n",
        "    _ = train_minimal(\n",
        "        model_id=BASE_MODEL_ID,\n",
        "        train_csv=TRAIN_CSV,\n",
        "        data_root=DATA_DIR,\n",
        "        output_dir=CKPT_DIR,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        epochs=EPOCHS, batch_size=BATCH_SIZE, grad_accum=GRAD_ACCUM, lr=LR,\n",
        "        sample_n=SAMPLE_TRAIN_SIZE\n",
        "    )\n",
        "else:\n",
        "    print('Running zero‑shot inference (no training)...')\n",
        "    sub = run_zero_shot_inference(\n",
        "        model_id=BASE_MODEL_ID,\n",
        "        test_csv=TEST_CSV,\n",
        "        data_root=DATA_DIR,\n",
        "        output_csv=SUB_CSV,\n",
        "        image_size=IMAGE_SIZE\n",
        "    )\n",
        "    display(sub.head())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
