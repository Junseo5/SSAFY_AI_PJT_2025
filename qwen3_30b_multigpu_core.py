"""
Qwen3-VL-30B Multi-GPU í•µì‹¬ ë¡œì§
T4 * 2 (32GB) í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ 30B ëª¨ë¸ ì‹¤í–‰

ì£¼ìš” ê¸°ëŠ¥:
1. Multi-GPU Model Parallelism
2. 4-bit Quantization with QLoRA
3. Memory-efficient Training
4. Parallel Inference
"""

import torch
import torch.nn.functional as F
from transformers import (
    Qwen2VLForConditionalGeneration,
    AutoProcessor,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from accelerate import Accelerator
import logging
import gc
from typing import Dict, List, Optional
from PIL import Image

logger = logging.getLogger('VQA_30B')

# ============================================================================
# 1. Multi-GPU ëª¨ë¸ ë¡œë“œ (í•µì‹¬!)
# ============================================================================

def create_model_and_processor_multigpu(
    model_id: str,
    image_size: int = 384,
    lora_r: int = 8,
    lora_alpha: int = 16,
    lora_dropout: float = 0.05,
    target_modules: List[str] = None,
    max_memory_per_gpu: Dict[int, str] = None,
    use_gradient_checkpointing: bool = True,
    logger = None
):
    """
    Multi-GPU í™˜ê²½ì—ì„œ 30B ëª¨ë¸ ë¡œë“œ

    Args:
        model_id: ëª¨ë¸ ID (Qwen/Qwen2.5-VL-30B-A3B-Instruct)
        image_size: ì´ë¯¸ì§€ í¬ê¸°
        lora_r: LoRA rank
        lora_alpha: LoRA alpha
        lora_dropout: LoRA dropout
        target_modules: LoRA target modules
        max_memory_per_gpu: GPUë‹¹ ìµœëŒ€ ë©”ëª¨ë¦¬ {0: "14GB", 1: "14GB"}
        use_gradient_checkpointing: Gradient checkpointing ì‚¬ìš© ì—¬ë¶€
        logger: ë¡œê±°

    Returns:
        (model, processor)
    """
    if logger:
        logger.info("ğŸ”§ Multi-GPU ëª¨ë¸ ë¡œë“œ ì‹œì‘...")

    # GPU í™•ì¸
    if not torch.cuda.is_available():
        raise RuntimeError("GPUê°€ í•„ìš”í•©ë‹ˆë‹¤!")

    gpu_count = torch.cuda.device_count()
    if logger:
        logger.info(f"   ì‚¬ìš© ê°€ëŠ¥ GPU: {gpu_count}ê°œ")

    # ê¸°ë³¸ max_memory ì„¤ì •
    if max_memory_per_gpu is None:
        if gpu_count >= 2:
            max_memory_per_gpu = {0: "14GB", 1: "14GB"}
        else:
            max_memory_per_gpu = {0: "14GB"}

    # 4-bit Quantization ì„¤ì • (í•„ìˆ˜!)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,  # Double quantization
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,  # T4ëŠ” BF16 ë¯¸ì§€ì›
    )

    if logger:
        logger.info(f"   4-bit Quantization ì„¤ì • ì™„ë£Œ")
        logger.info(f"   Max memory per GPU: {max_memory_per_gpu}")

    # Processor ë¡œë“œ
    try:
        processor = AutoProcessor.from_pretrained(
            model_id,
            min_pixels=image_size * image_size,
            max_pixels=image_size * image_size,
            trust_remote_code=True,
        )
        if logger:
            logger.info("âœ… Processor ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        if logger:
            logger.error(f"âŒ Processor ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

    # ëª¨ë¸ ë¡œë“œ with Multi-GPU
    try:
        if logger:
            logger.info("   Base model ë¡œë“œ ì¤‘...")

        # device_map="auto"ë¡œ ìë™ ë³‘ë ¬í™”
        base_model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto",  # í•µì‹¬! ìë™ìœ¼ë¡œ ì—¬ëŸ¬ GPUì— ë¶„ì‚°
            max_memory=max_memory_per_gpu,  # GPUë‹¹ ìµœëŒ€ ë©”ëª¨ë¦¬
            trust_remote_code=True,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,  # CPU ë©”ëª¨ë¦¬ ì ˆì•½
        )

        if logger:
            logger.info("âœ… Base model ë¡œë“œ ì™„ë£Œ")
            # ëª¨ë¸ì´ ì–´ëŠ GPUì— ë°°ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if hasattr(base_model, 'hf_device_map'):
                logger.info(f"   Device map: {base_model.hf_device_map}")

    except Exception as e:
        if logger:
            logger.error(f"âŒ Model ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

    # Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
    if use_gradient_checkpointing:
        base_model.gradient_checkpointing_enable()
        if logger:
            logger.info("âœ… Gradient checkpointing í™œì„±í™”")

    # QLoRA ì¤€ë¹„
    base_model = prepare_model_for_kbit_training(base_model)

    # LoRA Config
    if target_modules is None:
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        bias="none",
        target_modules=target_modules,
        task_type="CAUSAL_LM",
    )

    # PEFT ëª¨ë¸ ìƒì„±
    model = get_peft_model(base_model, lora_config)

    if logger:
        model.print_trainable_parameters()
        logger.info("âœ… QLoRA ëª¨ë¸ ìƒì„± ì™„ë£Œ")

    # ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
    if logger:
        print_gpu_memory_status(logger)

    return model, processor


def print_gpu_memory_status(logger):
    """ëª¨ë“  GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥"""
    if not torch.cuda.is_available():
        return

    logger.info("="*60)
    logger.info("ğŸ’¾ GPU Memory Status")
    logger.info("="*60)

    for i in range(torch.cuda.device_count()):
        allocated = torch.cuda.memory_allocated(i) / 1e9
        reserved = torch.cuda.memory_reserved(i) / 1e9
        total = torch.cuda.get_device_properties(i).total_memory / 1e9
        usage_pct = (allocated / total) * 100
        logger.info(
            f"GPU {i}: {allocated:.2f}GB / {total:.1f}GB ({usage_pct:.1f}%) | "
            f"Reserved: {reserved:.2f}GB"
        )

    logger.info("="*60)


def clear_gpu_memory():
    """ëª¨ë“  GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            with torch.cuda.device(i):
                torch.cuda.empty_cache()
                torch.cuda.synchronize()


# ============================================================================
# 2. Memory-Efficient Training Loop
# ============================================================================

def train_one_epoch_memory_efficient(
    model,
    train_loader,
    optimizer,
    scheduler,
    scaler,
    grad_accum_steps: int,
    max_grad_norm: float,
    device,
    logger=None
):
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ ë£¨í”„

    Args:
        model: ëª¨ë¸
        train_loader: ë°ì´í„° ë¡œë”
        optimizer: ì˜µí‹°ë§ˆì´ì €
        scheduler: ìŠ¤ì¼€ì¤„ëŸ¬
        scaler: AMP scaler
        grad_accum_steps: Gradient accumulation steps
        max_grad_norm: Max gradient norm
        device: ë””ë°”ì´ìŠ¤
        logger: ë¡œê±°

    Returns:
        average_loss
    """
    model.train()
    total_loss = 0.0
    steps = 0

    for batch_idx, batch in enumerate(tqdm(train_loader, desc="Training")):
        # ë°ì´í„°ë¥¼ deviceë¡œ ì´ë™ (multi-GPUì˜ ê²½ìš° ìë™ ì²˜ë¦¬)
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                 for k, v in batch.items()}

        # Forward with AMP
        with torch.amp.autocast('cuda', enabled=True, dtype=torch.float16):
            outputs = model(**batch)
            loss = outputs.loss / grad_accum_steps

        # Backward
        scaler.scale(loss).backward()
        total_loss += loss.item() * grad_accum_steps

        # Gradient accumulation
        if (batch_idx + 1) % grad_accum_steps == 0:
            # Gradient clipping
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

            # Optimizer step
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)  # ë©”ëª¨ë¦¬ ì ˆì•½

            scheduler.step()
            steps += 1

            # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬
            if steps % 50 == 0:
                clear_gpu_memory()

    avg_loss = total_loss / len(train_loader)
    return avg_loss


# ============================================================================
# 3. Parallel Inference (Multi-GPU)
# ============================================================================

def infer_parallel(
    model,
    processor,
    test_df,
    data_dir: str,
    img_col: str = 'path',
    system_instruct: str = "",
    logger=None
):
    """
    Multi-GPU ë³‘ë ¬ ì¶”ë¡ 

    Args:
        model: ëª¨ë¸ (ì´ë¯¸ Multi-GPUì— ë¶„ì‚°ë¨)
        processor: Processor
        test_df: Test ë°ì´í„°í”„ë ˆì„
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        img_col: ì´ë¯¸ì§€ ì»¬ëŸ¼ëª…
        system_instruct: System instruction
        logger: ë¡œê±°

    Returns:
        predictions: List of predictions
    """
    model.eval()
    predictions = []

    # Choice token IDs ì¶”ì¶œ
    choice_tokens = get_choice_token_ids_robust(processor)

    with torch.no_grad():
        for idx in tqdm(range(len(test_df)), desc="Inference"):
            row = test_df.iloc[idx]

            # ì´ë¯¸ì§€ ë¡œë“œ
            img_path = f"{data_dir}/{row[img_col]}"
            try:
                img = Image.open(img_path).convert("RGB")
            except:
                img = Image.new('RGB', (384, 384), color='white')

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            question_text = build_mc_prompt(
                row['question'], row['a'], row['b'], row['c'], row['d']
            )

            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_instruct}]},
                {"role": "user", "content": [
                    {"type": "image", "image": img},
                    {"type": "text", "text": question_text}
                ]}
            ]

            # Processorë¡œ ì²˜ë¦¬
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text], images=[img], return_tensors="pt")

            # ì…ë ¥ì„ ì ì ˆí•œ deviceë¡œ ì´ë™ (ì²« ë²ˆì§¸ GPU)
            inputs = {k: v.to("cuda:0") for k, v in inputs.items()}

            # Forward
            outputs = model(**inputs)
            logits = outputs.logits[0, -1, :]  # Last token logits

            # Choice í™•ë¥  ê³„ì‚°
            probs = extract_choice_probs_from_logits(logits, choice_tokens)
            pred = max(probs, key=probs.get)

            predictions.append(pred)

            # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬
            if (idx + 1) % 100 == 0:
                clear_gpu_memory()

    return predictions


# ============================================================================
# 4. Helper Functions
# ============================================================================

def build_mc_prompt(question, a, b, c, d):
    """Multiple choice í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return (
        f"{question}\\n"
        f"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n"
        "ì •ë‹µì„ ë°˜ë“œì‹œ a, b, c, d ì¤‘ í•˜ë‚˜ì˜ ì†Œë¬¸ì í•œ ê¸€ìë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."
    )


def get_choice_token_ids_robust(processor):
    """Choice token IDs ì¶”ì¶œ (ì—¬ëŸ¬ ë³€í˜• ê³ ë ¤)"""
    choice_tokens = {}
    for choice in ['a', 'b', 'c', 'd']:
        variants = [choice, f" {choice}", f"{choice} ", choice.upper()]
        all_token_ids = set()
        for variant in variants:
            try:
                token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)
                all_token_ids.update(token_ids)
            except:
                pass
        choice_tokens[choice] = list(all_token_ids)
    return choice_tokens


def extract_choice_probs_from_logits(logits, choice_tokens):
    """Logitsì—ì„œ choice í™•ë¥  ì¶”ì¶œ"""
    choice_logits = {}
    for choice, token_ids in choice_tokens.items():
        if len(token_ids) > 0:
            max_logit = max([logits[tid].item() for tid in token_ids])
            choice_logits[choice] = max_logit
        else:
            choice_logits[choice] = -float('inf')

    # Softmax
    logit_values = torch.tensor(list(choice_logits.values()))
    probs = F.softmax(logit_values, dim=0).numpy()

    return {choice: probs[i] for i, choice in enumerate(['a', 'b', 'c', 'd'])}


# ============================================================================
# 5. Accelerate í†µí•© (Advanced)
# ============================================================================

def setup_accelerator(
    gradient_accumulation_steps: int = 16,
    mixed_precision: str = "fp16",
    cpu_offload: bool = True
):
    """
    Accelerate ì„¤ì •

    Args:
        gradient_accumulation_steps: Gradient accumulation steps
        mixed_precision: Mixed precision ("fp16", "bf16", "no")
        cpu_offload: CPU offload ì‚¬ìš© ì—¬ë¶€

    Returns:
        Accelerator instance
    """
    from accelerate import Accelerator

    accelerator = Accelerator(
        gradient_accumulation_steps=gradient_accumulation_steps,
        mixed_precision=mixed_precision,
        log_with="tensorboard",
        cpu=cpu_offload,
    )

    return accelerator


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    print("="*60)
    print("Qwen3-VL-30B Multi-GPU Core Functions")
    print("="*60)
    print()
    print("ì£¼ìš” í•¨ìˆ˜:")
    print("1. create_model_and_processor_multigpu() - Multi-GPU ëª¨ë¸ ë¡œë“œ")
    print("2. train_one_epoch_memory_efficient() - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ")
    print("3. infer_parallel() - ë³‘ë ¬ ì¶”ë¡ ")
    print("4. print_gpu_memory_status() - GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§")
    print("5. clear_gpu_memory() - GPU ë©”ëª¨ë¦¬ ì •ë¦¬")
    print()
    print("="*60)
