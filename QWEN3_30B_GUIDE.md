# ğŸš€ Qwen3-VL-30B Multi-GPU ì™„ì „ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

T4 * 2 (ì´ 32GB) í™˜ê²½ì—ì„œ Qwen3-VL-30B (30B íŒŒë¼ë¯¸í„°) ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

### ğŸ¯ í•µì‹¬ ì „ëµ

| ê¸°ìˆ  | ëª©ì  | ë©”ëª¨ë¦¬ ì ˆê° |
|------|------|-----------|
| **4-bit Quantization** | ëª¨ë¸ í¬ê¸° ì¶•ì†Œ | ~75% â†“ |
| **Model Parallelism** | ì—¬ëŸ¬ GPUì— ë¶„ì‚° | 2x ê°€ìš© ë©”ëª¨ë¦¬ |
| **Gradient Checkpointing** | í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆì•½ | ~40% â†“ |
| **Gradient Accumulation** | ì‘ì€ ë°°ì¹˜ ë³´ì™„ | ë©”ëª¨ë¦¬ ì˜í–¥ ì—†ìŒ |
| **CPU Offload** | Optimizer states CPUë¡œ | ~20% â†“ |

### âš–ï¸ ë©”ëª¨ë¦¬ ê³„ì‚°

```
30B ëª¨ë¸:
- FP32: ~120GB (ë¶ˆê°€ëŠ¥)
- FP16: ~60GB (ë¶ˆê°€ëŠ¥)
- 4-bit: ~15GB (ê°€ëŠ¥!)

T4 * 2 í™˜ê²½:
- GPU 0: 16GB (ì‹¤ì œ ì‚¬ìš© ~14GB)
- GPU 1: 16GB (ì‹¤ì œ ì‚¬ìš© ~14GB)
- ì´: 28GB ê°€ìš©

ëª¨ë¸ ë¶„ì‚°:
- GPU 0: ~7.5GB (ëª¨ë¸ ì¼ë¶€)
- GPU 1: ~7.5GB (ëª¨ë¸ ë‚˜ë¨¸ì§€)
- ì—¬ìœ : ~13GB (í™œì„±í™”, gradient ë“±)
```

## ğŸ“¦ ì œê³µ íŒŒì¼

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| **qwen3_30b_multigpu_core.py** | í•µì‹¬ ë¡œì§ (ëª¨ë¸ ë¡œë“œ, í•™ìŠµ, ì¶”ë¡ ) |
| **Kaggle_Qwen3_30B_MultiGPU.ipynb** | ì‹¤í–‰ ë…¸íŠ¸ë¶ |
| **QWEN3_30B_GUIDE.md** | ì´ íŒŒì¼ - ì™„ì „ ê°€ì´ë“œ |

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### Step 1: í™˜ê²½ í™•ì¸

```bash
# GPU í™•ì¸
nvidia-smi

# í•„ìˆ˜: T4 * 2 ì´ìƒ
# GPU 0: NVIDIA Tesla T4 (16GB)
# GPU 1: NVIDIA Tesla T4 (16GB)
```

### Step 2: íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install transformers>=4.45.0 accelerate>=0.34.0 peft>=0.13.0
pip install bitsandbytes>=0.43.0 torch torchvision
pip install datasets pillow pandas scikit-learn tqdm scipy
pip install qwen-vl-utils==0.0.8
```

### Step 3: ì½”ë“œ ì‚¬ìš©

```python
from qwen3_30b_multigpu_core import (
    create_model_and_processor_multigpu,
    train_one_epoch_memory_efficient,
    infer_parallel,
    print_gpu_memory_status,
    clear_gpu_memory
)
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger('VQA_30B')
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
logger.addHandler(handler)

# ëª¨ë¸ ë¡œë“œ
model, processor = create_model_and_processor_multigpu(
    model_id="Qwen/Qwen2.5-VL-30B-A3B-Instruct",
    image_size=384,
    lora_r=8,
    lora_alpha=16,
    max_memory_per_gpu={0: "14GB", 1: "14GB"},
    use_gradient_checkpointing=True,
    logger=logger
)

# GPU ë©”ëª¨ë¦¬ í™•ì¸
print_gpu_memory_status(logger)
```

## ğŸ”§ í•µì‹¬ ì„¤ì • ê°€ì´ë“œ

### Config ìµœì í™”

```python
class Config:
    # ========== ëª¨ë¸ ==========
    MODEL_ID = "Qwen/Qwen2.5-VL-30B-A3B-Instruct"
    IMAGE_SIZE = 384  # 512ëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥

    # ========== í•™ìŠµ (ë©”ëª¨ë¦¬ ìµœì í™”) ==========
    BATCH_SIZE = 1  # í•„ìˆ˜!
    GRAD_ACCUM_STEPS = 16  # íš¨ê³¼ì  ë°°ì¹˜: 16
    NUM_EPOCHS = 2  # 30BëŠ” ì ì€ epochë„ ì¶©ë¶„
    LEARNING_RATE = 5e-5  # í° ëª¨ë¸ì€ ì‘ì€ LR

    # ========== LoRA (ë©”ëª¨ë¦¬ ìµœì í™”) ==========
    LORA_R = 8  # 30Bì—ëŠ” ì‘ì€ rank
    LORA_ALPHA = 16
    TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj"]  # í•„ìˆ˜ë§Œ

    # ========== ë©”ëª¨ë¦¬ ìµœì í™” ==========
    USE_GRADIENT_CHECKPOINTING = True  # í•„ìˆ˜!
    USE_AMP = True  # í•„ìˆ˜!
    USE_CPU_OFFLOAD = True  # ê¶Œì¥
    USE_EMA = False  # ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë¹„í™œì„±í™”
    USE_SWA = False  # ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë¹„í™œì„±í™”

    # ========== Multi-GPU ==========
    MAX_MEMORY_PER_GPU = {0: "14GB", 1: "14GB"}
    DEVICE_MAP = "auto"  # ìë™ ë³‘ë ¬í™”
```

### ë©”ëª¨ë¦¬ ë ˆë²¨ë³„ ì„¤ì •

#### ë ˆë²¨ 1: ì•ˆì „ (14GB + 14GB)
```python
IMAGE_SIZE = 384
LORA_R = 8
BATCH_SIZE = 1
GRAD_ACCUM_STEPS = 16
```

#### ë ˆë²¨ 2: ê· í˜• (ë©”ëª¨ë¦¬ ì¶©ë¶„ ì‹œ)
```python
IMAGE_SIZE = 448
LORA_R = 12
BATCH_SIZE = 1
GRAD_ACCUM_STEPS = 12
```

#### ë ˆë²¨ 3: ê³ ì„±ëŠ¥ (V100 * 2 ì´ìƒ)
```python
IMAGE_SIZE = 512
LORA_R = 16
BATCH_SIZE = 2
GRAD_ACCUM_STEPS = 8
```

## ğŸ“ ì „ì²´ ì›Œí¬í”Œë¡œìš°

### 1. ëª¨ë¸ ë¡œë“œ

```python
import torch
import logging
from qwen3_30b_multigpu_core import (
    create_model_and_processor_multigpu,
    clear_gpu_memory
)

# ë¡œê±° ì„¤ì •
logger = logging.getLogger('VQA_30B')
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(handler)

# ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬
clear_gpu_memory()

# ëª¨ë¸ ë¡œë“œ
logger.info("ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
model, processor = create_model_and_processor_multigpu(
    model_id="Qwen/Qwen2.5-VL-30B-A3B-Instruct",
    image_size=384,
    lora_r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    max_memory_per_gpu={0: "14GB", 1: "14GB"},
    use_gradient_checkpointing=True,
    logger=logger
)

logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
```

### 2. ë°ì´í„° ì¤€ë¹„

```python
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
import unicodedata

class VQADataset(Dataset):
    def __init__(self, df, processor, data_dir, train=True):
        self.df = df.reset_index(drop=True)
        self.processor = processor
        self.data_dir = data_dir
        self.train = train

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]

        # ì´ë¯¸ì§€ ë¡œë“œ
        img_path = f"{self.data_dir}/{row['path']}"
        try:
            img = Image.open(img_path).convert("RGB")
        except:
            img = Image.new('RGB', (384, 384), color='white')

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        question_text = (
            f"{row['question']}\\n"
            f"(a) {row['a']}\\n(b) {row['b']}\\n(c) {row['c']}\\n(d) {row['d']}\\n\\n"
            "ì •ë‹µì„ ë°˜ë“œì‹œ a, b, c, d ì¤‘ í•˜ë‚˜ì˜ ì†Œë¬¸ì í•œ ê¸€ìë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."
        )

        messages = [
            {"role": "system", "content": [{"type": "text", "text": "You are a helpful VQA assistant."}]},
            {"role": "user", "content": [
                {"type": "image", "image": img},
                {"type": "text", "text": question_text}
            ]}
        ]

        if self.train:
            answer = str(row["answer"]).strip().lower()
            messages.append({
                "role": "assistant",
                "content": [{"type": "text", "text": answer}]
            })

        return {"messages": messages, "image": img, "answer": row.get("answer")}


@dataclass
class DataCollator:
    processor: Any
    train: bool = True

    def __call__(self, batch):
        texts, images, answers = [], [], []

        for sample in batch:
            text = self.processor.apply_chat_template(
                sample["messages"],
                tokenize=False,
                add_generation_prompt=False
            )
            text = unicodedata.normalize('NFKC', text)
            texts.append(text)
            images.append(sample["image"])
            answers.append(sample["answer"])

        enc = self.processor(
            text=texts,
            images=images,
            padding=True,
            return_tensors="pt"
        )

        if self.train:
            labels = enc["input_ids"].clone()
            for i, answer in enumerate(answers):
                labels[i, :] = -100
                if answer and answer in ['a', 'b', 'c', 'd']:
                    answer_ids = self.processor.tokenizer.encode(answer, add_special_tokens=False)
                    if len(answer_ids) > 0:
                        labels[i, -len(answer_ids):] = torch.tensor(answer_ids)
            enc["labels"] = labels

        return enc


# DataLoader ìƒì„±
train_df = pd.read_csv("/content/train.csv")
train_ds = VQADataset(train_df, processor, "/content", train=True)
train_loader = DataLoader(
    train_ds,
    batch_size=1,
    shuffle=True,
    collate_fn=DataCollator(processor, train=True),
    num_workers=0
)
```

### 3. í•™ìŠµ

```python
from qwen3_30b_multigpu_core import train_one_epoch_memory_efficient
from transformers import get_cosine_schedule_with_warmup

# Optimizer & Scheduler
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=5e-5,
    weight_decay=0.01
)

num_training_steps = 2 * len(train_loader) // 16  # epochs * steps / grad_accum
num_warmup_steps = int(num_training_steps * 0.1)

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps,
    num_training_steps
)

# AMP Scaler
scaler = torch.amp.GradScaler('cuda')

# í•™ìŠµ
for epoch in range(2):
    logger.info(f"Epoch {epoch+1}/2 ì‹œì‘...")

    avg_loss = train_one_epoch_memory_efficient(
        model=model,
        train_loader=train_loader,
        optimizer=optimizer,
        scheduler=scheduler,
        scaler=scaler,
        grad_accum_steps=16,
        max_grad_norm=0.5,
        device=torch.device("cuda:0"),
        logger=logger
    )

    logger.info(f"Epoch {epoch+1} ì™„ë£Œ - Avg Loss: {avg_loss:.4f}")

    # ëª¨ë¸ ì €ì¥
    model.save_pretrained(f"/content/checkpoints/epoch{epoch+1}")
    processor.save_pretrained(f"/content/checkpoints/epoch{epoch+1}")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_gpu_memory()
```

### 4. ì¶”ë¡ 

```python
from qwen3_30b_multigpu_core import infer_parallel

# Test ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("/content/test.csv")

# ì¶”ë¡ 
predictions = infer_parallel(
    model=model,
    processor=processor,
    test_df=test_df,
    data_dir="/content",
    img_col='path',
    system_instruct="You are a helpful VQA assistant.",
    logger=logger
)

# ì œì¶œ íŒŒì¼ ìƒì„±
submission = pd.DataFrame({
    'id': test_df['id'],
    'answer': predictions
})
submission.to_csv("/content/submission.csv", index=False)

logger.info("ì¶”ë¡  ì™„ë£Œ!")
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### OOM (Out of Memory) ë°œìƒ

#### í•´ê²° 1: ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°
```python
IMAGE_SIZE = 384  # 512 â†’ 384
# ë˜ëŠ”
IMAGE_SIZE = 320  # ë” ì‘ê²Œ
```

#### í•´ê²° 2: LoRA rank ì¤„ì´ê¸°
```python
LORA_R = 4  # 8 â†’ 4
LORA_ALPHA = 8
```

#### í•´ê²° 3: Gradient Accumulation ëŠ˜ë¦¬ê¸°
```python
GRAD_ACCUM_STEPS = 32  # 16 â†’ 32
```

#### í•´ê²° 4: Max memory ì¤„ì´ê¸°
```python
MAX_MEMORY_PER_GPU = {0: "12GB", 1: "12GB"}  # 14GB â†’ 12GB
```

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

```python
# Gradient accumulation ì¤„ì´ê¸° (ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ)
GRAD_ACCUM_STEPS = 8  # 16 â†’ 8

# DataLoader num_workers ì¦ê°€
num_workers = 2  # 0 â†’ 2

# AMP í™•ì¸
USE_AMP = True  # í•„ìˆ˜
```

### GPU ë¶ˆê· í˜•

```python
# device_map ìˆ˜ë™ ì„¤ì •
model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_id,
    device_map={
        "model.embed_tokens": 0,
        "model.layers.0": 0,
        "model.layers.1": 0,
        ...
        "model.layers.30": 1,
        "model.norm": 1,
        "lm_head": 1,
    }
)
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### T4 * 2 í™˜ê²½

| ì„¤ì • | í•™ìŠµ ì†ë„ | ë©”ëª¨ë¦¬ ì‚¬ìš© | ì •í™•ë„ |
|------|---------|-----------|-------|
| IMAGE_SIZE=384, R=8, BS=1, GA=16 | ~2min/epoch | GPU0: 13GB, GPU1: 13GB | **88-90%** |
| IMAGE_SIZE=448, R=12, BS=1, GA=12 | ~3min/epoch | GPU0: 14.5GB, GPU1: 14.5GB | **89-91%** |
| IMAGE_SIZE=512, R=16, BS=1, GA=8 | ~5min/epoch | âš ï¸ OOM ìœ„í—˜ | N/A |

### ê¶Œì¥ ì„¤ì • (T4 * 2)

```python
# ğŸ¯ ìµœì  ì„¤ì • (ì•ˆì •ì„± + ì„±ëŠ¥)
IMAGE_SIZE = 384
LORA_R = 8
BATCH_SIZE = 1
GRAD_ACCUM_STEPS = 16
NUM_EPOCHS = 2
LEARNING_RATE = 5e-5
```

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•™ìŠµ ì „
- [ ] GPU 2ê°œ í™•ì¸ (`nvidia-smi`)
- [ ] íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ
- [ ] ë°ì´í„° ê²½ë¡œ í™•ì¸
- [ ] Config ì„¤ì • í™•ì¸

### í•™ìŠµ ì¤‘
- [ ] GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
- [ ] Loss ê°ì†Œ í™•ì¸
- [ ] ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™•ì¸

### í•™ìŠµ í›„
- [ ] ëª¨ë¸ ì €ì¥ í™•ì¸
- [ ] ì¶”ë¡  í…ŒìŠ¤íŠ¸
- [ ] ì œì¶œ íŒŒì¼ ìƒì„±

## ğŸ“š ì°¸ê³  ìë£Œ

- [Qwen2.5-VL ëª¨ë¸ í˜ì´ì§€](https://huggingface.co/Qwen/Qwen2.5-VL-30B-A3B-Instruct)
- [QLoRA ë…¼ë¬¸](https://arxiv.org/abs/2305.14314)
- [Accelerate ë¬¸ì„œ](https://huggingface.co/docs/accelerate)
- [BitsAndBytes ë¬¸ì„œ](https://github.com/TimDettmers/bitsandbytes)

---

**ğŸ¤– SSAFY AI Project 2025 - Qwen3-30B Multi-GPU Edition**
**âœ¨ Optimized for T4 * 2 (32GB)**
