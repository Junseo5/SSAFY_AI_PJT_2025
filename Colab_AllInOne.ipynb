{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab All-In-One (Qwen3-VL-8B)\n",
    "\n",
    "Colab A100 80GB 기준, 학습 1epoch와 추론을 각각 1시간 이내로 단축하도록 최적화한 통합 노트북입니다.\n",
    "- 모델: Qwen/Qwen3-VL-8B-Instruct (bf16, FlashAttention2 시도)\n",
    "- 학습: LoRA(q/v), bf16, fused AdamW, 큰 배치, 빠른 DataLoader\n",
    "- 추론: 배치 생성, 짧은 토큰 길이, 그리디 디코딩, KV 캐시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["setup", "colab"]},
   "outputs": [],
   "source": [
    "# Colab 전용 설치\n",
    "import os\n",
    "if \"COLAB_RELEASE_TAG\" in os.environ or \"COLAB_GPU\" in os.environ:\n",
    "    !pip install -U --quiet \"transformers>=4.46.0\" \"accelerate>=0.34.0\" \"peft>=0.12.0\" \"bitsandbytes>=0.43.3\"\n",
    "    !pip install -U --quiet qwen-vl-utils[decord]>=0.0.10 datasets pillow opencv-python pandas tqdm matplotlib seaborn python-dotenv\n",
    "    try:\n",
    "        !pip install -U --quiet flash-attn --no-build-isolation\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] flash-attn install skipped: {e}')\n",
    "else:\n",
    "    print('[INFO] Non-Colab env detected. Skipping installs.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["env"]},
   "outputs": [],
   "source": [
    "# 환경/전역 설정\n",
    "import os, sys, math, random, time, gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "SEED = int(os.environ.get('SEED', 42))\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "if hasattr(torch.backends, 'cuda') and hasattr(torch.backends.cuda, 'sdp_kernel'):\n",
    "    torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=True)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    print('VRAM(GB):', round(torch.cuda.get_device_properties(0).total_memory/1e9, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["colab", "drive"]},
   "outputs": [],
   "source": [
    "# Google Drive 마운트 (Colab 환경에서만)\n",
    "import os\n",
    "if \"COLAB_RELEASE_TAG\" in os.environ or \"COLAB_GPU\" in os.environ:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        print('[OK] Google Drive mounted.')\n",
    "    except Exception as e:\n",
    "        print('[WARN] Drive mount skipped:', e)\n",
    "else:\n",
    "    print('[INFO] Non-Colab env detected. Skipping drive mount.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["config"]},
   "outputs": [],
   "source": [
    "# 경로/설정\n",
    "PROJECT_ROOT = Path(os.environ.get('PROJECT_ROOT', '/content/drive/MyDrive/Colab Notebooks'))\n",
    "# 기본 데이터 경로: /content/drive/MyDrive/Colab Notebooks/data\n",
    "DATA_DIR = Path(os.environ.get('DATA_DIR', '/content/drive/MyDrive/Colab Notebooks/data'))\n",
    "# CSV가 가리키는 이미지 상대 경로(train/xxx.jpg 등)를 그대로 붙일 수 있도록 기본은 DATA_DIR로 설정\n",
    "IMAGE_DIR = Path(os.environ.get('IMAGE_DIR', DATA_DIR))\n",
    "TRAIN_CSV = Path(os.environ.get('TRAIN_CSV', DATA_DIR / 'train.csv'))\n",
    "TEST_CSV  = Path(os.environ.get('TEST_CSV', DATA_DIR / 'test.csv'))\n",
    "OUT_DIR = Path(os.environ.get('OUT_DIR', PROJECT_ROOT / 'runs' / 'qwen3vl_colab'))\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_ID = os.environ.get('MODEL_ID', 'Qwen/Qwen3-VL-8B-Instruct')\n",
    "USE_4BIT_DEFAULT = os.environ.get('USE_4BIT', 'false').lower() == 'true'\n",
    "LORA_R = int(os.environ.get('LORA_R', 8))\n",
    "LORA_DROPOUT = float(os.environ.get('LORA_DROPOUT', 0.05))\n",
    "LORA_ALPHA = int(os.environ.get('LORA_ALPHA', 16))\n",
    "IMAGE_SIZE = int(os.environ.get('IMAGE_SIZE', 384))\n",
    "MAX_SEQ_LEN = int(os.environ.get('MAX_SEQ_LEN', 768))\n",
    "TRAIN_EPOCHS = int(os.environ.get('EPOCHS', 1))\n",
    "PREFER_FULL_BF16 = (device.type == 'cuda')\n",
    "print('MODEL_ID =', MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["data"]},
   "outputs": [],
   "source": [
    "# VQA 데이터셋 (이미지 + 텍스트)\n",
    "from typing import Dict, Any\n",
    "\n",
    "class VqaDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, image_root: Path, processor: AutoProcessor, max_len: int = 768, image_size: int = 384, train: bool = True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_root = Path(image_root)\n",
    "        self.processor = processor\n",
    "        self.max_len = max_len\n",
    "        self.image_size = image_size\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _build_messages(self, row: pd.Series) -> Any:\n",
    "        question = row.get('question', '')\n",
    "        choices = [row.get('A',''), row.get('B',''), row.get('C',''), row.get('D','')]\n",
    "        sys_msg = {'role': 'system', 'content': 'You are a helpful vision-language assistant for multiple-choice VQA.'}\n",
    "        # 문자열 연결 방식으로 구성하여 노트북 JSON 안정화\n",
    "        text_prompt = (\n",
    "            'Question: ' + str(question) + '\\n' +\n",
    "            'Choices: A) ' + str(choices[0]) + ' B) ' + str(choices[1]) + ' C) ' + str(choices[2]) + ' D) ' + str(choices[3]) + '\\n' +\n",
    "            'Answer with only one letter (A/B/C/D).'\n",
    "        )\n",
    "        user_msg = {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': text_prompt},\n",
    "                {'type': 'image'}\n",
    "            ]\n",
    "        }\n",
    "        return [sys_msg, user_msg]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.image_root / str(row['image'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        messages = self._build_messages(row)\n",
    "        chat_text = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        enc = self.processor(\n",
    "            text=[chat_text],\n",
    "            images=[image],\n",
    "            do_resize=True,\n",
    "            size={'shortest_edge': self.image_size},\n",
    "            padding=True, truncation=True, max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v[0] for k, v in enc.items()}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["model"]},
   "outputs": [],
   "source": [
    "# 모델/프로세서 로드: bf16 full 우선, 실패 시 4bit QLoRA\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if hasattr(processor, 'tokenizer'):\n",
    "    processor.tokenizer.padding_side = 'left'\n",
    "\n",
    "def load_model(prefer_full_bf16: bool = True, use_4bit_default: bool = False):\n",
    "    compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    if prefer_full_bf16 and not use_4bit_default:\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_ID, torch_dtype=compute_dtype, device_map='auto', trust_remote_code=True, low_cpu_mem_usage=True\n",
    "            )\n",
    "            print('[OK] Loaded full model (bf16/fp16).')\n",
    "            return model, False\n",
    "        except Exception as e:\n",
    "            print('[WARN] Full model load failed, fallback to 4bit:', e)\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=compute_dtype)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map='auto', trust_remote_code=True)\n",
    "    print('[OK] Loaded 4bit model.')\n",
    "    return model, True\n",
    "\n",
    "model, is_4bit = load_model(PREFER_FULL_BF16, USE_4BIT_DEFAULT)\n",
    "target_modules = ['q_proj', 'v_proj']\n",
    "if is_4bit:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "if hasattr(model, 'enable_input_require_grads'):\n",
    "    model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "lora_cfg = LoraConfig(r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT, bias='none', task_type='CAUSAL_LM', target_modules=target_modules)\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "# pad/eos 설정 보정\n",
    "if getattr(model, 'generation_config', None) and hasattr(processor, 'tokenizer'):\n",
    "    if model.generation_config.pad_token_id is None:\n",
    "        model.generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    if model.generation_config.eos_token_id is None:\n",
    "        model.generation_config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "print('Trainable params (LoRA) ready.')\n",
    "\n",
    "# 선택적 torch.compile\n",
    "USE_COMPILE = os.environ.get('USE_COMPILE', 'true').lower() == 'true'\n",
    "if USE_COMPILE and hasattr(torch, 'compile') and device.type == 'cuda':\n",
    "    try:\n",
    "        model = torch.compile(model, mode='max-autotune')\n",
    "        print('[OK] torch.compile enabled')\n",
    "    except Exception as e:\n",
    "        print('[WARN] torch.compile disabled:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["dataloaders"]},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 DataLoader 구성\n",
    "assert TRAIN_CSV.exists(), f'TRAIN_CSV not found: {TRAIN_CSV}'\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_df, val_df = train_test_split(train_df, test_size=0.05, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_ds = VqaDataset(tr_df, IMAGE_DIR, processor, max_len=MAX_SEQ_LEN, image_size=IMAGE_SIZE, train=True)\n",
    "val_ds   = VqaDataset(val_df, IMAGE_DIR, processor, max_len=MAX_SEQ_LEN, image_size=IMAGE_SIZE, train=False)\n",
    "\n",
    "cpu_workers = max(2, min(os.cpu_count() or 8, 8))\n",
    "BATCH_TRAIN = int(os.environ.get('BATCH_TRAIN', 8 if not is_4bit else 12))\n",
    "BATCH_EVAL  = int(os.environ.get('BATCH_EVAL', 12 if not is_4bit else 16))\n",
    "\n",
    "def data_collator(features):\n",
    "    batch = processor.pad(features, return_tensors='pt')\n",
    "    # pixel_values가 리스트로 반환되면 텐서로 스택\n",
    "    if isinstance(batch.get('pixel_values', None), list):\n",
    "        import torch as _torch\n",
    "        batch['pixel_values'] = _torch.stack(batch['pixel_values'])\n",
    "    if 'labels' not in batch:\n",
    "        batch['labels'] = batch['input_ids'].clone()\n",
    "    return batch\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True, num_workers=cpu_workers, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
    "eval_loader  = DataLoader(val_ds, batch_size=BATCH_EVAL, shuffle=False, num_workers=cpu_workers, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
    "print('BATCH_TRAIN =', BATCH_TRAIN, 'BATCH_EVAL =', BATCH_EVAL, 'workers =', cpu_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["trainer"]},
   "outputs": [],
   "source": [
    "# Trainer 설정\n",
    "optim_name = 'adamw_torch_fused' if (torch.cuda.is_available()) else 'adamw_torch'\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR / 'checkpoints'),\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.03,\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    fp16=False,\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy='steps', eval_steps=200,\n",
    "    save_strategy='no', report_to='none',\n",
    "    optim=optim_name,\n",
    "    dataloader_num_workers=(os.cpu_count() or 8),\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_checkpointing=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "t0=time.time(); train_out = trainer.train(); t1=time.time()\n",
    "print('Train time (min):', round((t1-t0)/60,2))\n",
    "(OUT_DIR / 'lora_adapter').mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    model.save_pretrained(OUT_DIR / 'lora_adapter')\n",
    "except Exception as e:\n",
    "    print('[WARN] save_pretrained failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"tags": ["inference"]},
   "outputs": [],
   "source": [
    "# 추론: 배치 생성 + 그리디 디코딩\n",
    "assert TEST_CSV.exists(), f'TEST_CSV not found: {TEST_CSV}'\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "inf_ds = VqaDataset(test_df, IMAGE_DIR, processor, max_len=MAX_SEQ_LEN, image_size=IMAGE_SIZE, train=False)\n",
    "inf_loader = DataLoader(inf_ds, batch_size=BATCH_EVAL, shuffle=False, num_workers=max(2, (os.cpu_count() or 8)//2), pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "gen_cfg = dict(max_new_tokens=6, do_sample=False, temperature=0.0, top_p=1.0, use_cache=True)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "t0=time.time()\n",
    "with torch.inference_mode():\n",
    "    for batch in inf_loader:\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "        out = model.generate(**batch, **gen_cfg)\n",
    "        texts = processor.batch_decode(out, skip_special_tokens=True)\n",
    "        for txt in texts:\n",
    "            # 단일 문자(A/B/C/D)만 간단 추출\n",
    "            ans = 'A'\n",
    "            for cand in ['A','B','C','D']:\n",
    "                if (' '+cand+' ') in (' '+txt+' '):\n",
    "                    ans = cand; break\n",
    "            preds.append(ans)\n",
    "t1=time.time(); print('Inference time (min):', round((t1-t0)/60,2))\n",
    "\n",
    "sub = pd.DataFrame({'id': test_df.get('id', pd.RangeIndex(len(test_df))), 'answer': preds[:len(test_df)]})\n",
    "sub_path = OUT_DIR / 'submission.csv'\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print('Saved:', sub_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
