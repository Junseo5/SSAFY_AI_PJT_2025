# Kaggle VQA Challenge - ëª…ì„¸ì„œ ê°œì„  ë³´ì¶© ì‚¬í•­

## ğŸ” ì¶”ê°€ëœ í•µì‹¬ ê°œì„  ì‚¬í•­

### 1. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ëµ (ì‹ ê·œ)

#### 1.1 ì§ˆë¬¸ ìœ í˜•ë³„ ìµœì í™” í”„ë¡¬í”„íŠ¸

```python
# config/prompt_templates.yaml
prompt_templates:
  counting:
    system: "You are a visual counting expert. Analyze the image carefully and count objects precisely."
    user: |
      Question: {question}
      
      Choices:
      a) {choice_a}
      b) {choice_b}
      c) {choice_c}
      d) {choice_d}
      
      Instructions:
      1. Locate all relevant objects in the image
      2. Count each object carefully
      3. Match your count with the closest choice
      4. Answer ONLY with a single letter: a, b, c, or d
      
  color:
    system: "You are a color recognition expert. Identify colors accurately considering lighting and context."
    user: |
      Question: {question}
      
      Choices:
      a) {choice_a}
      b) {choice_b}
      c) {choice_c}
      d) {choice_d}
      
      Instructions:
      1. Identify the primary color of the specified object
      2. Consider lighting conditions
      3. Select the most accurate color description
      4. Answer ONLY with a single letter: a, b, c, or d
      
  ocr:
    system: "You are an OCR specialist. Extract text accurately from images."
    user: |
      Question: {question}
      
      Choices:
      a) {choice_a}
      b) {choice_b}
      c) {choice_c}
      d) {choice_d}
      
      Instructions:
      1. Locate text in the image
      2. Read it carefully (supports Korean, English, numbers)
      3. Match with the provided choices
      4. Answer ONLY with a single letter: a, b, c, or d
      
  yesno:
    system: "You are a visual reasoning expert. Determine if statements are true or false based on the image."
    user: |
      Question: {question}
      
      Choices:
      a) {choice_a}
      b) {choice_b}
      c) {choice_c}
      d) {choice_d}
      
      Instructions:
      1. Verify the statement against image content
      2. Answer with the appropriate yes/no/correct/incorrect option
      3. Answer ONLY with a single letter: a, b, c, or d
      
  general:
    system: "You are a visual question answering expert. Analyze images and answer questions accurately."
    user: |
      Question: {question}
      
      Choices:
      a) {choice_a}
      b) {choice_b}
      c) {choice_c}
      d) {choice_d}
      
      Instructions:
      1. Carefully analyze the image
      2. Consider all provided choices
      3. Select the most accurate answer
      4. Answer ONLY with a single letter: a, b, c, or d
```

#### 1.2 í”„ë¡¬í”„íŠ¸ ì ìš© ë¡œì§

```python
# scripts/prompt_manager.py
import yaml

class PromptManager:
    def __init__(self, templates_path='config/prompt_templates.yaml'):
        with open(templates_path, 'r', encoding='utf-8') as f:
            self.templates = yaml.safe_load(f)['prompt_templates']
    
    def format_prompt(self, question_type, question, choices):
        """ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = self.templates.get(question_type, self.templates['general'])
        
        return {
            'system': template['system'],
            'user': template['user'].format(
                question=question,
                choice_a=choices['a'],
                choice_b=choices['b'],
                choice_c=choices['c'],
                choice_d=choices['d']
            )
        }
    
    def apply_few_shot(self, prompt, question_type, n_shots=2):
        """Few-shot learning ì ìš©"""
        examples = self._get_examples(question_type, n_shots)
        
        few_shot_prefix = "\n\nHere are some examples:\n"
        for i, ex in enumerate(examples, 1):
            few_shot_prefix += f"\nExample {i}:\n"
            few_shot_prefix += f"Question: {ex['question']}\n"
            few_shot_prefix += f"Answer: {ex['answer']}\n"
        
        prompt['user'] = few_shot_prefix + "\n\nNow answer this:\n" + prompt['user']
        return prompt
    
    def _get_examples(self, question_type, n=2):
        """ìœ í˜•ë³„ ì˜ˆì‹œ ìƒ˜í”Œ (í•˜ë“œì½”ë”© ë˜ëŠ” DBì—ì„œ ë¡œë“œ)"""
        examples_db = {
            'counting': [
                {'question': 'ì´ë¯¸ì§€ì— ì‚¬ê³¼ê°€ ëª‡ ê°œ ìˆìŠµë‹ˆê¹Œ?', 'answer': 'c'},
                {'question': 'How many cars are in the parking lot?', 'answer': 'b'}
            ],
            'color': [
                {'question': 'ì´ ì°¨ëŠ” ë¬´ìŠ¨ ìƒ‰ì…ë‹ˆê¹Œ?', 'answer': 'a'},
                {'question': 'What color is the sky?', 'answer': 'd'}
            ],
            # ... ë‹¤ë¥¸ ìœ í˜•ë“¤
        }
        return examples_db.get(question_type, [])[:n]
```

---

### 2. ì²´ê³„ì  ì—ëŸ¬ ì²˜ë¦¬ ì „ëµ

```python
# scripts/error_handler.py
import logging
import torch
from typing import Callable, Any
from functools import wraps

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class VQAErrorHandler:
    """VQA í”„ë¡œì íŠ¸ ì „ìš© ì—ëŸ¬ í•¸ë“¤ëŸ¬"""
    
    @staticmethod
    def handle_gpu_oom(func: Callable) -> Callable:
        """GPU OOM ìë™ ë³µêµ¬"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    logger.warning("âš ï¸ GPU OOM detected. Clearing cache...")
                    torch.cuda.empty_cache()
                    
                    # Batch size ì¤„ì´ê¸°
                    if 'batch_size' in kwargs:
                        new_batch_size = max(1, kwargs['batch_size'] // 2)
                        logger.info(f"ğŸ”„ Reducing batch_size: {kwargs['batch_size']} â†’ {new_batch_size}")
                        kwargs['batch_size'] = new_batch_size
                        return func(*args, **kwargs)
                    else:
                        raise
                else:
                    raise
        return wrapper
    
    @staticmethod
    def handle_tokenization_error(func: Callable) -> Callable:
        """í•œê¸€ í† í°í™” ì˜¤ë¥˜ ë°©ì§€"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                if "encode" in str(e).lower() or "token" in str(e).lower():
                    logger.error(f"âŒ Tokenization failed: {e}")
                    
                    # í…ìŠ¤íŠ¸ ì •ê·œí™” ì¬ì‹œë„
                    if 'text' in kwargs:
                        import unicodedata
                        kwargs['text'] = unicodedata.normalize('NFKC', kwargs['text'])
                        logger.info("ğŸ”„ Retrying with normalized text...")
                        return func(*args, **kwargs)
                    else:
                        raise
                else:
                    raise
        return wrapper
    
    @staticmethod
    def handle_model_load_error(func: Callable) -> Callable:
        """ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì²˜ë¦¬"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                if "load" in str(e).lower() or "checkpoint" in str(e).lower():
                    logger.error(f"âŒ Model loading failed: {e}")
                    
                    # ë°±ì—… ì²´í¬í¬ì¸íŠ¸ ì‹œë„
                    if 'model_path' in kwargs:
                        backup_path = kwargs['model_path'].replace('fold0', 'fold1')
                        if backup_path != kwargs['model_path']:
                            logger.info(f"ğŸ”„ Trying backup checkpoint: {backup_path}")
                            kwargs['model_path'] = backup_path
                            return func(*args, **kwargs)
                    
                    raise
                else:
                    raise
        return wrapper
    
    @staticmethod
    def safe_inference(predictor, image_path, question, choices, max_retries=3):
        """ì•ˆì „í•œ ì¶”ë¡  (ì¬ì‹œë„ í¬í•¨)"""
        for attempt in range(max_retries):
            try:
                result = predictor.predict(image_path, question, choices)
                
                # ê²°ê³¼ ê²€ì¦
                if result['prediction'] not in ['a', 'b', 'c', 'd']:
                    raise ValueError(f"Invalid prediction: {result['prediction']}")
                
                return result
                
            except Exception as e:
                logger.warning(f"âš ï¸ Inference attempt {attempt+1} failed: {e}")
                
                if attempt == max_retries - 1:
                    logger.error(f"âŒ All retries failed for image: {image_path}")
                    # í´ë°±: ë¬´ì‘ìœ„ ë‹µ ë°˜í™˜
                    import random
                    fallback = random.choice(['a', 'b', 'c', 'd'])
                    logger.warning(f"ğŸ² Using fallback prediction: {fallback}")
                    return {'prediction': fallback, 'confidence': 0.0}
                
                # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                import time
                time.sleep(1)

# ì‚¬ìš© ì˜ˆì‹œ
@VQAErrorHandler.handle_gpu_oom
@VQAErrorHandler.handle_tokenization_error
def train_epoch(model, dataloader, optimizer, batch_size=4):
    # í•™ìŠµ ì½”ë“œ
    pass
```

---

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë°©ë²•ë¡ 

```python
# scripts/hyperparameter_search.py
import optuna
from optuna.integration import WeightsAndBiasesCallback
import wandb

class HyperparameterOptimizer:
    def __init__(self, study_name='vqa-optimization'):
        self.study_name = study_name
        
    def objective(self, trial):
        """Optuna ëª©ì  í•¨ìˆ˜"""
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
        config = {
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 5e-5),
            'lora_r': trial.suggest_categorical('lora_r', [16, 24, 32]),
            'lora_alpha': trial.suggest_categorical('lora_alpha', [32, 48, 64]),
            'num_epochs': trial.suggest_int('num_epochs', 3, 5),
            'weight_decay': trial.suggest_loguniform('weight_decay', 1e-3, 1e-1),
            'warmup_ratio': trial.suggest_uniform('warmup_ratio', 0.0, 0.1),
            'gradient_accumulation_steps': trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])
        }
        
        # í•™ìŠµ ì‹¤í–‰
        accuracy = self._train_and_evaluate(config)
        
        # WandB ë¡œê¹…
        wandb.log({
            'trial_number': trial.number,
            'accuracy': accuracy,
            **config
        })
        
        return accuracy
    
    def _train_and_evaluate(self, config):
        """ì„¤ì •ìœ¼ë¡œ í•™ìŠµ í›„ ê²€ì¦ ì •í™•ë„ ë°˜í™˜"""
        # ì‹¤ì œ í•™ìŠµ ì½”ë“œ í˜¸ì¶œ
        # return val_accuracy
        pass
    
    def optimize(self, n_trials=20):
        """ìµœì í™” ì‹¤í–‰"""
        wandb_callback = WeightsAndBiasesCallback(
            wandb_kwargs={'project': 'kaggle-vqa-optimization'}
        )
        
        study = optuna.create_study(
            study_name=self.study_name,
            direction='maximize',
            pruner=optuna.pruners.MedianPruner()  # ë‚˜ìœ ì‹œë„ ì¡°ê¸° ì¢…ë£Œ
        )
        
        study.optimize(
            self.objective,
            n_trials=n_trials,
            callbacks=[wandb_callback]
        )
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
        best_params = study.best_params
        best_value = study.best_value
        
        print(f"âœ… Best hyperparameters: {best_params}")
        print(f"âœ… Best accuracy: {best_value:.4f}")
        
        return best_params

# ì‚¬ìš©ë²•
if __name__ == "__main__":
    optimizer = HyperparameterOptimizer()
    best_config = optimizer.optimize(n_trials=15)  # Day 4ì— ì‹¤í–‰
    
    # ìµœì  ì„¤ì • ì €ì¥
    import yaml
    with open('config/best_config.yaml', 'w') as f:
        yaml.dump(best_config, f)
```

---

### 4. Cross-Validation ì „ëµ ìƒì„¸í™”

```python
# scripts/stratified_cv.py
from sklearn.model_selection import StratifiedKFold
import pandas as pd
import numpy as np

class VQAStratifiedSplitter:
    def __init__(self, n_folds=3, seed=42):
        self.n_folds = n_folds
        self.seed = seed
    
    def create_folds(self, df):
        """ì§ˆë¬¸ ìœ í˜• ë¹„ìœ¨ì„ ìœ ì§€í•˜ëŠ” Stratified K-Fold"""
        # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        df = self._classify_questions(df)
        
        # ë³µí•© stratify ë ˆì´ë¸” ìƒì„± (ìœ í˜• + ì •ë‹µ)
        df['stratify_label'] = df['question_type'] + '_' + df['answer']
        
        # Stratified K-Fold
        skf = StratifiedKFold(
            n_splits=self.n_folds,
            shuffle=True,
            random_state=self.seed
        )
        
        df['fold'] = -1
        for fold_idx, (train_idx, val_idx) in enumerate(
            skf.split(df, df['stratify_label'])
        ):
            df.loc[val_idx, 'fold'] = fold_idx
        
        # ë¶„í¬ í™•ì¸
        self._print_fold_distribution(df)
        
        return df
    
    def _classify_questions(self, df):
        """ì§ˆë¬¸ ìœ í˜• ìë™ ë¶„ë¥˜"""
        import re
        
        def classify(question):
            patterns = {
                'counting': r'ëª‡|ê°œìˆ˜|ìˆ˜|how many',
                'color': r'ìƒ‰|ìƒ‰ê¹”|color|ë¬´ìŠ¨ìƒ‰',
                'ocr': r'ê¸€ì|ë¬¸ì|ìˆ«ì|ë²ˆí˜¸|ì½|text|number',
                'yesno': r'ì¸ê°€|ì…ë‹ˆê¹Œ|\?$|ìˆëŠ”ê°€|ë§ëŠ”ê°€',
                'location': r'ì–´ë””|ìœ„ì¹˜|where|ì¥ì†Œ',
                'attribute': r'ë¬´ì—‡|what|ì–´ë–¤|kind'
            }
            
            for qtype, pattern in patterns.items():
                if re.search(pattern, question, re.I):
                    return qtype
            return 'general'
        
        df['question_type'] = df['question'].apply(classify)
        return df
    
    def _print_fold_distribution(self, df):
        """Foldë³„ ë¶„í¬ ì¶œë ¥"""
        print("\nğŸ“Š Fold Distribution:")
        print("=" * 60)
        
        for fold in range(self.n_folds):
            fold_df = df[df['fold'] == fold]
            print(f"\nFold {fold} ({len(fold_df)} samples):")
            
            # ì§ˆë¬¸ ìœ í˜• ë¶„í¬
            type_dist = fold_df['question_type'].value_counts()
            for qtype, count in type_dist.items():
                pct = count / len(fold_df) * 100
                print(f"  {qtype:12s}: {count:4d} ({pct:5.1f}%)")
            
            # ì •ë‹µ ë¶„í¬
            answer_dist = fold_df['answer'].value_counts()
            print(f"  Answers: {dict(answer_dist)}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    train_df = pd.read_csv('data/train.csv')
    
    splitter = VQAStratifiedSplitter(n_folds=3, seed=42)
    train_df = splitter.create_folds(train_df)
    
    train_df.to_csv('data/train_with_folds.csv', index=False)
```

---

### 5. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”

```python
# scripts/memory_optimizer.py
import torch
import gc

class GPUMemoryManager:
    def __init__(self):
        self.peak_memory = 0
    
    @staticmethod
    def clear_cache():
        """GPU ìºì‹œ ì •ë¦¬"""
        gc.collect()
        torch.cuda.empty_cache()
        if torch.cuda.is_available():
            torch.cuda.synchronize()
    
    @staticmethod
    def get_memory_stats():
        """ë©”ëª¨ë¦¬ ì‚¬ìš© í†µê³„"""
        if not torch.cuda.is_available():
            return {}
        
        stats = {
            'allocated': torch.cuda.memory_allocated() / 1e9,  # GB
            'reserved': torch.cuda.memory_reserved() / 1e9,
            'max_allocated': torch.cuda.max_memory_allocated() / 1e9
        }
        return stats
    
    @staticmethod
    def optimize_training_config(available_memory_gb=15):
        """ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  ì„¤ì •"""
        configs = {
            15: {  # T4 GPU
                'batch_size': 4,
                'gradient_accumulation_steps': 2,
                'max_seq_length': 512,
                'use_gradient_checkpointing': True,
                'mixed_precision': 'bf16'
            },
            30: {  # T4 x2
                'batch_size': 8,
                'gradient_accumulation_steps': 1,
                'max_seq_length': 1024,
                'use_gradient_checkpointing': False,
                'mixed_precision': 'bf16'
            }
        }
        
        # ê°€ì¥ ê°€ê¹Œìš´ ì„¤ì • ì„ íƒ
        available_configs = [k for k in configs.keys() if k <= available_memory_gb]
        if available_configs:
            selected = max(available_configs)
            return configs[selected]
        else:
            return configs[15]  # ê¸°ë³¸ê°’
    
    def monitor_training(self, callback_interval=100):
        """í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
        def callback(step):
            if step % callback_interval == 0:
                stats = self.get_memory_stats()
                self.peak_memory = max(self.peak_memory, stats['allocated'])
                
                print(f"Step {step} - Memory: {stats['allocated']:.2f}GB / {stats['reserved']:.2f}GB")
                
                # ë©”ëª¨ë¦¬ ê²½ê³ 
                if stats['allocated'] > 13:  # 15GBì˜ 87%
                    print("âš ï¸ WARNING: Memory usage is high!")
                    self.clear_cache()
        
        return callback

# Training Argumentsì— ì ìš©
from transformers import TrainingArguments

def create_memory_efficient_training_args(output_dir='checkpoints'):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ ì„¤ì •"""
    manager = GPUMemoryManager()
    config = manager.optimize_training_config(available_memory_gb=15)
    
    return TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=config['batch_size'],
        gradient_accumulation_steps=config['gradient_accumulation_steps'],
        gradient_checkpointing=config['use_gradient_checkpointing'],
        fp16=False,
        bf16=True if config['mixed_precision'] == 'bf16' else False,
        optim="paged_adamw_8bit",  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì˜µí‹°ë§ˆì´ì €
        dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
        max_grad_norm=1.0,
        logging_steps=10,
        save_strategy="steps",
        save_steps=100,
        save_total_limit=2,  # ì²´í¬í¬ì¸íŠ¸ 2ê°œë§Œ ìœ ì§€
    )
```

---

### 6. ì‹¤í—˜ ì¶”ì  ì²´ê³„

```python
# scripts/experiment_tracker.py
import wandb
import json
from datetime import datetime
from pathlib import Path

class ExperimentTracker:
    def __init__(self, project_name='kaggle-vqa', entity=None):
        self.project_name = project_name
        self.entity = entity
        self.experiment_log = []
    
    def start_experiment(self, config, experiment_name=None):
        """ì‹¤í—˜ ì‹œì‘"""
        if experiment_name is None:
            experiment_name = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        wandb.init(
            project=self.project_name,
            entity=self.entity,
            name=experiment_name,
            config=config,
            tags=self._generate_tags(config)
        )
        
        self.current_experiment = {
            'name': experiment_name,
            'config': config,
            'start_time': datetime.now().isoformat(),
            'status': 'running'
        }
        
        return wandb.run
    
    def log_metrics(self, metrics, step=None):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        wandb.log(metrics, step=step)
    
    def log_model_artifact(self, model_path, name, metadata=None):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì—…ë¡œë“œ"""
        artifact = wandb.Artifact(
            name=name,
            type='model',
            metadata=metadata
        )
        artifact.add_dir(model_path)
        wandb.log_artifact(artifact)
    
    def finish_experiment(self, final_metrics):
        """ì‹¤í—˜ ì¢…ë£Œ"""
        self.current_experiment['end_time'] = datetime.now().isoformat()
        self.current_experiment['final_metrics'] = final_metrics
        self.current_experiment['status'] = 'completed'
        
        self.experiment_log.append(self.current_experiment)
        
        # ë¡œì»¬ì— ì €ì¥
        log_path = Path('experiments/log.json')
        log_path.parent.mkdir(exist_ok=True)
        
        with open(log_path, 'w') as f:
            json.dump(self.experiment_log, f, indent=2)
        
        wandb.finish()
    
    def compare_experiments(self, experiment_names):
        """ì‹¤í—˜ ë¹„êµ"""
        api = wandb.Api()
        runs = api.runs(f"{self.entity}/{self.project_name}")
        
        comparison = []
        for run in runs:
            if run.name in experiment_names:
                comparison.append({
                    'name': run.name,
                    'config': run.config,
                    'summary': run.summary._json_dict
                })
        
        return comparison
    
    def _generate_tags(self, config):
        """ì„¤ì • ê¸°ë°˜ íƒœê·¸ ìƒì„±"""
        tags = []
        
        if 'model_id' in config:
            if '7B' in config['model_id']:
                tags.append('7B')
            elif '3B' in config['model_id']:
                tags.append('3B')
        
        if 'fold' in config:
            tags.append(f"fold{config['fold']}")
        
        if 'learning_rate' in config:
            lr_tag = f"lr{config['learning_rate']:.0e}"
            tags.append(lr_tag)
        
        return tags

# ì‚¬ìš© ì˜ˆì‹œ
tracker = ExperimentTracker(project_name='kaggle-vqa')

config = {
    'model_id': 'Qwen/Qwen2.5-VL-7B-Instruct',
    'fold': 0,
    'learning_rate': 2e-5,
    'lora_r': 24,
    'num_epochs': 3
}

run = tracker.start_experiment(config, experiment_name='7b_fold0_v1')

# í•™ìŠµ...
tracker.log_metrics({'train_loss': 0.35, 'val_accuracy': 0.82})

# ì™„ë£Œ
tracker.finish_experiment({'final_accuracy': 0.845})
```

---

### 7. ë””ë²„ê¹… ê°€ì´ë“œ

```markdown
## ğŸ”§ Debugging Guide

### Common Issues & Solutions

#### Issue 1: GPU Out of Memory
**ì¦ìƒ:**
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**í•´ê²° ë°©ë²•:**
1. Batch size ì¤„ì´ê¸°: `per_device_train_batch_size=2`
2. Gradient accumulation ì¦ê°€: `gradient_accumulation_steps=4`
3. Gradient checkpointing í™œì„±í™”: `gradient_checkpointing=True`
4. Mixed precision ì‚¬ìš©: `bf16=True`

```python
# scripts/debug_memory.py
import torch
print(f"Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB")
print(f"Reserved: {torch.cuda.memory_reserved()/1e9:.2f}GB")
torch.cuda.empty_cache()
```

---

#### Issue 2: í•œê¸€ í† í°í™” ì˜¤ë¥˜
**ì¦ìƒ:**
```
UnicodeEncodeError: 'utf-8' codec can't encode...
```

**í•´ê²° ë°©ë²•:**
```python
import unicodedata

def safe_tokenize(text):
    # NFKC ì •ê·œí™”
    text = unicodedata.normalize('NFKC', text)
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    
    return tokenizer(text, truncation=True, max_length=512)
```

---

#### Issue 3: ëª¨ë¸ ì˜ˆì¸¡ì´ í•­ìƒ 'a'
**ì¦ìƒ:**
- ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ë™ì¼í•œ ë‹µ ì˜ˆì¸¡
- Validation accuracyê°€ 25% ê·¼ì²˜

**ì›ì¸:**
- í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì˜¤ë¥˜
- Forced-choice íŒŒì‹± ì‹¤íŒ¨

**í•´ê²° ë°©ë²•:**
```python
# scripts/debug_prediction.py
def debug_single_prediction(predictor, sample):
    # ì „ì²´ ìƒì„± í…ìŠ¤íŠ¸ í™•ì¸
    with torch.no_grad():
        outputs = predictor.model.generate(
            **inputs,
            max_new_tokens=50,
            output_scores=True,
            return_dict_in_generate=True
        )
    
    generated_text = predictor.processor.decode(outputs.sequences[0])
    print(f"Full output: {generated_text}")
    
    # íŒŒì‹± ê³¼ì • í™•ì¸
    parsed = predictor._parse_answer(generated_text)
    print(f"Parsed answer: {parsed}")
```

---

#### Issue 4: Cross-Validation ì ìˆ˜ ë¶ˆì•ˆì •
**ì¦ìƒ:**
- Fold 0: 85%, Fold 1: 72%, Fold 2: 88%
- í° ë¶„ì‚°

**ì›ì¸:**
- Fold ë¶„í• ì´ ì§ˆë¬¸ ìœ í˜• ë¹„ìœ¨ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
- íŠ¹ì • Foldì— ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘

**í•´ê²° ë°©ë²•:**
- Stratified K-Fold ì‚¬ìš© (ìœ„ Section 4 ì°¸ê³ )
- Foldë³„ ì§ˆë¬¸ ìœ í˜• ë¶„í¬ í™•ì¸

---

#### Issue 5: ì œì¶œ íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜
**ì¦ìƒ:**
```
Submission failed: Invalid format
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
```python
# scripts/validate_submission.py
import pandas as pd

def validate_submission(file_path):
    df = pd.read_csv(file_path)
    
    # 1. ì»¬ëŸ¼ í™•ì¸
    assert list(df.columns) == ['id', 'answer'], "Columns must be ['id', 'answer']"
    
    # 2. ID ì¤‘ë³µ í™•ì¸
    assert not df['id'].duplicated().any(), "Duplicate IDs found"
    
    # 3. ë‹µ í˜•ì‹ í™•ì¸
    assert df['answer'].isin(['a', 'b', 'c', 'd']).all(), "Invalid answers found"
    
    # 4. ê³µë°± í™•ì¸
    assert not df['answer'].str.contains(' ').any(), "Whitespace in answers"
    
    # 5. ëª¨ë“  test ID í¬í•¨ í™•ì¸
    test_ids = pd.read_csv('data/test.csv')['id'].values
    assert set(df['id']) == set(test_ids), "Missing or extra IDs"
    
    print("âœ… Submission file is valid!")

validate_submission('outputs/submission_final.csv')
```
```

---

## ğŸ“ í†µí•© ì²´í¬ë¦¬ìŠ¤íŠ¸

```markdown
## Day-by-Day Implementation Checklist

### Day 1 (Foundation)
- [ ] Environment setup completed
  - [ ] All packages installed
  - [ ] GPU connectivity verified
  - [ ] WandB login successful
- [ ] EDA & Analysis
  - [ ] Question type distribution analyzed
  - [ ] Normalization rules created
  - [ ] Prompt templates prepared
- [ ] Baseline
  - [ ] Zero-shot inference working
  - [ ] First submission made (65-68%)
- [ ] Data Pipeline
  - [ ] Augmentation scripts tested
  - [ ] Stratified CV splits created
  - [ ] Data loaders validated

### Day 2 (7B Training)
- [ ] Training Setup
  - [ ] 4-bit quantization working
  - [ ] LoRA configuration validated
  - [ ] Memory usage optimized (<13GB)
- [ ] Model Training
  - [ ] Fold 0 completed
  - [ ] Fold 1 completed
  - [ ] Fold 2 running overnight
- [ ] Monitoring
  - [ ] WandB tracking active
  - [ ] Checkpoints saved
  - [ ] CV accuracy logged

### Day 3 (3B + Inference)
- [ ] 3B Training (GPU 1)
  - [ ] 3-fold training completed
  - [ ] Models saved
- [ ] 7B Inference
  - [ ] Forced-choice predictions generated
  - [ ] Submission file validated
  - [ ] 2nd submission made (79-82%)
- [ ] Analysis
  - [ ] 7B vs 3B performance compared
  - [ ] Error patterns identified

### Day 4 (Optimization)
- [ ] Ensemble
  - [ ] 7B 3-fold ensemble implemented
  - [ ] 7B+3B mixed ensemble tested
  - [ ] Margin-based re-inference working
- [ ] Hyperparameter Search
  - [ ] Optuna study completed (10+ trials)
  - [ ] Best config identified
  - [ ] Re-training with best params
- [ ] Advanced Features
  - [ ] OCR pipeline integrated
  - [ ] Type-specific post-processing applied
  - [ ] 3rd submission made (83-85%)

### Day 5 (Final Push)
- [ ] Error Analysis
  - [ ] Top 20 errors analyzed
  - [ ] Weak question types identified
  - [ ] Targeted data augmentation done
- [ ] Final Optimizations
  - [ ] High-resolution (448px) inference
  - [ ] TTA applied
  - [ ] Ensemble weights tuned
- [ ] Submission
  - [ ] 4-5 final submissions made
  - [ ] Best result documented
  - [ ] Target achieved (85-88%)
- [ ] Documentation
  - [ ] README completed
  - [ ] Code cleaned
  - [ ] Presentation prepared
```
