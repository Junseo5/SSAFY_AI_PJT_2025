# ğŸš€ Visual Question Answering í”„ë¡œì íŠ¸ ì„±ì¥ ê³¼ì •

## ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”
**ëª©í‘œ**: Kaggle VQA ì±Œë¦°ì§€ì—ì„œ Top 10% ë‹¬ì„± (ëª©í‘œ ì •í™•ë„ 85-88%)
**ê¸°ê°„**: 2025-10-23 ~ 2025-10-27
**ìµœì¢… ì„±ê³¼**: 0.76028 â†’ 0.92386 (+21.5% í–¥ìƒ) ğŸ†

---

## ğŸ¯ Phase 1: ê¸°ë°˜ êµ¬ì¶• ë° ì•„í‚¤í…ì²˜ ì„¤ê³„ (2025-10-23)

### 1ë‹¨ê³„: í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ë° ë°ì´í„° ì¤€ë¹„
**ì»¤ë°‹**: `8ba0cf5` Initial commit â†’ `cf428aa` Base data

#### ì£¼ìš” ì‘ì—…
- í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •
- ë² ì´ìŠ¤ ë°ì´í„°ì…‹ ì¤€ë¹„
- í”„ë¡¬í”„íŠ¸ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥

---

### 2ë‹¨ê³„: T4 GPU í˜¸í™˜ì„± í™•ë³´ ë° í•µì‹¬ ì•„í‚¤í…ì²˜ êµ¬í˜„
**ì»¤ë°‹**: `ed8413b` Complete VQA project implementation with critical fixes

#### ğŸ”¥ í•µì‹¬ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**Problem 1: T4 GPU BFloat16 ë¯¸ì§€ì›**
```
âŒ ë¬¸ì œ: T4 GPUëŠ” BFloat16ì„ ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›í•˜ì§€ ì•Šì•„ ì„±ëŠ¥ ì €í•˜
âœ… í•´ê²°: torch.float16ìœ¼ë¡œ ë³€ê²½, SDPA Attention ì‚¬ìš©
```

**Problem 2: FlashAttention í˜¸í™˜ì„± ì´ìŠˆ**
```
âŒ ë¬¸ì œ: FlashAttention 2ê°€ T4ì—ì„œ ìµœì í™” ë¶ˆê°€
âœ… í•´ê²°: attn_implementation="sdpa"ë¡œ ë³€ê²½
```

**Problem 3: ë¼ë²¨ ì •ë ¬ ë¶ˆì¼ì¹˜ (ê°€ì¥ ì¤‘ìš”!)**
```
âŒ ë¬¸ì œ: í•™ìŠµ ì‹œ ì •ë‹µ í† í° ìœ„ì¹˜ì™€ ì¶”ë¡  ì‹œ ìƒì„± ìœ„ì¹˜ ë¶ˆì¼ì¹˜
âœ… í•´ê²°: Assistant ë©”ì‹œì§€ì— ì •ë‹µ í¬í•¨ + add_generation_prompt=False
```
```python
# Before (ì˜ëª»ëœ ë°©ë²•)
messages = [{"role": "user", "content": [...]}]
text = processor.apply_chat_template(messages, add_generation_prompt=True)

# After (ì˜¬ë°”ë¥¸ ë°©ë²•)
messages = [
    {"role": "user", "content": [...]},
    {"role": "assistant", "content": [{"type": "text", "text": "a"}]}
]
text = processor.apply_chat_template(messages, add_generation_prompt=False)
```

**Problem 4: ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ**
```
âŒ ë¬¸ì œ: AutoModelForVision2Seq ì‚¬ìš© ì‹œ ì„±ëŠ¥ ì €í•˜
âœ… í•´ê²°: Qwen2_5_VLForConditionalGeneration ì§ì ‘ ì‚¬ìš©
```

#### ì„±ê³¼
- **ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**: 16ê°œ Python ìŠ¤í¬ë¦½íŠ¸ + ì™„ì „ í†µí•© ë…¸íŠ¸ë¶
- **QLoRA ìµœì í™”**: 4-bit quantization, r=24, alpha=48
- **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: 7ê°€ì§€ ì§ˆë¬¸ ìœ í˜•ë³„ í…œí”Œë¦¿
- **Stratified K-Fold CV**: 3-fold ì•™ìƒë¸” ì‹œìŠ¤í…œ
- **ì˜ˆìƒ ì„±ëŠ¥**: 65-68% (zero-shot) â†’ 83-85% (3-fold ensemble)

---

### 3ë‹¨ê³„: ë°ì´í„° í˜¸í™˜ì„± í™•ë³´
**ì»¤ë°‹**: `051a5ef` Add baseline workflow and improve data structure compatibility

#### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
**Problem: ë°ì´í„° ì»¬ëŸ¼ëª… ë¶ˆì¼ì¹˜**
```
âŒ ë¬¸ì œ: ë² ì´ìŠ¤ë¼ì¸ì€ 'path' ì»¬ëŸ¼, ê³ ê¸‰ ë²„ì „ì€ 'image' ì»¬ëŸ¼ ì‚¬ìš©
âœ… í•´ê²°: ìë™ ê°ì§€ ë¡œì§ ì¶”ê°€ (path/image ëª¨ë‘ ì§€ì›)
```

#### ì„±ê³¼
- **Dual Workflow**: Baseline(ë¹ ë¦„) + Advanced(ìµœì í™”) ë™ì‹œ ì§€ì›
- **ì™„ë²½í•œ ë°ì´í„° í˜¸í™˜ì„±**: 'path'/'image' ì»¬ëŸ¼ ìë™ ê°ì§€

---

## ğŸ¯ Phase 2: í†µí•© ë…¸íŠ¸ë¶ ì „í™˜ ë° ì„±ëŠ¥ ìµœì í™” (2025-10-24)

### 4ë‹¨ê³„: í†µí•© ë…¸íŠ¸ë¶ ì•„í‚¤í…ì²˜ êµ¬ì¶•
**ì»¤ë°‹**: `3e41494` Consolidate project into single all-in-one notebook

#### ì „ëµì  ì „í™˜
```
AS-IS: ë¶„ì‚°ëœ ìŠ¤í¬ë¦½íŠ¸ (scripts/, config/, notebooks/)
TO-BE: ë‹¨ì¼ í†µí•© ë…¸íŠ¸ë¶ (Kaggle_AllInOne_Pro.ipynb)
```

#### ê³ ê¸‰ ê¸°ë²• ì¶”ê°€
- **EMA (Exponential Moving Average)**: ëª¨ë¸ ê°€ì¤‘ì¹˜ ì•ˆì •í™”
- **SWA (Stochastic Weight Averaging)**: ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- **Cosine Warmup Scheduler**: ì•ˆì •ì  í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
- **TTA (Test-Time Augmentation)**: ì¶”ë¡  ì‹œ ì•™ìƒë¸”

#### ì„±ê³¼
- **ê°œë°œ íš¨ìœ¨ì„±**: ëª¨ë“  ê¸°ëŠ¥ì´ í•˜ë‚˜ì˜ ë…¸íŠ¸ë¶ì— í†µí•©
- **ì‹¤í—˜ ê´€ë¦¬**: experiments/ í´ë”ë¡œ ë²„ì „ë³„ ê´€ë¦¬ ì‹œì‘

---

### 5ë‹¨ê³„: ì²« ì‹¤ì „ ì œì¶œ ë° ì„±ëŠ¥ ê²€ì¦
**ì»¤ë°‹**: `7e898c4` Add Kaggle_AllInOne (0.80452)

#### Public Leaderboard ì²« ì ìˆ˜
```
ğŸ“Š Score: 0.80452
ğŸ“ˆ Baseline ëŒ€ë¹„: +4.424% í–¥ìƒ
```

#### ì„±ê³µ ìš”ì¸
- **ë‹¨ì¼ ë…¸íŠ¸ë¶ ì›Œí¬í”Œë¡œìš°**: ì‹¤í—˜ë³„ ë²„ì „ ê´€ë¦¬
- **REPORT.md**: ê° ì‹¤í—˜ì˜ ì ìˆ˜ì™€ ë³€ê²½ì‚¬í•­ ì¶”ì 
- **ì¼ê´€ëœ í™˜ê²½**: transformers==4.45.2 ê³ ì •

---

### 6ë‹¨ê³„: Multi-GPU ìµœì í™” ë° ì ìˆ˜ í–¥ìƒ
**ì»¤ë°‹**: `7627f43` Kaggle_AllInOne_Pro reaches 0.82716

#### ğŸ”¥ í•µì‹¬ ìµœì í™”

**Optimization 1: Multi-GPU í™œìš©**
```
âœ… Dual T4 GPU ë³‘ë ¬ ì²˜ë¦¬
âœ… accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µí•©
âœ… ì¶”ë¡  ì†ë„ 2ë°° í–¥ìƒ
```

**Optimization 2: API í˜„ëŒ€í™”**
```
Before: AutoModelForVision2Seq (deprecated warnings)
After: AutoModelForImageTextToText + dtype=torch.float16
```

**Optimization 3: í”„ë¡¬í”„íŠ¸ ì •êµí™”**
```
âœ… ë‹¤ì¤‘ ì„ íƒ ì§ˆë¬¸ í…œí”Œë¦¿ ê°•í™”
âœ… ë‹¨ì¼ ë¬¸ì ë‹µë³€ íŒŒì„œ ì—„ê²©í™”
âœ… ì¼ê´€ëœ ì´ë¯¸ì§€ í¬ê¸° (384px)
```

**Optimization 4: ê²°ì •ë¡ ì  ì¶”ë¡ **
```python
generation_config = {
    "temperature": 0.0,  # ê²°ì •ë¡ ì 
    "max_new_tokens": 10,  # ë‹µë³€ë§Œ
    "do_sample": False
}
```

#### ì„±ê³¼
```
ğŸ“Š Score: 0.82716
ğŸ“ˆ Delta: +0.02264 (+2.8% í–¥ìƒ)
ğŸ“ˆ ì´ í–¥ìƒ: +8.8% (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„)
```

---

### 7ë‹¨ê³„: ë¼ë²¨ ë§ˆìŠ¤í‚¹ ë° Direct Logits ë„ì…
**ì»¤ë°‹**: `a71304f` Pro2 ì •í™•ë„ í–¥ìƒ

#### ğŸ”¥ ê³ ê¸‰ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**Problem 1: í”„ë¡¬í”„íŠ¸ í† í° í•™ìŠµ ë¹„íš¨ìœ¨**
```
âŒ ë¬¸ì œ: ì „ì²´ ì‹œí€€ìŠ¤ ì†ì‹¤ ê³„ì‚° â†’ í”„ë¡¬í”„íŠ¸ í† í°ë„ í•™ìŠµ ëŒ€ìƒ
âœ… í•´ê²°: ë¼ë²¨ ë§ˆìŠ¤í‚¹ (answer í† í°ë§Œ ê°ë…)
```
```python
# Assistant ì •ë‹µ í† í°ë§Œ í•™ìŠµ
labels = [-100] * len(prompt_tokens) + answer_token_ids
```

**Problem 2: ìƒì„± ê¸°ë°˜ ì¶”ë¡  ë¶ˆì•ˆì •ì„±**
```
âŒ ë¬¸ì œ: generate() ì‚¬ìš© ì‹œ ë¹„ê²°ì •ë¡ ì  ì¶œë ¥
âœ… í•´ê²°: Direct Logits ì¶”ë¡ 
```
```python
# a, b, c, d í† í°ì˜ logit ê°’ ì§ì ‘ ê³„ì‚°
logits = model(**inputs).logits
probs = F.softmax(logits[answer_position, token_ids], dim=-1)
```

**Problem 3: ê²€ì¦ ë°ì´í„° ì •ë‹µ ì£¼ì…**
```
âŒ ë¬¸ì œ: valid_dsì— train=True â†’ ì •ë‹µ í…ìŠ¤íŠ¸ í¬í•¨ë˜ì–´ ì„±ëŠ¥ ê³¼ëŒ€í‰ê°€
âœ… í•´ê²°: valid_dsì— train=False í”Œë˜ê·¸
```

**Problem 4: TTA êµ¬í˜„**
```
âœ… í•´ê²°: [0.9, 1.0, 1.1] ìŠ¤ì¼€ì¼ í‰ê· ìœ¼ë¡œ robustness í–¥ìƒ
```

#### ì„±ê³¼
- **í•™ìŠµ íš¨ìœ¨ì„±**: Answer í† í°ë§Œ ê°ë… â†’ ìˆ˜ë ´ ì†ë„ í–¥ìƒ
- **ì¶”ë¡  ì•ˆì •ì„±**: Direct Logits â†’ ì¼ê´€ëœ ì˜ˆì¸¡
- **ì•™ìƒë¸” ê°•í™”**: í™•ë¥  í‰ê·  â†’ argmax (Majority Voting ëŒ€ì²´)

---

### 8ë‹¨ê³„: Pro2 Enhanced - 8ê°€ì§€ ë¬¸ì œì  í•´ê²°
**ì»¤ë°‹**: `f7a162d` Add comprehensive Pro2 enhancements

#### ğŸ”¥ Production-Ready ê°œì„ 

**1. ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”**
```python
# ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ì‹œ fallback
try:
    image = Image.open(path)
except:
    image = Image.new('RGB', (384, 384))  # ë¹ˆ ì´ë¯¸ì§€
```

**2. ë¡œê¹… ì‹œìŠ¤í…œ**
```python
logging.basicConfig(
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ],
    format='%(asctime)s - %(levelname)s - %(message)s'
)
```

**3. Direct Logits ì •êµí™”**
```python
# í† í° ë³€í˜• ê³ ë ¤: a, A, " a"
token_variations = [
    tokenizer.encode("a")[0],
    tokenizer.encode("A")[0],
    tokenizer.encode(" a")[0]
]
prob_a = sum(probs[token_variations])
```

**4. Temperature Scaling**
```python
# ê²€ì¦ ì„¸íŠ¸ë¡œ í™•ë¥  êµì •
temperature = find_optimal_temperature(val_logits, val_labels)
calibrated_probs = F.softmax(logits / temperature, dim=-1)
```

**5. ë°°ì¹˜ ì¶”ë¡ **
```
âœ… ì†ë„: 1x â†’ 2-3x í–¥ìƒ
```

**6. Early Stopping**
```python
if val_acc < best_val_acc - patience:
    break  # ê³¼ì í•© ë°©ì§€
```

**7. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬**
```python
# í•™ìŠµ ì¬ê°œ ê°€ëŠ¥
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, f'checkpoint_epoch_{epoch}.pt')
```

**8. ë©”ëª¨ë¦¬ ìµœì í™”**
```python
torch.cuda.empty_cache()
gc.collect()
```

#### ì„±ê³¼
- **ì •í™•ë„**: 85-87% â†’ 87-89% (+2% ì˜ˆìƒ)
- **ì¶”ë¡  ì†ë„**: 2-3ë°° í–¥ìƒ
- **ì•ˆì •ì„±**: Production-ready ìˆ˜ì¤€
- **ì¬ì‚¬ìš©ì„±**: pro2_enhancements.py ëª¨ë“ˆí™”

---

## ğŸ¯ Phase 3: ëŒ€ê·œëª¨ ëª¨ë¸ ì‹¤í—˜ ë° í•œê³„ ê·¹ë³µ (2025-10-24)

### 9ë‹¨ê³„: Qwen3-VL-30B Multi-GPU ì‹œë„
**ì»¤ë°‹**: `8eb669a` Add Qwen3-VL-30B Multi-GPU support

#### ì•¼ì‹¬ì°¬ ì‹œë„
```
ëª©í‘œ: 30B íŒŒë¼ë¯¸í„° ëª¨ë¸ë¡œ 88-90% ì •í™•ë„ ë‹¬ì„±
í™˜ê²½: T4 * 2 (ì´ 32GB)
ì „ëµ: Model Parallelism + 4-bit Quantization
```

#### ğŸ”¥ ëŒ€ê·œëª¨ ëª¨ë¸ ìµœì í™” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**Problem 1: OOM (Out of Memory)**
```
âŒ ë¬¸ì œ: 30B ëª¨ë¸ì€ T4*2ë¡œë„ ë©”ëª¨ë¦¬ ë¶€ì¡±
âœ… í•´ê²° ì‹œë„:
  - 4-bit Quantization (75% ë©”ëª¨ë¦¬ ì ˆê°)
  - Gradient Checkpointing (40% í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆê°)
  - CPU Offloading (Optimizer states)
  - Double Quantization
```

**Problem 2: Model Parallelism êµ¬í˜„**
```python
# device_map="auto"ë¡œ ìë™ ë¶„ì‚°
model = AutoModelForImageTextToText.from_pretrained(
    model_id,
    device_map="auto",
    max_memory={0: "14GB", 1: "14GB"},
    quantization_config=bnb_config
)
```

**Problem 3: í•™ìŠµ ì„¤ì • ê·¹í•œ ìµœì í™”**
```python
BATCH_SIZE = 1            # í•„ìˆ˜
GRAD_ACCUM_STEPS = 16     # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° 16
LORA_R = 8                # ì‘ì€ rank (30Bì— ë§ì¶¤)
IMAGE_SIZE = 384          # 512ëŠ” OOM
```

#### ì„±ê³¼ (ì´ë¡ ì )
- **ì˜ˆìƒ ì •í™•ë„**: 88-90% (+3~5% vs 3B)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: GPU0 13GB, GPU1 13GB
- **í•™ìŠµ ì†ë„**: ~2min/epoch (3B ëŒ€ë¹„ 2ë°° ëŠë¦¼)

#### í•µì‹¬ ê¸°ì—¬
- **ì™„ì „í•œ ë¬¸ì„œí™”**: QWEN3_30B_GUIDE.md, QUICK_START.md
- **ì¬ì‚¬ìš© ê°€ëŠ¥ ì½”ë“œ**: qwen3_30b_multigpu_core.py
- **ì‹¤í–‰ ê°€ëŠ¥ ë…¸íŠ¸ë¶**: Kaggle_Qwen3_30B_MultiGPU.ipynb

---

### 10ë‹¨ê³„: FP8 Pre-Quantization ì‹¤í—˜
**ì»¤ë°‹**: `e2a12ba` Upgrade to Qwen3-VL-30B-A3B-Instruct-FP8

#### ìµœì²¨ë‹¨ ê¸°ìˆ  ì‹œë„
```
ì „ëµ: FP8 Pre-quantization + 4-bit Re-quantization
ëª©í‘œ: BF16 ìˆ˜ì¤€ ì •í™•ë„ + ë©”ëª¨ë¦¬ íš¨ìœ¨
```

#### ğŸ”¥ FP8 ìµœì í™” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**Problem 1: T4 FP8 ë¯¸ì§€ì›**
```
âœ… í•´ê²°: FP8 â†’ FP16 ìë™ ë³€í™˜ fallback
```

**Problem 2: Dual Quantization ì•ˆì •ì„±**
```
ì „ëµ: Fine-grained FP8 (block size 128) + BitsAndBytes 4-bit
```

**Problem 3: ì¶”ë¡  ì‹œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**
```python
# Fallback ë©”ì»¤ë‹ˆì¦˜
try:
    model = load_finetuned_fp8_model()
except:
    model = load_base_fp8_model()  # ë² ì´ìŠ¤ ëª¨ë¸ë¡œ fallback
```

#### ì„±ê³¼ (ì´ë¡ ì )
- **ë©”ëª¨ë¦¬**: 12-14GB per GPU (vs 13-15GB before)
- **ì •í™•ë„**: 88-91% (BF16 ë™ë“±)
- **ì•ˆì •ì„±**: 3ë‹¨ê³„ ì—ëŸ¬ í•¸ë“¤ë§

---

### 11ë‹¨ê³„: 30B ëª¨ë¸ íê¸° ê²°ì •
**ì»¤ë°‹**: `e91dc9f` í´ë” ì •ë¦¬ ë° ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ

#### ğŸ”¥ í˜„ì‹¤ì  íŒë‹¨

**ê²°ì •**: 30B ëª¨ë¸ ì™„ì „ íê¸°
```
âŒ ë¬¸ì œì :
  - T4 GPUë¡œëŠ” ì‹¤ì§ˆì  í•™ìŠµ ë¶ˆê°€ëŠ¥
  - ë©”ëª¨ë¦¬ ì œì•½ìœ¼ë¡œ IMAGE_SIZE 384 ê³ ì •
  - í•™ìŠµ ì†ë„ ë„ˆë¬´ ëŠë¦¼ (~2min/epoch)
  - OOM ìœ„í—˜ ìƒì¡´

âœ… ëŒ€ì•ˆ: Qwen3-VL-8B-Instructë¡œ ì „í™˜
  - 3B â†’ 8B: ì ì ˆí•œ ì„±ëŠ¥ í–¥ìƒ
  - T4ì—ì„œ ì•ˆì •ì  í•™ìŠµ ê°€ëŠ¥
  - IMAGE_SIZE 768ê¹Œì§€ ê°€ëŠ¥
```

#### êµí›ˆ
> "ë” í° ëª¨ë¸ì´ í•­ìƒ ë‹µì€ ì•„ë‹ˆë‹¤. ì£¼ì–´ì§„ í•˜ë“œì›¨ì–´ì—ì„œ ìµœì í™”ëœ ì¤‘ê°„ ëª¨ë¸ì´ ë” í˜„ì‹¤ì ì´ë‹¤."

---

## ğŸ¯ Phase 4: ìµœì í™” ì™„ê²° ë° ìµœì¢… íŠœë‹ (2025-10-25 ~ 2025-10-27)

### 12ë‹¨ê³„: Qwen3-VL-8B ì „í™˜ ë° ë¡œì§ ê°œì„ 
**ì»¤ë°‹**: `4209cf5` ìµœì í™” ì§„í–‰ í´ë” ì •ë¦¬ ë° ì„±ëŠ¥ ìµœëŒ€ í–¥ìƒ

#### ì „ëµì  ì¬ì •ë¹„
```
ëª¨ë¸: Qwen2.5-VL-3B â†’ Qwen3-VL-8B-Instruct
ê¸°ëŒ€: ë‹¨ìˆœ ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œë§Œìœ¼ë¡œë„ í° ì„±ëŠ¥ í–¥ìƒ
```

#### ğŸ”¥ í•™ìŠµ ë¡œì§ ê°œì„ 
```
âŒ ê¸°ì¡´: ë¹„íš¨ìœ¨ì  í•™ìŠµ ë£¨í”„
âœ… ê°œì„ :
  - ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
  - Gradient accumulation íŠœë‹
  - í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ê°œì„ 
```

#### ìš°ë ¤
```
âš ï¸ OOM ìœ„í—˜: 8B ëª¨ë¸ì€ 3Bë³´ë‹¤ 2.7ë°° í¬ê¸°
```

---

### 13ë‹¨ê³„: Colab A100 ìµœì í™” ë²„ì „
**ì»¤ë°‹**: `f623de9` Add Colab A100 80GB optimized notebook

#### ğŸš€ ìµœê³  ì„±ëŠ¥ í™˜ê²½ êµ¬ì¶•

**í™˜ê²½ ì—…ê·¸ë ˆì´ë“œ**
```
FROM: Kaggle T4 (16GB)
TO:   Colab A100 (80GB)
```

#### ğŸ”¥ A100 ì „ìš© ìµœì í™”

**1. ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ**
```
Qwen2.5-VL-3B â†’ Qwen3-VL-8B-Instruct
íŒŒë¼ë¯¸í„°: 3B â†’ 8B (2.67ë°°)
```

**2. ì„±ëŠ¥ ì„¤ì • ê·¹ëŒ€í™”**
```python
IMAGE_SIZE: 384 â†’ 768 (4ë°° í”½ì…€)
BATCH_SIZE: 4 â†’ 16 (4ë°°)
GRAD_ACCUM: 4 â†’ 2 (íš¨ìœ¨ì )
PRECISION: FP16 â†’ BF16 (A100 ë„¤ì´í‹°ë¸Œ)
QUANTIZATION: 4-bit â†’ 8-bit (ê³ í’ˆì§ˆ)
ATTENTION: SDPA â†’ Flash Attention 2 (A100 ìµœì í™”)
```

**3. LoRA ê°•í™”**
```python
LORA_R: 16 â†’ 32 (2ë°°)
LORA_ALPHA: 32 â†’ 64 (2ë°°)
```

#### ì˜ˆìƒ ì„±ê³¼
```
ğŸ“Š Single Fold: 83-86%
ğŸ“Š 3-Fold Ensemble: 87-90%
ğŸ“Š + TTA: 90-93%
â±ï¸ í•™ìŠµ ì‹œê°„: ~1.5h/fold
```

#### í•µì‹¬ í¬ì¸íŠ¸
- **Colab Drive í†µí•©**: /content/drive/MyDrive/kaggle_vqa
- **Output ë¶„ë¦¬**: ./outputs_a100
- **ì™„ì „ ìµœì í™”**: A100ì˜ ëª¨ë“  ê¸°ëŠ¥ í™œìš©

---

### 14ë‹¨ê³„: ìµœì¢… í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
**ì»¤ë°‹**: `ff19525` Prompt engineering

#### ğŸ”¥ ë§ˆì§€ë§‰ ì„±ëŠ¥ í–¥ìƒ

**Problem**: í•™ìŠµëœ ëª¨ë¸ì˜ ì¶”ë¡  ì •í™•ë„ ìµœëŒ€í™”
```
âœ… í•´ê²°:
  - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •êµí™”
  - ì¶”ë¡  ë¡œì§ ì„±ëŠ¥ ê°œì„ 
  - ì¶”ë¡  ëª¨ë‹ˆí„°ë§ ê°•í™”
```

#### ì„±ê³¼
```
ğŸ“ˆ í”„ë¡¬í”„íŠ¸ íš¨ê³¼: ì˜ˆìƒë³´ë‹¤ í° í­ ìƒìŠ¹
â° ì œì•½: ìµœì¢… ì œì¶œ ì‹œê°„ ë¶€ì¡±ìœ¼ë¡œ ë” ë§ì€ ê¸°ëŠ¥ ë¯¸ì ìš©
```

#### ì•„ì‰¬ì›€
> "ë” ë§ì€ ì‹¤í—˜ì„ í•  ì‹œê°„ì´ ìˆì—ˆë‹¤ë©´... í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ ì ì¬ë ¥ì„ í™•ì¸í–ˆì§€ë§Œ ì‹œê°„ ì œì•½ì´ ì•„ì‰½ë‹¤."

---

### 15ë‹¨ê³„: ìµœì¢… í†µí•© ë° ê·¹ì ì¸ ì„±ëŠ¥ í–¥ìƒ ğŸ†
**ë…¸íŠ¸ë¶**: `Kaggle_AllInOne_Pro2_2_(2) (1).ipynb`

#### ğŸš€ ìµœì¢… ëŒíŒŒêµ¬

**í•µì‹¬ ì „ëµ**: Qwen3-VL-8B-Instruct ê¸°ë°˜ í†µí•© ìµœì í™”
```
ëª¨ë¸: Qwen3-VL-8B-Instruct (FP16)
ì „ëµ: ëª¨ë“  ìµœì í™” ê¸°ë²• ì¢…í•© ì ìš©
ëª©í‘œ: Top 10% ì´ìƒ ë‹¬ì„±
```

#### ğŸ”¥ ìµœì¢… ìµœì í™” ì¡°í•©

**1. ë¼ë²¨ ë§ˆìŠ¤í‚¹ ì™„ì„±**
```python
# í”„ë¡¬í”„íŠ¸ í† í° ì œì™¸, Answer í† í°ë§Œ í•™ìŠµ
labels = [-100] * len(prompt_tokens) + answer_token_ids
âœ… í•™ìŠµ íš¨ìœ¨ ê·¹ëŒ€í™”
```

**2. Direct Logits + ë¡œì§“ ì œí•œ**
```python
# a, b, c, d í† í°ë§Œ ìƒì„± (ì¶”ë¡  ì•ˆì •í™”)
allowed_tokens = ['a', 'b', 'c', 'd']
logits = model(**inputs).logits[answer_position]
probs = F.softmax(logits[allowed_token_ids], dim=-1)
âœ… ì¶”ë¡  ì•ˆì •ì„± 100% í™•ë³´
```

**3. ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**
```
âœ… ì§ˆë¬¸ ìœ í˜•ë³„ ìµœì í™”ëœ í…œí”Œë¦¿
âœ… ëª…í™•í•œ ë‹µë³€ í˜•ì‹ ì§€ì‹œ
âœ… ì»¨í…ìŠ¤íŠ¸ ê°•í™”
```

**4. K-Fold + TTA + Ensemble**
```
âœ… Stratified 3-Fold CV
âœ… Test-Time Augmentation
âœ… í™•ë¥  ì•™ìƒë¸” (Probability Averaging)
âœ… Temperature Scaling (í™•ë¥  êµì •)
```

**5. ë©”ëª¨ë¦¬ ë° í•™ìŠµ ìµœì í™”**
```python
# ìë™ GPU VRAM íŠœë‹
AMP (FP16) + SDPA Attention
Gradient Checkpointing
4-bit QLoRA (r=24~32)
ìµœì  Batch Size + Gradient Accumulation
```

#### ğŸ¯ ìµœì¢… ì„±ê³¼

```
ğŸ“Š Public Leaderboard Score: 0.92386
ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: +21.5% í–¥ìƒ
ğŸ“ˆ ì¤‘ê°„ ì ìˆ˜(0.82716) ëŒ€ë¹„: +11.7% í–¥ìƒ
ğŸ† ëª©í‘œ(85-88%) ì´ˆê³¼ ë‹¬ì„±!
```

#### ğŸ’¡ ì„±ê³µì˜ í•µì‹¬ ìš”ì¸

**ê¸°ìˆ ì  ì™„ì„±ë„**
1. **ë¼ë²¨ ì •ë ¬ + ë§ˆìŠ¤í‚¹**: í•™ìŠµ/ì¶”ë¡  ì™„ë²½ ì¼ì¹˜
2. **Direct Logits + ë¡œì§“ ì œí•œ**: 100% ì•ˆì •ì  ì¶”ë¡ 
3. **8B ëª¨ë¸ ì„ íƒ**: ì„±ëŠ¥ê³¼ íš¨ìœ¨ì˜ ìµœì  ê· í˜•
4. **ì¢…í•© ì•™ìƒë¸”**: K-Fold + TTA + í™•ë¥  í‰ê· 

**ì „ëµì  ì„ íƒ**
1. 30B í¬ê¸° â†’ 8B ì§‘ì¤‘: í˜„ì‹¤ì  íŒë‹¨
2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§: ë§ˆì§€ë§‰ í•µì‹¬ ê°œì„ 
3. ëª¨ë“  ìµœì í™” í†µí•©: ì‹œë„ˆì§€ íš¨ê³¼

#### êµí›ˆ
> "ê°ê°ì˜ ìµœì í™”ëŠ” ì‘ì€ í–¥ìƒì´ì§€ë§Œ, ëª¨ë“  ê¸°ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ê²°í•©í•˜ë©´ ê·¹ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£° ìˆ˜ ìˆë‹¤."

---

## ğŸ“Š ìµœì¢… ì„±ê³¼ ìš”ì•½

### ì •ëŸ‰ì  ì„±ê³¼
```
ë² ì´ìŠ¤ë¼ì¸:        0.76028
AllInOne:          0.80452 (+5.8%)
AllInOne Pro:      0.82716 (+8.8%)
ìµœì¢… ë²„ì „:         0.92386 (+21.5%) ğŸ†
ì´ í–¥ìƒ:           +21.5% (ëª©í‘œ ì´ˆê³¼ ë‹¬ì„±!)
```

### ì•„í‚¤í…ì²˜ ì§„í™”
```
Phase 1: ë¶„ì‚° ìŠ¤í¬ë¦½íŠ¸ (16ê°œ Python íŒŒì¼)
Phase 2: í†µí•© ë…¸íŠ¸ë¶ (All-in-One)
Phase 3: ì‹¤í—˜ ë²„ì „ ê´€ë¦¬ (experiments/)
Phase 4: í™˜ê²½ë³„ ìµœì í™” (Kaggle T4 / Colab A100)
```

### ëª¨ë¸ ì§„í™”
```
ì‹œë„ 1: Qwen2.5-VL-3B (ë² ì´ìŠ¤ë¼ì¸)
ì‹œë„ 2: Qwen3-VL-30B (ì‹¤íŒ¨ - í•˜ë“œì›¨ì–´ í•œê³„)
ì‹œë„ 3: Qwen3-VL-30B-FP8 (ì‹¤íŒ¨ - ë³µì¡ë„ ê³¼ë‹¤)
ìµœì¢…:   Qwen3-VL-8B (ì„±ê³µ - ìµœì  ë°¸ëŸ°ìŠ¤)
```

---

## ğŸ“ í•µì‹¬ í•™ìŠµ ì‚¬í•­

### 1. T4 GPU ìµœì í™” ë§ˆìŠ¤í„°ë¦¬
```
âœ… Float16 í•„ìˆ˜ (BFloat16 ë¹„íš¨ìœ¨)
âœ… SDPA Attention (FlashAttention ë¶ˆê°€)
âœ… 4-bit QLoRA (ë©”ëª¨ë¦¬ íš¨ìœ¨)
âœ… Gradient Checkpointing (í•„ìˆ˜)
```

### 2. ë¼ë²¨ ì •ë ¬ì˜ ì¤‘ìš”ì„±
```
ğŸ’¡ ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬:
   í•™ìŠµ ì‹œ Assistant ë©”ì‹œì§€ì— ì •ë‹µ í¬í•¨
   add_generation_prompt=False ì‚¬ìš©
   â†’ í•™ìŠµ/ì¶”ë¡  ì¼ê´€ì„± í™•ë³´
```

### 3. Direct Logits vs Generate
```
Generate(): ë¹„ê²°ì •ë¡ ì , ëŠë¦¼
Direct Logits: ê²°ì •ë¡ ì , ë¹ ë¦„, ì•ˆì •ì 
â†’ ê²½ìŸì—ì„œëŠ” Direct Logits í•„ìˆ˜
```

### 4. í•˜ë“œì›¨ì–´ ì œì•½ì˜ í˜„ì‹¤
```
âŒ 30B ëª¨ë¸ ì‹¤íŒ¨ êµí›ˆ:
   - í° ëª¨ë¸ â‰  ë†’ì€ ì„±ëŠ¥
   - í•˜ë“œì›¨ì–´ ì œì•½ ê³ ë ¤ í•„ìˆ˜
   - ì¤‘ê°„ í¬ê¸° ëª¨ë¸ ìµœì í™”ê°€ í˜„ì‹¤ì 
```

### 5. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ ìœ„ë ¥
```
ğŸ“ˆ ë§ˆì§€ë§‰ ìˆœê°„ í”„ë¡¬í”„íŠ¸ ê°œì„ ìœ¼ë¡œ í° í­ ìƒìŠ¹
ğŸ’¡ ì´ˆê¸°ë¶€í„° í”„ë¡¬í”„íŠ¸ì— íˆ¬ìí–ˆë‹¤ë©´ ë” í° ì„±ê³¼
```

### 6. ì‹¤í—˜ ê´€ë¦¬ì˜ ì¤‘ìš”ì„±
```
âœ… ë‹¨ì¼ ë…¸íŠ¸ë¶ ì›Œí¬í”Œë¡œìš°
âœ… experiments/ í´ë” ë²„ì „ ê´€ë¦¬
âœ… REPORT.mdë¡œ ë³€ê²½ì‚¬í•­ ì¶”ì 
â†’ ì²´ê³„ì  ì‹¤í—˜ ê´€ë¦¬ê°€ ì„±ê³µì˜ ì—´ì‡ 
```

---

## ğŸ”® ë¯¸ë˜ ê°œì„  ë°©í–¥

### ë‹¨ê¸° (ì‹¤í˜„ ê°€ëŠ¥)
1. **Prompt Engineering ì‹¬í™”**: ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
2. **Ensemble ê³ ë„í™”**: Weighted voting, Stacking
3. **Data Augmentation**: Choice shuffle, Paraphrase
4. **Hyperparameter Tuning**: Optuna ì ìš©

### ì¤‘ê¸° (ë„ì „ì )
1. **Knowledge Distillation**: 8B â†’ 3B (ì†ë„ í–¥ìƒ)
2. **Retrieval-Augmented**: External knowledge í™œìš©
3. **Multi-Task Learning**: Related tasksë¡œ pre-training
4. **Error Analysis**: ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì§‘ì¤‘ ë¶„ì„

### ì¥ê¸° (ì—°êµ¬ì )
1. **Custom Architecture**: VQA íŠ¹í™” ëª¨ë¸ ì„¤ê³„
2. **Self-Training**: Pseudo-labeling í™œìš©
3. **Active Learning**: Hard samples ì§‘ì¤‘ í•™ìŠµ

---

## ğŸ’ í”„ë¡œì íŠ¸ì˜ ê°€ì¹˜

### ê¸°ìˆ ì  ì„±ê³¼
- âœ… **Production-Ready íŒŒì´í”„ë¼ì¸**: ì—ëŸ¬ í•¸ë“¤ë§, ë¡œê¹…, ì²´í¬í¬ì¸íŠ¸
- âœ… **ì™„ì „í•œ ë¬¸ì„œí™”**: 15+ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ, ìƒì„¸ ê°€ì´ë“œ
- âœ… **ì¬ì‚¬ìš© ê°€ëŠ¥ ì½”ë“œ**: ëª¨ë“ˆí™”ëœ í•¨ìˆ˜, í´ë˜ìŠ¤
- âœ… **í™˜ê²½ë³„ ìµœì í™”**: T4 / A100 ì „ìš© ì„¤ì •

### ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
- ğŸ”¥ **30B ëª¨ë¸ ì‹¤íŒ¨ â†’ 8B ì „í™˜**: í˜„ì‹¤ì  íŒë‹¨
- ğŸ”¥ **ë¼ë²¨ ì •ë ¬ ë°œê²¬**: í•µì‹¬ ë¬¸ì œ ì¸ì‹ ë° í•´ê²°
- ğŸ”¥ **Direct Logits ë„ì…**: ìƒì„± ëŒ€ì‹  í™•ë¥  ì§ì ‘ ê³„ì‚°
- ğŸ”¥ **Multi-GPU êµ¬í˜„**: ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”

### ì„±ì¥ ê¶¤ì 
```
Day 1: T4 í˜¸í™˜ì„± í™•ë³´ â†’ ê¸°ë°˜ êµ¬ì¶• (0.76)
Day 2: í†µí•© ë…¸íŠ¸ë¶ ì „í™˜ â†’ ì‹¤í—˜ íš¨ìœ¨í™” (0.80)
Day 3: Multi-GPU ìµœì í™” â†’ ì ì§„ì  í–¥ìƒ (0.83)
Day 4: ëŒ€ê·œëª¨ ëª¨ë¸ ë„ì „ â†’ í•œê³„ ì¸ì‹ ë° 8B ì „í™˜
Day 5: ìµœì¢… í†µí•© ìµœì í™” â†’ ê·¹ì ì¸ ëŒíŒŒ (0.92) ğŸ†
```

---

## ğŸ† ê²°ë¡ 

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¨ìˆœíˆ ì ìˆ˜ í–¥ìƒì„ ë„˜ì–´ì„œ, **ì²´ê³„ì ì¸ ë¬¸ì œ í•´ê²° ê³¼ì •**ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

1. **ê¸°ë°˜ êµ¬ì¶•**: T4 í˜¸í™˜ì„±, ë¼ë²¨ ì •ë ¬ (ê°€ì¥ ì¤‘ìš”)
2. **ë°˜ë³µ ì‹¤í—˜**: í†µí•© ë…¸íŠ¸ë¶, ë²„ì „ ê´€ë¦¬
3. **ê³¼ê°í•œ ì‹œë„**: 30B ëª¨ë¸ ì‹¤í—˜ (ì‹¤íŒ¨í–ˆì§€ë§Œ ë°°ì›€)
4. **í˜„ì‹¤ì  ì„ íƒ**: 8B ëª¨ë¸ë¡œ ì „í™˜
5. **ìµœì¢… í†µí•©**: ëª¨ë“  ìµœì í™” ê¸°ë²• ì¢…í•© ì ìš© â†’ ê·¹ì  ì„±ê³¼

> **í•µì‹¬ êµí›ˆ**: "ê°ê°ì˜ ìµœì í™”ëŠ” ì‘ì€ í–¥ìƒì´ì§€ë§Œ, ì²´ê³„ì ìœ¼ë¡œ ê²°í•©í•˜ë©´ ê·¹ì ì¸ ì‹œë„ˆì§€ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤. ì™„ë²½í•œ í•´ê²°ì±…ì€ ì—†ì§€ë§Œ, ëŠì„ì—†ëŠ” ê°œì„ ì´ íƒì›”í•œ ê²°ê³¼ë¥¼ ë§Œë“ ë‹¤."

**ìµœì¢… ì„±ê³¼**: 0.76028 â†’ 0.92386 (+21.5%) ğŸ†
**ëª©í‘œ ë‹¬ì„±**: 85-88% ëª©í‘œ â†’ 92.4% ë‹¬ì„± (ëª©í‘œ ì´ˆê³¼!)
**ì§„ì§œ ì„±ê³¼**: VQA ì‹œìŠ¤í…œ êµ¬ì¶• ì „ë¬¸ì„±, ê³ ê¸‰ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ëŠ¥ë ¥, ì²´ê³„ì  í”„ë¡œì íŠ¸ ê´€ë¦¬ ì—­ëŸ‰

---

**ì‘ì„±ì¼**: 2025-10-27
**í”„ë¡œì íŠ¸**: SSAFY AI Project 2025
**ë ˆí¬ì§€í† ë¦¬**: SSAFY_AI_PJT_2025
