"""
Kaggle_AllInOne_Pro2 ê°œì„  ì½”ë“œ ëª¨ìŒ
ì´ íŒŒì¼ì˜ í•¨ìˆ˜ë“¤ì„ ë…¸íŠ¸ë¶ì— ë³µì‚¬-ë¶™ì—¬ë„£ê¸°í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
"""

import logging
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
from datetime import datetime
from scipy.optimize import minimize
from pathlib import Path

# ============================================================================
# 1. ë¡œê¹… ì‹œìŠ¤í…œ
# ============================================================================

def setup_logging(log_dir, level=logging.INFO):
    """
    ê°•í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •

    ì‚¬ìš©ë²•:
        logger = setup_logging("/content/logs")
        logger.info("Training started")
    """
    Path(log_dir).mkdir(parents=True, exist_ok=True)

    logger = logging.getLogger('VQA')
    logger.setLevel(level)
    logger.handlers.clear()

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    file_handler = logging.FileHandler(f"{log_dir}/training_{timestamp}.log")
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    return logger


# ============================================================================
# 2. ì•ˆì „í•œ ì´ë¯¸ì§€ ë¡œë“œ
# ============================================================================

def load_image_safe(img_path, fallback_size=512, logger=None):
    """
    ì•ˆì „í•œ ì´ë¯¸ì§€ ë¡œë“œ with fallback

    Args:
        img_path: ì´ë¯¸ì§€ ê²½ë¡œ
        fallback_size: fallback ì´ë¯¸ì§€ í¬ê¸°
        logger: ë¡œê±° (ì„ íƒ)

    Returns:
        (Image, success: bool)
    """
    try:
        img = Image.open(img_path).convert("RGB")
        # ì´ë¯¸ì§€ ê²€ì¦
        if img.size[0] < 10 or img.size[1] < 10:
            raise ValueError(f"Image too small: {img.size}")
        return img, True
    except Exception as e:
        if logger:
            logger.warning(f"âš ï¸ Image load failed ({img_path}): {e}")
        # Fallback: í°ìƒ‰ ì´ë¯¸ì§€
        return Image.new('RGB', (fallback_size, fallback_size), color='white'), False


# ============================================================================
# 3. Direct Logits - ê°œì„ ëœ í† í° í™•ë¥  ê³„ì‚°
# ============================================================================

def get_choice_token_ids_robust(processor):
    """
    ê°•í™”ëœ í† í° ID ì¶”ì¶œ - ì—¬ëŸ¬ ë³€í˜• ê³ ë ¤

    Args:
        processor: Huggingface processor

    Returns:
        dict: {choice: [token_ids]}
    """
    choice_tokens = {}
    for choice in ['a', 'b', 'c', 'd']:
        # ì—¬ëŸ¬ ë³€í˜• ê³ ë ¤
        variants = [
            choice,           # "a"
            f" {choice}",     # " a"
            f"{choice} ",     # "a "
            choice.upper(),   # "A"
            f" {choice.upper()}",  # " A"
        ]
        all_token_ids = set()
        for variant in variants:
            try:
                token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)
                all_token_ids.update(token_ids)
            except:
                pass
        choice_tokens[choice] = list(all_token_ids)
    return choice_tokens


def extract_choice_probs_enhanced(logits, choice_tokens):
    """
    ê°œì„ ëœ choice í™•ë¥  ì¶”ì¶œ

    Args:
        logits: [vocab_size] tensor
        choice_tokens: get_choice_token_ids_robust() ê²°ê³¼

    Returns:
        dict: {choice: probability}
    """
    choice_logits = {}
    for choice, token_ids in choice_tokens.items():
        if len(token_ids) > 0:
            # ì—¬ëŸ¬ í† í°ì˜ logit ì¤‘ ìµœëŒ€ê°’ ì‚¬ìš© (or í‰ê· )
            max_logit = max([logits[tid].item() for tid in token_ids])
            choice_logits[choice] = max_logit
        else:
            choice_logits[choice] = -float('inf')

    # Softmaxë¡œ í™•ë¥  ë³€í™˜
    logit_values = torch.tensor(list(choice_logits.values()))
    probs = F.softmax(logit_values, dim=0).numpy()

    return {choice: probs[i] for i, choice in enumerate(['a', 'b', 'c', 'd'])}


# ============================================================================
# 4. Temperature Scaling
# ============================================================================

class TemperatureScaler:
    """
    Temperature Scaling for Probability Calibration

    ì‚¬ìš©ë²•:
        # í•™ìŠµ
        scaler = TemperatureScaler()
        scaler.fit(val_logits, val_labels)  # val_logits: [N, 4], val_labels: [N] (0/1/2/3)

        # ì ìš©
        calibrated_probs = scaler.transform(test_logits)
    """

    def __init__(self):
        self.temperature = 1.0

    def fit(self, logits, labels, logger=None):
        """
        ê²€ì¦ ì„¸íŠ¸ë¡œ ìµœì  temperature í•™ìŠµ

        Args:
            logits: numpy array [N, 4] - raw logits for [a, b, c, d]
            labels: numpy array [N] - true labels (0=a, 1=b, 2=c, 3=d)
            logger: ë¡œê±° (ì„ íƒ)

        Returns:
            self
        """
        logits = np.array(logits)
        labels = np.array(labels)

        def nll_loss(temp):
            temp = max(temp[0], 0.01)  # ìµœì†Œê°’ ë°©ì§€
            scaled_probs = F.softmax(torch.tensor(logits) / temp, dim=1).numpy()
            # Negative log-likelihood
            nll = -np.log(scaled_probs[np.arange(len(labels)), labels] + 1e-10).mean()
            return nll

        result = minimize(nll_loss, x0=[1.0], bounds=[(0.1, 10.0)], method='L-BFGS-B')
        self.temperature = result.x[0]

        if logger:
            logger.info(f"âœ… Optimal Temperature: {self.temperature:.4f}")
        else:
            print(f"âœ… Optimal Temperature: {self.temperature:.4f}")

        return self

    def transform(self, logits):
        """
        í•™ìŠµëœ temperatureë¡œ í™•ë¥  ìŠ¤ì¼€ì¼

        Args:
            logits: numpy array [N, 4]

        Returns:
            numpy array [N, 4] - calibrated probabilities
        """
        logits = np.array(logits)
        return F.softmax(torch.tensor(logits) / self.temperature, dim=1).numpy()


# ============================================================================
# 5. ë©”ëª¨ë¦¬ ìµœì í™”
# ============================================================================

def clear_memory(logger=None):
    """
    ë©”ëª¨ë¦¬ ì •ë¦¬

    ì‚¬ìš©ë²•:
        for fold in range(3):
            train_fold(...)
            clear_memory(logger)
    """
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9

        if logger:
            logger.info(f"ğŸ’¾ Memory cleared - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
        else:
            print(f"ğŸ’¾ Memory cleared - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")


# ============================================================================
# 6. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ============================================================================

def save_checkpoint(model, optimizer, scheduler, epoch, fold, metrics, save_path, logger=None):
    """
    ì²´í¬í¬ì¸íŠ¸ ì €ì¥

    Args:
        model: ëª¨ë¸
        optimizer: ì˜µí‹°ë§ˆì´ì €
        scheduler: ìŠ¤ì¼€ì¤„ëŸ¬
        epoch: í˜„ì¬ ì—í­
        fold: í˜„ì¬ fold
        metrics: dict - {'val_loss': x, 'val_acc': y}
        save_path: ì €ì¥ ê²½ë¡œ
        logger: ë¡œê±° (ì„ íƒ)
    """
    checkpoint = {
        'epoch': epoch,
        'fold': fold,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'metrics': metrics,
    }

    torch.save(checkpoint, save_path)

    if logger:
        logger.info(f"ğŸ’¾ Checkpoint saved: {save_path}")
    else:
        print(f"ğŸ’¾ Checkpoint saved: {save_path}")


def load_checkpoint(model, optimizer, scheduler, checkpoint_path, logger=None):
    """
    ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

    Args:
        model: ëª¨ë¸
        optimizer: ì˜µí‹°ë§ˆì´ì €
        scheduler: ìŠ¤ì¼€ì¤„ëŸ¬
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        logger: ë¡œê±° (ì„ íƒ)

    Returns:
        (epoch, fold, metrics)
    """
    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if scheduler and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    if logger:
        logger.info(f"âœ… Checkpoint loaded: epoch {checkpoint['epoch']}, fold {checkpoint['fold']}")
    else:
        print(f"âœ… Checkpoint loaded: epoch {checkpoint['epoch']}, fold {checkpoint['fold']}")

    return checkpoint['epoch'], checkpoint['fold'], checkpoint['metrics']


# ============================================================================
# 7. Early Stopping
# ============================================================================

class EarlyStopping:
    """
    Early Stopping êµ¬í˜„

    ì‚¬ìš©ë²•:
        early_stopping = EarlyStopping(patience=2, mode='max')  # mode='max' for accuracy

        for epoch in range(epochs):
            val_acc = validate(...)
            if early_stopping(val_acc, model):
                print("Early stopping triggered")
                break
    """

    def __init__(self, patience=3, mode='min', delta=0.0, logger=None):
        """
        Args:
            patience: ê°œì„ ì´ ì—†ì–´ë„ ê¸°ë‹¤ë¦´ ì—í­ ìˆ˜
            mode: 'min' (loss) or 'max' (accuracy)
            delta: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰
            logger: ë¡œê±° (ì„ íƒ)
        """
        self.patience = patience
        self.mode = mode
        self.delta = delta
        self.logger = logger

        self.best_score = None
        self.counter = 0
        self.early_stop = False

        if mode == 'min':
            self.is_better = lambda new, best: new < best - delta
        else:
            self.is_better = lambda new, best: new > best + delta

    def __call__(self, score, model=None):
        """
        Args:
            score: í˜„ì¬ ìŠ¤ì½”ì–´ (loss or accuracy)
            model: ëª¨ë¸ (best ì €ì¥ìš©, ì„ íƒ)

        Returns:
            bool: True if should stop
        """
        if self.best_score is None:
            self.best_score = score
            return False

        if self.is_better(score, self.best_score):
            self.best_score = score
            self.counter = 0
            if self.logger:
                self.logger.info(f"âœ… New best score: {score:.4f}")
        else:
            self.counter += 1
            if self.logger:
                self.logger.info(f"âš ï¸ No improvement for {self.counter}/{self.patience} epochs")

            if self.counter >= self.patience:
                self.early_stop = True
                if self.logger:
                    self.logger.info(f"ğŸ›‘ Early stopping triggered!")
                return True

        return False


# ============================================================================
# 8. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
# ============================================================================

def optimize_ensemble_weights(predictions_list, val_labels, logger=None):
    """
    ê²€ì¦ ì„¸íŠ¸ë¡œ ìµœì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ íƒìƒ‰

    Args:
        predictions_list: List[np.array] - ê° foldì˜ í™•ë¥  [N, 4]
        val_labels: np.array [N] - ì •ë‹µ ë ˆì´ë¸” (0/1/2/3)
        logger: ë¡œê±° (ì„ íƒ)

    Returns:
        np.array - ìµœì  ê°€ì¤‘ì¹˜

    ì‚¬ìš©ë²•:
        weights = optimize_ensemble_weights([fold0_probs, fold1_probs, fold2_probs], val_labels)
        # ì´í›„ test ì‹œ
        ensemble_probs = sum(w * pred for w, pred in zip(weights, test_predictions))
    """
    n_models = len(predictions_list)

    def negative_accuracy(weights):
        weights = np.abs(weights)
        weights = weights / weights.sum()  # Normalize

        # ê°€ì¤‘ í‰ê· 
        ensemble_preds = sum(w * pred for w, pred in zip(weights, predictions_list))
        ensemble_labels = ensemble_preds.argmax(axis=1)

        acc = (ensemble_labels == val_labels).mean()
        return -acc  # Minimize negative accuracy

    # ì´ˆê¸° ê°€ì¤‘ì¹˜: ë™ì¼
    init_weights = np.ones(n_models) / n_models

    # ìµœì í™”
    result = minimize(
        negative_accuracy,
        init_weights,
        method='L-BFGS-B',
        bounds=[(0.0, 1.0)] * n_models
    )

    # Normalize
    optimal_weights = np.abs(result.x)
    optimal_weights = optimal_weights / optimal_weights.sum()

    if logger:
        logger.info(f"âœ… Optimal weights: {optimal_weights}")
        logger.info(f"   Expected accuracy: {-result.fun:.4f}")
    else:
        print(f"âœ… Optimal weights: {optimal_weights}")
        print(f"   Expected accuracy: {-result.fun:.4f}")

    return optimal_weights


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("Pro2 Enhancement Functions")
    print("=" * 60)
    print()
    print("ì´ íŒŒì¼ì˜ í•¨ìˆ˜ë“¤ì„ ë…¸íŠ¸ë¶ì— ë³µì‚¬-ë¶™ì—¬ë„£ê¸°í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”:")
    print()
    print("1. setup_logging() - ë¡œê¹… ì‹œìŠ¤í…œ")
    print("2. load_image_safe() - ì•ˆì „í•œ ì´ë¯¸ì§€ ë¡œë“œ")
    print("3. get_choice_token_ids_robust() - ê°œì„ ëœ í† í° ID ì¶”ì¶œ")
    print("4. extract_choice_probs_enhanced() - ê°œì„ ëœ í™•ë¥  ê³„ì‚°")
    print("5. TemperatureScaler - Temperature scaling")
    print("6. clear_memory() - ë©”ëª¨ë¦¬ ì •ë¦¬")
    print("7. save_checkpoint(), load_checkpoint() - ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬")
    print("8. EarlyStopping - Early stopping")
    print("9. optimize_ensemble_weights() - ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”")
    print()
    print("=" * 60)
