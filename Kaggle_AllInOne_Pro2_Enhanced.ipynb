{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“’ Kaggle_AllInOne_Pro2_Enhanced.ipynb â€“ ìµœê³  ì„±ëŠ¥ ë²„ì „\n",
        "\n",
        "## ğŸ¯ Pro2 Enhanced ì£¼ìš” ê°œì„ ì‚¬í•­\n",
        "\n",
        "### âœ… í•™ìŠµ ê°œì„ \n",
        "- Val Accuracy + Confusion Matrix + Classification Report\n",
        "- Best ëª¨ë¸: Val Acc ìš°ì„  ì €ì¥ + ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ\n",
        "- í•™ìŠµ ê³¡ì„  ì‹œê°í™” (Train Loss í¬í•¨)\n",
        "- ë¼ë²¨ ë§ˆìŠ¤í‚¹ (í”„ë¡¬í”„íŠ¸ ì†ì‹¤ ì œì™¸)\n",
        "- ê²€ì¦ ë°ì´í„° train=False\n",
        "- Early Stopping ì˜µì…˜\n",
        "\n",
        "### âœ… ì¶”ë¡  ê°œì„ \n",
        "- Direct Logits (ê°œì„ ëœ í† í° í™•ë¥  ê³„ì‚°)\n",
        "- TTA [0.9, 1.0, 1.1] + ë°°ì¹˜ ì¶”ë¡ \n",
        "- pad_token_id ìë™ ë³´ì •\n",
        "- ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”\n",
        "\n",
        "### âœ… ì•™ìƒë¸” ê°œì„ \n",
        "- Temperature Scaling (ì‹¤ì œ êµ¬í˜„)\n",
        "- í™•ë¥  ì•™ìƒë¸” + Weighted Voting\n",
        "- í™•ë¥  ì»¬ëŸ¼ ì €ì¥\n",
        "\n",
        "### âœ… ì‹œìŠ¤í…œ ê°œì„ \n",
        "- ê°•ë ¥í•œ ì—ëŸ¬ í•¸ë“¤ë§\n",
        "- ë¡œê¹… ì‹œìŠ¤í…œ (íŒŒì¼ + ì½˜ì†”)\n",
        "- ë©”ëª¨ë¦¬ ìµœì í™”\n",
        "- ì½”ë“œ ì¤‘ë³µ ì œê±°\n",
        "\n",
        "### âš™ï¸ íŠœë‹ ì„¤ì •\n",
        "```python\n",
        "USE_SAMPLE=False, IMAGE_SIZE=512, NUM_EPOCHS=3\n",
        "GRAD_ACCUM_STEPS=8, WARMUP_RATIO=0.06, LORA_R=16\n",
        "USE_DIRECT_LOGIT_DECODE=True, TTA_SCALES=[0.9,1.0,1.1]\n",
        "ENSEMBLE_METHOD='prob', USE_TEMPERATURE_SCALING=True\n",
        "EARLY_STOPPING_PATIENCE=2\n",
        "```\n",
        "\n",
        "**ğŸ¤– SSAFY AI Project 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q transformers accelerate peft bitsandbytes datasets pillow pandas torch torchvision scikit-learn matplotlib seaborn tqdm scipy --upgrade\n",
        "# !pip install -q qwen-vl-utils==0.0.8\n",
        "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ! ëŸ°íƒ€ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, re, math, random, warnings, json, pickle, logging\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "import unicodedata\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForVision2Seq,\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ğŸ”§ Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ 3. Config ì„¤ì • (Pro2 Enhanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # ì‹œë“œ\n",
        "    SEED = 42\n",
        "    \n",
        "    # ëª¨ë¸\n",
        "    MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "    IMAGE_SIZE = 512\n",
        "    USE_ADVANCED_MODEL = False\n",
        "    \n",
        "    # ë°ì´í„°\n",
        "    DATA_DIR = \"/content\"\n",
        "    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
        "    TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
        "    \n",
        "    # K-Fold\n",
        "    N_FOLDS = 3\n",
        "    USE_KFOLD = True\n",
        "    TRAIN_FOLDS = [0, 1, 2]\n",
        "    \n",
        "    # QLoRA\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 32\n",
        "    LORA_DROPOUT = 0.05\n",
        "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    \n",
        "    # í•™ìŠµ\n",
        "    NUM_EPOCHS = 3\n",
        "    BATCH_SIZE = 1\n",
        "    GRAD_ACCUM_STEPS = 8\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    WARMUP_RATIO = 0.06\n",
        "    MAX_GRAD_NORM = 1.0\n",
        "    \n",
        "    # Early Stopping\n",
        "    USE_EARLY_STOPPING = False\n",
        "    EARLY_STOPPING_PATIENCE = 2\n",
        "    \n",
        "    # ê³ ê¸‰ ê¸°ë²•\n",
        "    USE_AMP = True\n",
        "    USE_EMA = True\n",
        "    EMA_DECAY = 0.999\n",
        "    USE_SWA = True\n",
        "    SWA_START_EPOCH = 1\n",
        "    USE_COSINE_SCHEDULE = True\n",
        "    \n",
        "    # TTA\n",
        "    USE_TTA = True\n",
        "    TTA_SCALES = [0.9, 1.0, 1.1]\n",
        "    \n",
        "    # ì¶”ë¡ \n",
        "    USE_DIRECT_LOGIT_DECODE = True\n",
        "    USE_BATCH_INFERENCE = True  # Enhanced: ë°°ì¹˜ ì¶”ë¡  í™œì„±í™”\n",
        "    INFER_BATCH_SIZE = 4\n",
        "    MAX_NEW_TOKENS = 8\n",
        "    \n",
        "    # Temperature Scaling\n",
        "    USE_TEMPERATURE_SCALING = True\n",
        "    \n",
        "    # ì•™ìƒë¸”\n",
        "    ENSEMBLE_METHOD = \"prob\"  # \"prob\" or \"vote\" or \"weighted\"\n",
        "    FOLD_WEIGHTS = None  # Noneì´ë©´ ë™ì¼ ê°€ì¤‘ì¹˜, [0.4, 0.3, 0.3] ë“± ê°€ëŠ¥\n",
        "    \n",
        "    # ì €ì¥\n",
        "    SAVE_DIR = f\"{DATA_DIR}/checkpoints\"\n",
        "    OUTPUT_DIR = f\"{DATA_DIR}/outputs\"\n",
        "    LOG_DIR = f\"{DATA_DIR}/logs\"\n",
        "    SAVE_EVERY_EPOCH = True  # ë§¤ ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "    \n",
        "    # ìƒ˜í”Œë§\n",
        "    USE_SAMPLE = False\n",
        "    SAMPLE_SIZE = 200\n",
        "    \n",
        "    # í”„ë¡¬í”„íŠ¸\n",
        "    SYSTEM_INSTRUCT = (\n",
        "        \"You are a helpful visual question answering assistant. \"\n",
        "        \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n",
        "    )\n",
        "    \n",
        "    # ë¡œê¹…\n",
        "    LOG_LEVEL = logging.INFO\n",
        "    LOG_TO_FILE = True\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "for dir_path in [cfg.SAVE_DIR, cfg.OUTPUT_DIR, cfg.LOG_DIR]:\n",
        "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ë¡œê¹… ì„¤ì •\n",
        "def setup_logging():\n",
        "    logger = logging.getLogger('VQA')\n",
        "    logger.setLevel(cfg.LOG_LEVEL)\n",
        "    logger.handlers.clear()\n",
        "    \n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    \n",
        "    # ì½˜ì†” í•¸ë“¤ëŸ¬\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "    \n",
        "    # íŒŒì¼ í•¸ë“¤ëŸ¬\n",
        "    if cfg.LOG_TO_FILE:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        file_handler = logging.FileHandler(f\"{cfg.LOG_DIR}/training_{timestamp}.log\")\n",
        "        file_handler.setFormatter(formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "    \n",
        "    return logger\n",
        "\n",
        "logger = setup_logging()\n",
        "\n",
        "# ì‹œë“œ ê³ ì •\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(cfg.SEED)\n",
        "logger.info(f\"âœ… Config ì„¤ì • ì™„ë£Œ\")\n",
        "logger.info(f\"   Model: {cfg.MODEL_ID}\")\n",
        "logger.info(f\"   Image Size: {cfg.IMAGE_SIZE}\")\n",
        "logger.info(f\"   Epochs: {cfg.NUM_EPOCHS}, Grad Accum: {cfg.GRAD_ACCUM_STEPS}\")\n",
        "logger.info(f\"   LoRA R: {cfg.LORA_R}, Warmup: {cfg.WARMUP_RATIO}\")\n",
        "logger.info(f\"   Direct Logits: {cfg.USE_DIRECT_LOGIT_DECODE}, TTA: {cfg.USE_TTA}\")\n",
        "logger.info(f\"   Ensemble: {cfg.ENSEMBLE_METHOD}, Temp Scaling: {cfg.USE_TEMPERATURE_SCALING}\")\n",
        "print(f\"\\nğŸ“ ë¡œê·¸ ì €ì¥ ìœ„ì¹˜: {cfg.LOG_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š 4. ë°ì´í„° ë¡œë“œ & EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
        "    test_df = pd.read_csv(cfg.TEST_CSV)\n",
        "    logger.info(f\"ğŸ“ Train: {len(train_df):,} samples\")\n",
        "    logger.info(f\"ğŸ“ Test: {len(test_df):,} samples\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    raise\n",
        "\n",
        "# ë°ì´í„° ê²€ì¦\n",
        "required_cols = ['question', 'a', 'b', 'c', 'd', 'answer']\n",
        "missing_cols = set(required_cols) - set(train_df.columns)\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}\")\n",
        "\n",
        "# ì´ë¯¸ì§€ ê²½ë¡œ ì»¬ëŸ¼ í™•ì¸\n",
        "img_col = 'path' if 'path' in train_df.columns else 'image'\n",
        "logger.info(f\"ğŸ“· ì´ë¯¸ì§€ ì»¬ëŸ¼: {img_col}\")\n",
        "\n",
        "if cfg.USE_SAMPLE:\n",
        "    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n",
        "    logger.warning(f\"âš ï¸  Sampled {len(train_df)} samples\")\n",
        "\n",
        "logger.info(f\"\\nğŸ“Š Answer Distribution:\")\n",
        "answer_dist = train_df['answer'].value_counts().sort_index()\n",
        "for ans, count in answer_dist.items():\n",
        "    logger.info(f\"   {ans}: {count:4d} ({count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "# ì‹œê°í™”\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "answer_dist.plot(kind='bar', ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('Answer Distribution (Train)', fontsize=12, weight='bold')\n",
        "axes[0].set_xlabel('Answer')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "train_df['question_len'] = train_df['question'].str.len()\n",
        "train_df['question_len'].hist(bins=30, ax=axes[1], color='salmon')\n",
        "axes[1].set_title('Question Length Distribution', fontsize=12, weight='bold')\n",
        "axes[1].set_xlabel('Length (chars)')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{cfg.LOG_DIR}/data_distribution.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "logger.info(f\"âœ… ë°ì´í„° ë¡œë“œ ë° EDA ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ 5. Stratified K-Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if cfg.USE_KFOLD:\n",
        "    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n",
        "    train_df['fold'] = -1\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n",
        "        train_df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "    logger.info(f\"âœ… {cfg.N_FOLDS}-Fold CV ìƒì„±\")\n",
        "    fold_dist = train_df['fold'].value_counts().sort_index()\n",
        "    for fold, count in fold_dist.items():\n",
        "        logger.info(f\"   Fold {fold}: {count:4d} samples\")\n",
        "    \n",
        "    # Foldë³„ ë‹µë³€ ë¶„í¬ í™•ì¸\n",
        "    logger.info(f\"\\nğŸ“Š Answer Distribution per Fold:\")\n",
        "    for fold in range(cfg.N_FOLDS):\n",
        "        fold_data = train_df[train_df['fold'] == fold]\n",
        "        dist = fold_data['answer'].value_counts(normalize=True).sort_index()\n",
        "        dist_str = \", \".join([f\"{k}:{v:.2%}\" for k, v in dist.items()])\n",
        "        logger.info(f\"   Fold {fold}: {dist_str}\")\n",
        "else:\n",
        "    split_idx = int(len(train_df) * 0.9)\n",
        "    train_df['fold'] = -1\n",
        "    train_df.loc[split_idx:, 'fold'] = 0\n",
        "    logger.info(f\"âœ… Single split (90:10)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ—‚ï¸ 6. Dataset & DataCollator (Enhanced)\n",
        "\n",
        "âœ… **ê°œì„ ì‚¬í•­**:\n",
        "- ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”\n",
        "- ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ì‹œ fallback\n",
        "- ë¼ë²¨ ë§ˆìŠ¤í‚¹ ì •êµí™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_mc_prompt(question, a, b, c, d):\n",
        "    \"\"\"Multiple Choice í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
        "    return (\n",
        "        f\"{question}\\n\"\n",
        "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
        "        \"ì •ë‹µì„ ë°˜ë“œì‹œ a, b, c, d ì¤‘ í•˜ë‚˜ì˜ ì†Œë¬¸ì í•œ ê¸€ìë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.\"\n",
        "    )\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    \"\"\"Enhanced VQA Dataset with Error Handling\"\"\"\n",
        "    \n",
        "    def __init__(self, df, processor, data_dir=\"\", train=True, use_advanced=False, img_col='path'):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.data_dir = data_dir\n",
        "        self.train = train\n",
        "        self.use_advanced = use_advanced\n",
        "        self.img_col = img_col\n",
        "        self.failed_images = []\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def _load_image(self, img_path):\n",
        "        \"\"\"ì•ˆì „í•œ ì´ë¯¸ì§€ ë¡œë“œ with fallback\"\"\"\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            # ì´ë¯¸ì§€ ê²€ì¦\n",
        "            if img.size[0] < 10 or img.size[1] < 10:\n",
        "                raise ValueError(f\"ì´ë¯¸ì§€ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {img.size}\")\n",
        "            return img, True\n",
        "        except Exception as e:\n",
        "            if len(self.failed_images) < 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸\n",
        "                logger.warning(f\"âš ï¸  ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ({img_path}): {e}\")\n",
        "            self.failed_images.append(img_path)\n",
        "            # Fallback: í°ìƒ‰ ì´ë¯¸ì§€\n",
        "            return Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white'), False\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "            \n",
        "            # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "            img_path = os.path.join(self.data_dir, row[self.img_col])\n",
        "            img, success = self._load_image(img_path)\n",
        "            \n",
        "            # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "            user_text = build_mc_prompt(\n",
        "                str(row[\"question\"]), str(row[\"a\"]), \n",
        "                str(row[\"b\"]), str(row[\"c\"]), str(row[\"d\"])\n",
        "            )\n",
        "            \n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": user_text}\n",
        "                ]}\n",
        "            ]\n",
        "            \n",
        "            # í•™ìŠµ ì‹œì—ë§Œ ì •ë‹µ í¬í•¨\n",
        "            answer = None\n",
        "            if self.train:\n",
        "                answer = str(row[\"answer\"]).strip().lower()\n",
        "                if answer not in ['a', 'b', 'c', 'd']:\n",
        "                    logger.warning(f\"âš ï¸  ì˜ëª»ëœ ë‹µë³€ ({idx}): {answer}, 'a'ë¡œ ëŒ€ì²´\")\n",
        "                    answer = 'a'\n",
        "                messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [{\"type\": \"text\", \"text\": answer}]\n",
        "                })\n",
        "            \n",
        "            return {\"messages\": messages, \"image\": img, \"answer\": answer, \"idx\": idx}\n",
        "        \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ Dataset __getitem__ ì—ëŸ¬ (idx={idx}): {e}\")\n",
        "            # Fallback: ë”ë¯¸ ë°ì´í„°\n",
        "            dummy_img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": dummy_img},\n",
        "                    {\"type\": \"text\", \"text\": \"dummy question\\n(a) a\\n(b) b\\n(c) c\\n(d) d\"}\n",
        "                ]}\n",
        "            ]\n",
        "            if self.train:\n",
        "                messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"a\"}]})\n",
        "            return {\"messages\": messages, \"image\": dummy_img, \"answer\": 'a' if self.train else None, \"idx\": idx}\n",
        "\n",
        "@dataclass\n",
        "class DataCollator:\n",
        "    \"\"\"Enhanced Data Collator\"\"\"\n",
        "    processor: Any\n",
        "    train: bool = True\n",
        "    use_advanced: bool = False\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        texts, images, answers = [], [], []\n",
        "        \n",
        "        for sample in batch:\n",
        "            try:\n",
        "                text = self.processor.apply_chat_template(\n",
        "                    sample[\"messages\"],\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=False\n",
        "                )\n",
        "                text = unicodedata.normalize('NFKC', text)\n",
        "                texts.append(text)\n",
        "                images.append(sample[\"image\"])\n",
        "                answers.append(sample[\"answer\"])\n",
        "            except Exception as e:\n",
        "                logger.error(f\"âŒ DataCollator ì—ëŸ¬: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if not texts:\n",
        "            raise ValueError(\"âŒ ë°°ì¹˜ì— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
        "        \n",
        "        # ì¸ì½”ë”©\n",
        "        enc = self.processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # ë¼ë²¨ ë§ˆìŠ¤í‚¹\n",
        "        if self.train:\n",
        "            labels = enc[\"input_ids\"].clone()\n",
        "            for i, answer in enumerate(answers):\n",
        "                if answer is None or answer not in ['a', 'b', 'c', 'd']:\n",
        "                    labels[i, :] = -100\n",
        "                else:\n",
        "                    labels[i, :] = -100\n",
        "                    answer_ids = self.processor.tokenizer.encode(answer, add_special_tokens=False)\n",
        "                    if len(answer_ids) > 0:\n",
        "                        labels[i, -len(answer_ids):] = torch.tensor(answer_ids)\n",
        "            enc[\"labels\"] = labels\n",
        "        \n",
        "        return enc\n",
        "\n",
        "logger.info(\"âœ… Dataset & DataCollator ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– 7. Model & Processor ë¡œë“œ (Enhanced)\n",
        "\n",
        "âœ… **ê°œì„ ì‚¬í•­**:\n",
        "- ì—ëŸ¬ í•¸ë“¤ë§\n",
        "- ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„\n",
        "- ë©”ëª¨ë¦¬ ì²´í¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model_and_processor(model_id, use_advanced=False, max_retries=3):\n",
        "    \"\"\"Enhanced ëª¨ë¸ ë° Processor ìƒì„±\"\"\"\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            logger.info(f\"ğŸ”§ ëª¨ë¸ ë¡œë“œ ì‹œë„ {attempt+1}/{max_retries}...\")\n",
        "            \n",
        "            # ì–‘ìí™” ì„¤ì •\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "            )\n",
        "            \n",
        "            # Processor ë¡œë“œ\n",
        "            processor = AutoProcessor.from_pretrained(\n",
        "                model_id,\n",
        "                min_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
        "                max_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            logger.info(\"âœ… Processor ë¡œë“œ ì™„ë£Œ\")\n",
        "            \n",
        "            # ëª¨ë¸ ë¡œë“œ\n",
        "            if use_advanced:\n",
        "                base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "                    model_id,\n",
        "                    quantization_config=bnb_config,\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    attn_implementation=\"sdpa\",\n",
        "                )\n",
        "            else:\n",
        "                base_model = AutoModelForVision2Seq.from_pretrained(\n",
        "                    model_id,\n",
        "                    quantization_config=bnb_config,\n",
        "                    trust_remote_code=True,\n",
        "                )\n",
        "            logger.info(\"âœ… Base model ë¡œë“œ ì™„ë£Œ\")\n",
        "            \n",
        "            # QLoRA ì¤€ë¹„\n",
        "            base_model = prepare_model_for_kbit_training(base_model)\n",
        "            base_model.gradient_checkpointing_enable()\n",
        "            \n",
        "            # LoRA Config\n",
        "            lora_config = LoraConfig(\n",
        "                r=cfg.LORA_R,\n",
        "                lora_alpha=cfg.LORA_ALPHA,\n",
        "                lora_dropout=cfg.LORA_DROPOUT,\n",
        "                bias=\"none\",\n",
        "                target_modules=cfg.TARGET_MODULES,\n",
        "                task_type=\"CAUSAL_LM\",\n",
        "            )\n",
        "            \n",
        "            # PEFT ëª¨ë¸ ìƒì„±\n",
        "            model = get_peft_model(base_model, lora_config)\n",
        "            model.print_trainable_parameters()\n",
        "            \n",
        "            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "            model = model.to(device)\n",
        "            \n",
        "            # ë©”ëª¨ë¦¬ ì²´í¬\n",
        "            if torch.cuda.is_available():\n",
        "                allocated = torch.cuda.memory_allocated() / 1e9\n",
        "                reserved = torch.cuda.memory_reserved() / 1e9\n",
        "                logger.info(f\"ğŸ’¾ GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
        "            \n",
        "            logger.info(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "            return model, processor\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                logger.info(\"ğŸ”„ ì¬ì‹œë„ ì¤‘...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                import time\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "model, processor = create_model_and_processor(cfg.MODEL_ID, cfg.USE_ADVANCED_MODEL)\n",
        "\n",
        "# ì´ë¯¸ì§€ ì»¬ëŸ¼ ìë™ ê°ì§€\n",
        "img_col = 'path' if 'path' in train_df.columns else 'image'\n",
        "logger.info(f\"ğŸ“· ì´ë¯¸ì§€ ì»¬ëŸ¼: {img_col}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
