{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📒 Kaggle_AllInOne_Pro2_Enhanced.ipynb – 최고 성능 버전\n",
        "\n",
        "## 🎯 Pro2 Enhanced 주요 개선사항\n",
        "\n",
        "### ✅ 학습 개선\n",
        "- Val Accuracy + Confusion Matrix + Classification Report\n",
        "- Best 모델: Val Acc 우선 저장 + 체크포인트 재개\n",
        "- 학습 곡선 시각화 (Train Loss 포함)\n",
        "- 라벨 마스킹 (프롬프트 손실 제외)\n",
        "- 검증 데이터 train=False\n",
        "- Early Stopping 옵션\n",
        "\n",
        "### ✅ 추론 개선\n",
        "- Direct Logits (개선된 토큰 확률 계산)\n",
        "- TTA [0.9, 1.0, 1.1] + 배치 추론\n",
        "- pad_token_id 자동 보정\n",
        "- 에러 핸들링 강화\n",
        "\n",
        "### ✅ 앙상블 개선\n",
        "- Temperature Scaling (실제 구현)\n",
        "- 확률 앙상블 + Weighted Voting\n",
        "- 확률 컬럼 저장\n",
        "\n",
        "### ✅ 시스템 개선\n",
        "- 강력한 에러 핸들링\n",
        "- 로깅 시스템 (파일 + 콘솔)\n",
        "- 메모리 최적화\n",
        "- 코드 중복 제거\n",
        "\n",
        "### ⚙️ 튜닝 설정\n",
        "```python\n",
        "USE_SAMPLE=False, IMAGE_SIZE=512, NUM_EPOCHS=3\n",
        "GRAD_ACCUM_STEPS=8, WARMUP_RATIO=0.06, LORA_R=16\n",
        "USE_DIRECT_LOGIT_DECODE=True, TTA_SCALES=[0.9,1.0,1.1]\n",
        "ENSEMBLE_METHOD='prob', USE_TEMPERATURE_SCALING=True\n",
        "EARLY_STOPPING_PATIENCE=2\n",
        "```\n",
        "\n",
        "**🤖 SSAFY AI Project 2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📦 1. 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q transformers accelerate peft bitsandbytes datasets pillow pandas torch torchvision scikit-learn matplotlib seaborn tqdm scipy --upgrade\n",
        "# !pip install -q qwen-vl-utils==0.0.8\n",
        "print(\"✅ 설치 완료! 런타임 재시작하세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 2. 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, re, math, random, warnings, json, pickle, logging\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "import unicodedata\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForVision2Seq,\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"🔧 Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ 3. Config 설정 (Pro2 Enhanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # 시드\n",
        "    SEED = 42\n",
        "    \n",
        "    # 모델\n",
        "    MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "    IMAGE_SIZE = 512\n",
        "    USE_ADVANCED_MODEL = False\n",
        "    \n",
        "    # 데이터\n",
        "    DATA_DIR = \"/content\"\n",
        "    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
        "    TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
        "    \n",
        "    # K-Fold\n",
        "    N_FOLDS = 3\n",
        "    USE_KFOLD = True\n",
        "    TRAIN_FOLDS = [0, 1, 2]\n",
        "    \n",
        "    # QLoRA\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 32\n",
        "    LORA_DROPOUT = 0.05\n",
        "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    \n",
        "    # 학습\n",
        "    NUM_EPOCHS = 3\n",
        "    BATCH_SIZE = 1\n",
        "    GRAD_ACCUM_STEPS = 8\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    WARMUP_RATIO = 0.06\n",
        "    MAX_GRAD_NORM = 1.0\n",
        "    \n",
        "    # Early Stopping\n",
        "    USE_EARLY_STOPPING = False\n",
        "    EARLY_STOPPING_PATIENCE = 2\n",
        "    \n",
        "    # 고급 기법\n",
        "    USE_AMP = True\n",
        "    USE_EMA = True\n",
        "    EMA_DECAY = 0.999\n",
        "    USE_SWA = True\n",
        "    SWA_START_EPOCH = 1\n",
        "    USE_COSINE_SCHEDULE = True\n",
        "    \n",
        "    # TTA\n",
        "    USE_TTA = True\n",
        "    TTA_SCALES = [0.9, 1.0, 1.1]\n",
        "    \n",
        "    # 추론\n",
        "    USE_DIRECT_LOGIT_DECODE = True\n",
        "    USE_BATCH_INFERENCE = True  # Enhanced: 배치 추론 활성화\n",
        "    INFER_BATCH_SIZE = 4\n",
        "    MAX_NEW_TOKENS = 8\n",
        "    \n",
        "    # Temperature Scaling\n",
        "    USE_TEMPERATURE_SCALING = True\n",
        "    \n",
        "    # 앙상블\n",
        "    ENSEMBLE_METHOD = \"prob\"  # \"prob\" or \"vote\" or \"weighted\"\n",
        "    FOLD_WEIGHTS = None  # None이면 동일 가중치, [0.4, 0.3, 0.3] 등 가능\n",
        "    \n",
        "    # 저장\n",
        "    SAVE_DIR = f\"{DATA_DIR}/checkpoints\"\n",
        "    OUTPUT_DIR = f\"{DATA_DIR}/outputs\"\n",
        "    LOG_DIR = f\"{DATA_DIR}/logs\"\n",
        "    SAVE_EVERY_EPOCH = True  # 매 에폭마다 체크포인트 저장\n",
        "    \n",
        "    # 샘플링\n",
        "    USE_SAMPLE = False\n",
        "    SAMPLE_SIZE = 200\n",
        "    \n",
        "    # 프롬프트\n",
        "    SYSTEM_INSTRUCT = (\n",
        "        \"You are a helpful visual question answering assistant. \"\n",
        "        \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n",
        "    )\n",
        "    \n",
        "    # 로깅\n",
        "    LOG_LEVEL = logging.INFO\n",
        "    LOG_TO_FILE = True\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# 디렉토리 생성\n",
        "for dir_path in [cfg.SAVE_DIR, cfg.OUTPUT_DIR, cfg.LOG_DIR]:\n",
        "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 로깅 설정\n",
        "def setup_logging():\n",
        "    logger = logging.getLogger('VQA')\n",
        "    logger.setLevel(cfg.LOG_LEVEL)\n",
        "    logger.handlers.clear()\n",
        "    \n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    \n",
        "    # 콘솔 핸들러\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "    \n",
        "    # 파일 핸들러\n",
        "    if cfg.LOG_TO_FILE:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        file_handler = logging.FileHandler(f\"{cfg.LOG_DIR}/training_{timestamp}.log\")\n",
        "        file_handler.setFormatter(formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "    \n",
        "    return logger\n",
        "\n",
        "logger = setup_logging()\n",
        "\n",
        "# 시드 고정\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(cfg.SEED)\n",
        "logger.info(f\"✅ Config 설정 완료\")\n",
        "logger.info(f\"   Model: {cfg.MODEL_ID}\")\n",
        "logger.info(f\"   Image Size: {cfg.IMAGE_SIZE}\")\n",
        "logger.info(f\"   Epochs: {cfg.NUM_EPOCHS}, Grad Accum: {cfg.GRAD_ACCUM_STEPS}\")\n",
        "logger.info(f\"   LoRA R: {cfg.LORA_R}, Warmup: {cfg.WARMUP_RATIO}\")\n",
        "logger.info(f\"   Direct Logits: {cfg.USE_DIRECT_LOGIT_DECODE}, TTA: {cfg.USE_TTA}\")\n",
        "logger.info(f\"   Ensemble: {cfg.ENSEMBLE_METHOD}, Temp Scaling: {cfg.USE_TEMPERATURE_SCALING}\")\n",
        "print(f\"\\n📝 로그 저장 위치: {cfg.LOG_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 4. 데이터 로드 & EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
        "    test_df = pd.read_csv(cfg.TEST_CSV)\n",
        "    logger.info(f\"📁 Train: {len(train_df):,} samples\")\n",
        "    logger.info(f\"📁 Test: {len(test_df):,} samples\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"❌ 데이터 로드 실패: {e}\")\n",
        "    raise\n",
        "\n",
        "# 데이터 검증\n",
        "required_cols = ['question', 'a', 'b', 'c', 'd', 'answer']\n",
        "missing_cols = set(required_cols) - set(train_df.columns)\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"❌ 필수 컬럼 누락: {missing_cols}\")\n",
        "\n",
        "# 이미지 경로 컬럼 확인\n",
        "img_col = 'path' if 'path' in train_df.columns else 'image'\n",
        "logger.info(f\"📷 이미지 컬럼: {img_col}\")\n",
        "\n",
        "if cfg.USE_SAMPLE:\n",
        "    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n",
        "    logger.warning(f\"⚠️  Sampled {len(train_df)} samples\")\n",
        "\n",
        "logger.info(f\"\\n📊 Answer Distribution:\")\n",
        "answer_dist = train_df['answer'].value_counts().sort_index()\n",
        "for ans, count in answer_dist.items():\n",
        "    logger.info(f\"   {ans}: {count:4d} ({count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "# 시각화\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "answer_dist.plot(kind='bar', ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('Answer Distribution (Train)', fontsize=12, weight='bold')\n",
        "axes[0].set_xlabel('Answer')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "train_df['question_len'] = train_df['question'].str.len()\n",
        "train_df['question_len'].hist(bins=30, ax=axes[1], color='salmon')\n",
        "axes[1].set_title('Question Length Distribution', fontsize=12, weight='bold')\n",
        "axes[1].set_xlabel('Length (chars)')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{cfg.LOG_DIR}/data_distribution.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "logger.info(f\"✅ 데이터 로드 및 EDA 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 5. Stratified K-Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if cfg.USE_KFOLD:\n",
        "    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n",
        "    train_df['fold'] = -1\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n",
        "        train_df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "    logger.info(f\"✅ {cfg.N_FOLDS}-Fold CV 생성\")\n",
        "    fold_dist = train_df['fold'].value_counts().sort_index()\n",
        "    for fold, count in fold_dist.items():\n",
        "        logger.info(f\"   Fold {fold}: {count:4d} samples\")\n",
        "    \n",
        "    # Fold별 답변 분포 확인\n",
        "    logger.info(f\"\\n📊 Answer Distribution per Fold:\")\n",
        "    for fold in range(cfg.N_FOLDS):\n",
        "        fold_data = train_df[train_df['fold'] == fold]\n",
        "        dist = fold_data['answer'].value_counts(normalize=True).sort_index()\n",
        "        dist_str = \", \".join([f\"{k}:{v:.2%}\" for k, v in dist.items()])\n",
        "        logger.info(f\"   Fold {fold}: {dist_str}\")\n",
        "else:\n",
        "    split_idx = int(len(train_df) * 0.9)\n",
        "    train_df['fold'] = -1\n",
        "    train_df.loc[split_idx:, 'fold'] = 0\n",
        "    logger.info(f\"✅ Single split (90:10)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🗂️ 6. Dataset & DataCollator (Enhanced)\n",
        "\n",
        "✅ **개선사항**:\n",
        "- 에러 핸들링 강화\n",
        "- 이미지 로드 실패 시 fallback\n",
        "- 라벨 마스킹 정교화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_mc_prompt(question, a, b, c, d):\n",
        "    \"\"\"Multiple Choice 프롬프트 생성\"\"\"\n",
        "    return (\n",
        "        f\"{question}\\n\"\n",
        "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
        "        \"정답을 반드시 a, b, c, d 중 하나의 소문자 한 글자로만 출력하세요.\"\n",
        "    )\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    \"\"\"Enhanced VQA Dataset with Error Handling\"\"\"\n",
        "    \n",
        "    def __init__(self, df, processor, data_dir=\"\", train=True, use_advanced=False, img_col='path'):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.data_dir = data_dir\n",
        "        self.train = train\n",
        "        self.use_advanced = use_advanced\n",
        "        self.img_col = img_col\n",
        "        self.failed_images = []\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def _load_image(self, img_path):\n",
        "        \"\"\"안전한 이미지 로드 with fallback\"\"\"\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            # 이미지 검증\n",
        "            if img.size[0] < 10 or img.size[1] < 10:\n",
        "                raise ValueError(f\"이미지 크기가 너무 작음: {img.size}\")\n",
        "            return img, True\n",
        "        except Exception as e:\n",
        "            if len(self.failed_images) < 10:  # 처음 10개만 로그\n",
        "                logger.warning(f\"⚠️  이미지 로드 실패 ({img_path}): {e}\")\n",
        "            self.failed_images.append(img_path)\n",
        "            # Fallback: 흰색 이미지\n",
        "            return Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white'), False\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "            \n",
        "            # 이미지 로드\n",
        "            img_path = os.path.join(self.data_dir, row[self.img_col])\n",
        "            img, success = self._load_image(img_path)\n",
        "            \n",
        "            # 프롬프트 생성\n",
        "            user_text = build_mc_prompt(\n",
        "                str(row[\"question\"]), str(row[\"a\"]), \n",
        "                str(row[\"b\"]), str(row[\"c\"]), str(row[\"d\"])\n",
        "            )\n",
        "            \n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": user_text}\n",
        "                ]}\n",
        "            ]\n",
        "            \n",
        "            # 학습 시에만 정답 포함\n",
        "            answer = None\n",
        "            if self.train:\n",
        "                answer = str(row[\"answer\"]).strip().lower()\n",
        "                if answer not in ['a', 'b', 'c', 'd']:\n",
        "                    logger.warning(f\"⚠️  잘못된 답변 ({idx}): {answer}, 'a'로 대체\")\n",
        "                    answer = 'a'\n",
        "                messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [{\"type\": \"text\", \"text\": answer}]\n",
        "                })\n",
        "            \n",
        "            return {\"messages\": messages, \"image\": img, \"answer\": answer, \"idx\": idx}\n",
        "        \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Dataset __getitem__ 에러 (idx={idx}): {e}\")\n",
        "            # Fallback: 더미 데이터\n",
        "            dummy_img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": dummy_img},\n",
        "                    {\"type\": \"text\", \"text\": \"dummy question\\n(a) a\\n(b) b\\n(c) c\\n(d) d\"}\n",
        "                ]}\n",
        "            ]\n",
        "            if self.train:\n",
        "                messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"a\"}]})\n",
        "            return {\"messages\": messages, \"image\": dummy_img, \"answer\": 'a' if self.train else None, \"idx\": idx}\n",
        "\n",
        "@dataclass\n",
        "class DataCollator:\n",
        "    \"\"\"Enhanced Data Collator\"\"\"\n",
        "    processor: Any\n",
        "    train: bool = True\n",
        "    use_advanced: bool = False\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        texts, images, answers = [], [], []\n",
        "        \n",
        "        for sample in batch:\n",
        "            try:\n",
        "                text = self.processor.apply_chat_template(\n",
        "                    sample[\"messages\"],\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=False\n",
        "                )\n",
        "                text = unicodedata.normalize('NFKC', text)\n",
        "                texts.append(text)\n",
        "                images.append(sample[\"image\"])\n",
        "                answers.append(sample[\"answer\"])\n",
        "            except Exception as e:\n",
        "                logger.error(f\"❌ DataCollator 에러: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if not texts:\n",
        "            raise ValueError(\"❌ 배치에 유효한 데이터가 없습니다\")\n",
        "        \n",
        "        # 인코딩\n",
        "        enc = self.processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # 라벨 마스킹\n",
        "        if self.train:\n",
        "            labels = enc[\"input_ids\"].clone()\n",
        "            for i, answer in enumerate(answers):\n",
        "                if answer is None or answer not in ['a', 'b', 'c', 'd']:\n",
        "                    labels[i, :] = -100\n",
        "                else:\n",
        "                    labels[i, :] = -100\n",
        "                    answer_ids = self.processor.tokenizer.encode(answer, add_special_tokens=False)\n",
        "                    if len(answer_ids) > 0:\n",
        "                        labels[i, -len(answer_ids):] = torch.tensor(answer_ids)\n",
        "            enc[\"labels\"] = labels\n",
        "        \n",
        "        return enc\n",
        "\n",
        "logger.info(\"✅ Dataset & DataCollator 정의 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 7. Model & Processor 로드 (Enhanced)\n",
        "\n",
        "✅ **개선사항**:\n",
        "- 에러 핸들링\n",
        "- 모델 로드 실패 시 재시도\n",
        "- 메모리 체크"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model_and_processor(model_id, use_advanced=False, max_retries=3):\n",
        "    \"\"\"Enhanced 모델 및 Processor 생성\"\"\"\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            logger.info(f\"🔧 모델 로드 시도 {attempt+1}/{max_retries}...\")\n",
        "            \n",
        "            # 양자화 설정\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "            )\n",
        "            \n",
        "            # Processor 로드\n",
        "            processor = AutoProcessor.from_pretrained(\n",
        "                model_id,\n",
        "                min_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
        "                max_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            logger.info(\"✅ Processor 로드 완료\")\n",
        "            \n",
        "            # 모델 로드\n",
        "            if use_advanced:\n",
        "                base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "                    model_id,\n",
        "                    quantization_config=bnb_config,\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    attn_implementation=\"sdpa\",\n",
        "                )\n",
        "            else:\n",
        "                base_model = AutoModelForVision2Seq.from_pretrained(\n",
        "                    model_id,\n",
        "                    quantization_config=bnb_config,\n",
        "                    trust_remote_code=True,\n",
        "                )\n",
        "            logger.info(\"✅ Base model 로드 완료\")\n",
        "            \n",
        "            # QLoRA 준비\n",
        "            base_model = prepare_model_for_kbit_training(base_model)\n",
        "            base_model.gradient_checkpointing_enable()\n",
        "            \n",
        "            # LoRA Config\n",
        "            lora_config = LoraConfig(\n",
        "                r=cfg.LORA_R,\n",
        "                lora_alpha=cfg.LORA_ALPHA,\n",
        "                lora_dropout=cfg.LORA_DROPOUT,\n",
        "                bias=\"none\",\n",
        "                target_modules=cfg.TARGET_MODULES,\n",
        "                task_type=\"CAUSAL_LM\",\n",
        "            )\n",
        "            \n",
        "            # PEFT 모델 생성\n",
        "            model = get_peft_model(base_model, lora_config)\n",
        "            model.print_trainable_parameters()\n",
        "            \n",
        "            # 디바이스로 이동\n",
        "            model = model.to(device)\n",
        "            \n",
        "            # 메모리 체크\n",
        "            if torch.cuda.is_available():\n",
        "                allocated = torch.cuda.memory_allocated() / 1e9\n",
        "                reserved = torch.cuda.memory_reserved() / 1e9\n",
        "                logger.info(f\"💾 GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
        "            \n",
        "            logger.info(\"✅ 모델 로드 완료\")\n",
        "            return model, processor\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 모델 로드 실패 (시도 {attempt+1}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                logger.info(\"🔄 재시도 중...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                import time\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "model, processor = create_model_and_processor(cfg.MODEL_ID, cfg.USE_ADVANCED_MODEL)\n",
        "\n",
        "# 이미지 컬럼 자동 감지\n",
        "img_col = 'path' if 'path' in train_df.columns else 'image'\n",
        "logger.info(f\"📷 이미지 컬럼: {img_col}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
