{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📒 Kaggle_AllInOne_Pro.ipynb – 단일 노트북 통합 버전\n",
    "\n",
    "## 🎯 개요\n",
    "\n",
    "본 노트북은 **VQA Kaggle Challenge**를 위한 **완전 통합 고성능 파이프라인**입니다.\n",
    "\n",
    "### ✨ 주요 기능\n",
    "\n",
    "- ✅ **T4 GPU 완벽 호환** (Float16, SDPA attention)\n",
    "- ✅ **라벨 정렬 교정** (Assistant 메시지에 정답 포함)\n",
    "- ✅ **K-Fold Cross-Validation** (Stratified)\n",
    "- ✅ **고급 학습 기법** (AMP, EMA, SWA, Cosine Warmup)\n",
    "- ✅ **데이터 증강** (Choice Shuffle, Paraphrase)\n",
    "- ✅ **TTA (Test-Time Augmentation)**\n",
    "- ✅ **앙상블** (Weighted Voting)\n",
    "- ✅ **메모리 최적화** (Gradient Checkpointing, 4-bit QLoRA)\n",
    "\n",
    "### 📊 예상 성능\n",
    "\n",
    "| 설정 | 정확도 | 시간 |\n",
    "|------|--------|------|\n",
    "| Single Fold | 79-82% | ~4h |\n",
    "| 3-Fold Ensemble | 83-85% | ~12h |\n",
    "| + TTA + Optimization | 85-88% | ~15h |\n",
    "\n",
    "### 🚀 실행 순서\n",
    "\n",
    "1. **환경 설정** - 패키지 설치 및 임포트\n",
    "2. **Config** - 하이퍼파라미터 설정\n",
    "3. **데이터 로드** - Train/Test 데이터 로드\n",
    "4. **EDA** - 탐색적 데이터 분석\n",
    "5. **Stratified K-Fold** - CV Splits 생성\n",
    "6. **Dataset & DataLoader** - 커스텀 데이터셋 정의\n",
    "7. **Model & Processor** - QLoRA 모델 로드\n",
    "8. **Training Loop** - 고급 기법 적용 학습\n",
    "9. **Inference** - TTA를 활용한 추론\n",
    "10. **Ensemble** - 앙상블 및 제출 파일 생성\n",
    "\n",
    "---\n",
    "\n",
    "**🤖 Generated for SSAFY AI Project 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 1. 환경 설정 및 패키지 설치\n",
    "\n",
    "필요한 라이브러리를 설치합니다. (첫 실행 시 1회만)\n",
    "\n",
    "### ⚠️ 중요: 설치 후 런타임 재시작 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글드라이브 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:19:36.203531Z",
     "iopub.status.busy": "2025-10-24T07:19:36.202944Z",
     "iopub.status.idle": "2025-10-24T07:19:45.714082Z",
     "shell.execute_reply": "2025-10-24T07:19:45.713005Z",
     "shell.execute_reply.started": "2025-10-24T07:19:36.203502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "✅ 패키지 설치 완료! 런타임을 재시작하세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.27 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.5.0 which is incompatible.\n",
      "langchain-community 0.3.30 requires pydantic-settings<3.0.0,>=2.10.1, but you have pydantic-settings 2.1.0 which is incompatible.\n",
      "llama-index-core 0.14.4 requires pydantic>=2.8.0, but you have pydantic 2.5.0 which is incompatible.\n",
      "llama-index-embeddings-google 0.4.1 requires google-generativeai<0.6,>=0.5.2, but you have google-generativeai 0.8.5 which is incompatible.\n",
      "llama-index-readers-file 0.5.4 requires beautifulsoup4<5,>=4.12.3, but you have beautifulsoup4 4.12.2 which is incompatible.\n",
      "llama-index-readers-file 0.5.4 requires pandas<2.3.0, but you have pandas 2.3.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 패키지 설치 (Colab/Kaggle 환경)\n",
    "# 첫 실행 시에만 주석 해제하고 실행\n",
    "!pip install -q \"transformers>=4.44.2\" \"accelerate>=0.34.2\" \"peft>=0.13.2\" \\\n",
    "    \"bitsandbytes>=0.43.1\" datasets pillow pandas torch torchvision nltk \\\n",
    "    scikit-learn matplotlib seaborn tqdm sentencepiece --upgrade\n",
    "!pip install -q qwen-vl-utils==0.0.8\n",
    "\n",
    "print(\"✅ 패키지 설치 완료! 런타임을 재시작하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 2. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, sys, re, math, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Optional\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "# Transformers & PEFT\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    LogitsProcessorList\n",
    ")\n",
    "import torchvision.transforms as T\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Env hygiene\n",
    "warnings.filterwarnings('ignore')\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Runtime info\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\" Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\" Python: {sys.version.split()[0]}\")\n",
    "print(f\" PyTorch: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ 3. Config 설정\n",
    "\n",
    "모든 하이퍼파라미터를 한 곳에서 관리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "﻿import os\n",
    "import torch\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Qwen3-VL-8B Instruct 설정 (FP16)\n",
    "\n",
    "    목표: 높은 정답률과 안정적/빠른 실행\n",
    "    - 모델: Qwen/Qwen3-VL-8B-Instruct (FP16)\n",
    "    - 정밀도: AMP(FP16) + SDPA 어텐션\n",
    "    - 라벨 마스킹: 프롬프트(-100), 정답만 학습\n",
    "    - 로짓 제한: a/b/c/d만 생성 (추론 안정화)\n",
    "    - 자동 튜닝: GPU VRAM에 맞춰 배치/누적 스텝 자동 결정\n",
    "\n",
    "    전환 옵션:\n",
    "    - QUANTIZATION=\"bnb4\" → 4bit QLoRA (메모리 절약)\n",
    "    - QUANTIZATION=\"fp16\" → FP16 (기본, 정확도 우선)\n",
    "\n",
    "    권장 실행 순서:\n",
    "    1) 데이터 로드/EDA\n",
    "    2) K-Fold 분할(선택): USE_KFOLD True\n",
    "    3) 모델 로드(자동 튜닝 포함)\n",
    "    4) 학습(라벨 마스킹 적용)\n",
    "    5) 추론(TTA/로짓 제한)\n",
    "    6) 제출 파일 생성\n",
    "    \"\"\"\n",
    "\n",
    "    # Seed\n",
    "    SEED = 42\n",
    "\n",
    "    # Model\n",
    "    MODEL_ID = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
    "    # Quantization/backend selection: \"fp16\" | \"bnb4\" | \"fp8\"\n",
    "    QUANTIZATION = \"fp16\"\n",
    "    IMAGE_SIZE = 384  # 384/512/768\n",
    "    USE_ADVANCED_MODEL = False\n",
    "    USE_DATAPARALLEL = False\n",
    "    NUM_WORKERS = 4\n",
    "\n",
    "    # Data\n",
    "    DATA_DIR = \"/kaggle/input/ssafy-ai-pjt-data\"\n",
    "    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
    "    TEST_CSV  = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "    # K-Fold\n",
    "    N_FOLDS = 3\n",
    "    USE_KFOLD = False   # 정확도 극대화 시 True (학습 시간 증가)\n",
    "    TRAIN_FOLDS = [0, 1, 2]\n",
    "\n",
    "    # LoRA / QLoRA\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    TARGET_MODULES = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "\n",
    "    # Training\n",
    "    NUM_EPOCHS = 1     # 정확도 ↑ 원하면 3~5로 증가\n",
    "    BATCH_SIZE = 1     # 자동 튜닝으로 조정됨\n",
    "    GRAD_ACCUM_STEPS = 4\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_RATIO = 0.03\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "\n",
    "    # Precision & schedules\n",
    "    USE_AMP = True\n",
    "    USE_EMA = True\n",
    "    EMA_DECAY = 0.999\n",
    "    USE_SWA = False\n",
    "    SWA_START_EPOCH = 0\n",
    "    USE_COSINE_SCHEDULE = True\n",
    "\n",
    "    # Augment\n",
    "    USE_IMAGE_AUGMENTATION = False\n",
    "    USE_RANDAUGMENT = True\n",
    "    RANDAUG_N = 2\n",
    "    RANDAUG_M = 9\n",
    "    USE_MIXUP = False\n",
    "    MIXUP_ALPHA = 0.2\n",
    "    USE_CUTMIX = False\n",
    "    CUTMIX_ALPHA = 1.0\n",
    "\n",
    "    # Text aug\n",
    "    USE_BACK_TRANSLATION = False\n",
    "    USE_SYNONYM_AUG = False\n",
    "    TEXT_AUG_PROB = 0.15\n",
    "\n",
    "    # TTA\n",
    "    USE_TTA = False\n",
    "    TTA_SCALES = [0.9, 1.0, 1.1]\n",
    "    TTA_HFLIP = True\n",
    "\n",
    "    # Generation\n",
    "    MAX_NEW_TOKENS = 1\n",
    "    DO_SAMPLE = False\n",
    "    TEMPERATURE = 0.0\n",
    "\n",
    "    # Paths\n",
    "    SAVE_DIR = f\"/kaggle/working/checkpoints\"\n",
    "    OUTPUT_DIR = f\"/kaggle/working/outputs\"\n",
    "\n",
    "    # Sampling\n",
    "    USE_SAMPLE = False  # 정확도 우선: 전체 데이터 사용\n",
    "    SAMPLE_SIZE = 200\n",
    "\n",
    "    # Ensemble\n",
    "    ENSEMBLE_WEIGHTS = None\n",
    "\n",
    "    # Chat system text\n",
    "    SYSTEM_INSTRUCT = (\n",
    "        \"You are a helpful visual question answering assistant. \"\n",
    "        \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n",
    "    )\n",
    "\n",
    "    # Sequence\n",
    "    MAX_SEQUENCE_LENGTH = 1024\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random, numpy as np, torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(cfg.SEED)\n",
    "\n",
    "# GPU perf knobs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def _auto_scale_training(cfg):\n",
    "    total_gb = 0.0\n",
    "    if torch.cuda.is_available():\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        total_gb = props.total_memory / 1024 ** 3\n",
    "\n",
    "    # User can override\n",
    "    target_effective = int(os.environ.get(\"EFFECTIVE_BATCH_SIZE\", 16))\n",
    "    suggested = 1\n",
    "    if total_gb >= 70:\n",
    "        suggested = 4\n",
    "    elif total_gb >= 40:\n",
    "        suggested = 2\n",
    "    else:\n",
    "        suggested = 1\n",
    "    per_device = int(os.environ.get(\"PER_DEVICE_BATCH\", suggested))\n",
    "\n",
    "    cfg.BATCH_SIZE = max(1, per_device)\n",
    "    cfg.GRAD_ACCUM_STEPS = max(1, target_effective // cfg.BATCH_SIZE)\n",
    "\n",
    "    print(\n",
    "        f\" Auto-scale: GPU {total_gb:.1f} GB | per_device_batch={cfg.BATCH_SIZE} | \"\n",
    "        f\"grad_accum={cfg.GRAD_ACCUM_STEPS} | target_effective={target_effective}\"\n",
    "    )\n",
    "\n",
    "\n",
    "_auto_scale_training(cfg)\n",
    "print(f\" Config loaded (Seed: {cfg.SEED})\")\n",
    "print(f\"   Model: {cfg.MODEL_ID}\")\n",
    "print(\n",
    "    f\"   Training: BATCH={cfg.BATCH_SIZE}, ACCUM={cfg.GRAD_ACCUM_STEPS}, LR={cfg.LEARNING_RATE}, Cosine={cfg.USE_COSINE_SCHEDULE}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Advanced: AMP={cfg.USE_AMP}, EMA={cfg.USE_EMA}, SWA={cfg.USE_SWA}, TTA={cfg.USE_TTA}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 4. 데이터 로드 및 EDA\n",
    "\n",
    "데이터를 로드하고 간단한 탐색적 분석을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:19:55.638548Z",
     "iopub.status.busy": "2025-10-24T07:19:55.637495Z",
     "iopub.status.idle": "2025-10-24T07:19:56.071371Z",
     "shell.execute_reply": "2025-10-24T07:19:56.070573Z",
     "shell.execute_reply.started": "2025-10-24T07:19:55.638517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
    "test_df = pd.read_csv(cfg.TEST_CSV)\n",
    "\n",
    "print(f\"📁 Train: {len(train_df):,} samples\")\n",
    "print(f\"📁 Test: {len(test_df):,} samples\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "\n",
    "# 샘플링 (디버깅용)\n",
    "if cfg.USE_SAMPLE:\n",
    "    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n",
    "    print(f\"\\n⚠️  Sampled {len(train_df)} samples for quick testing\")\n",
    "\n",
    "# 기본 통계\n",
    "print(f\"\\n📊 Answer Distribution:\")\n",
    "print(train_df['answer'].value_counts().sort_index())\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# 답변 분포\n",
    "train_df['answer'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Answer Distribution (Train)', fontsize=12, weight='bold')\n",
    "axes[0].set_xlabel('Answer')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 질문 길이 분포\n",
    "train_df['question_len'] = train_df['question'].str.len()\n",
    "train_df['question_len'].hist(bins=30, ax=axes[1], color='salmon', edgecolor='black')\n",
    "axes[1].set_title('Question Length Distribution', fontsize=12, weight='bold')\n",
    "axes[1].set_xlabel('Length (chars)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 샘플 출력\n",
    "print(\"\\n📝 Sample Data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 5. Stratified K-Fold Cross-Validation\n",
    "\n",
    "답변 분포를 유지하면서 K-Fold를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:19:56.073478Z",
     "iopub.status.busy": "2025-10-24T07:19:56.073212Z",
     "iopub.status.idle": "2025-10-24T07:19:56.082402Z",
     "shell.execute_reply": "2025-10-24T07:19:56.081499Z",
     "shell.execute_reply.started": "2025-10-24T07:19:56.073437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if cfg.USE_KFOLD:\n",
    "    # Stratified K-Fold 생성\n",
    "    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n",
    "    train_df['fold'] = -1\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n",
    "        train_df.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "    print(f\"✅ {cfg.N_FOLDS}-Fold CV 생성 완료\")\n",
    "    print(f\"\\nFold Distribution:\")\n",
    "    print(train_df['fold'].value_counts().sort_index())\n",
    "    \n",
    "    # Fold별 답변 분포 확인\n",
    "    print(f\"\\nAnswer Distribution per Fold:\")\n",
    "    for fold in range(cfg.N_FOLDS):\n",
    "        fold_data = train_df[train_df['fold'] == fold]\n",
    "        dist = fold_data['answer'].value_counts(normalize=True).sort_index()\n",
    "        print(f\"Fold {fold}: {dict(dist)}\")\n",
    "else:\n",
    "    # 단일 모델 학습 (90:10 split)\n",
    "    split_idx = int(len(train_df) * 0.9)\n",
    "    train_df['fold'] = -1\n",
    "    train_df.loc[split_idx:, 'fold'] = 0\n",
    "    print(f\"✅ Single split (90:10) 생성 완료\")\n",
    "    print(f\"   Train: {len(train_df[train_df['fold'] == -1])}\")\n",
    "    print(f\"   Valid: {len(train_df[train_df['fold'] == 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗂️ 6. Dataset & DataLoader\n",
    "\n",
    "커스텀 데이터셋 및 DataCollator를 정의합니다.\n",
    "\n",
    "### ✅ 라벨 정렬 교정 적용\n",
    "- Assistant 메시지에 정답 포함\n",
    "- `add_generation_prompt=False` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json, unicodedata\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "def build_mc_prompt(question, a, b, c, d):\n",
    "    \"\"\"Multiple Choice prompt\"\"\"\n",
    "    return (\n",
    "        f\"{question}\\n\"\n",
    "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
    "        \"답변은 a, b, c, d 중 하나의 문자만 출력하세요.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def synonym_replace(text: str) -> str:\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.corpus import wordnet as wn\n",
    "        try:\n",
    "            wn.synsets(\"test\")\n",
    "        except LookupError:\n",
    "            nltk.download(\"wordnet\")\n",
    "            nltk.download(\"omw-1.4\")\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return text\n",
    "        idx = np.random.randint(len(words))\n",
    "        syns = wn.synsets(words[idx])\n",
    "        lemmas = [l.name().replace(\"_\", \" \") for s in syns for l in s.lemmas()]\n",
    "        lemmas = [w for w in lemmas if w.lower() != words[idx].lower()]\n",
    "        if not lemmas:\n",
    "            return text\n",
    "        words[idx] = np.random.choice(lemmas)\n",
    "        return \" \".join(words)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "\n",
    "def maybe_augment_text(question: str, options: Dict[str, str]) -> (str, Dict[str, str]):\n",
    "    q, opts = question, dict(options)\n",
    "    if cfg.USE_SYNONYM_AUG and np.random.rand() < cfg.TEXT_AUG_PROB:\n",
    "        q = synonym_replace(q)\n",
    "    return q, opts\n",
    "\n",
    "\n",
    "class VQADataset(Dataset):\n",
    "    \"\"\"VQA Dataset (assistant label returned separately for proper masking).\"\"\"\n",
    "\n",
    "    def __init__(self, df, processor, data_dir=\"\", train=True, use_advanced=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.data_dir = data_dir\n",
    "        self.train = train\n",
    "        self.use_advanced = use_advanced\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_path = os.path.join(self.data_dir, row[\"path\"]) if \"path\" in row else row.get(\"image_path\", \"\")\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n",
    "\n",
    "        q0 = str(row[\"question\"]) if \"question\" in row else \"\"\n",
    "        opts0 = {\n",
    "            \"a\": str(row.get(\"a\", \"\")),\n",
    "            \"b\": str(row.get(\"b\", \"\")),\n",
    "            \"c\": str(row.get(\"c\", \"\")),\n",
    "            \"d\": str(row.get(\"d\", \"\")),\n",
    "        }\n",
    "        if self.train:\n",
    "            q0, opts0 = maybe_augment_text(q0, opts0)\n",
    "\n",
    "        user_text = build_mc_prompt(q0, opts0[\"a\"], opts0[\"b\"], opts0[\"c\"], opts0[\"d\"])\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": user_text}\n",
    "            ]}\n",
    "        ]\n",
    "\n",
    "        out = {\"messages\": messages, \"image\": img}\n",
    "        if self.train:\n",
    "            out[\"answer\"] = str(row.get(\"answer\", \"\")).strip().lower()\n",
    "        return out\n",
    "\n",
    "\n",
    "def _build_image_transform():\n",
    "    if not cfg.USE_IMAGE_AUGMENTATION:\n",
    "        return None\n",
    "    ops = [T.RandomHorizontalFlip(p=0.5)]\n",
    "    if cfg.USE_RANDAUGMENT:\n",
    "        try:\n",
    "            ops.append(T.RandAugment(num_ops=cfg.RANDAUG_N, magnitude=cfg.RANDAUG_M))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return T.Compose(ops)\n",
    "\n",
    "\n",
    "_IMG_TF = _build_image_transform()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollator:\n",
    "    \"\"\"Data Collator with correct prompt masking for decoder-only training.\"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    train: bool = True\n",
    "    use_advanced: bool = False\n",
    "    augment_images: bool = True\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, prompts, full_texts = [], [], []\n",
    "\n",
    "        for sample in batch:\n",
    "            img = sample[\"image\"]\n",
    "            if self.train and self.augment_images and _IMG_TF is not None:\n",
    "                try:\n",
    "                    img = _IMG_TF(img)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            messages = sample[\"messages\"]\n",
    "            prompt_text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            if self.train and \"answer\" in sample and sample[\"answer\"]:\n",
    "                conversation = messages + [\n",
    "                    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": str(sample[\"answer\"]).strip().lower()}]}\n",
    "                ]\n",
    "                full_text = self.processor.apply_chat_template(\n",
    "                    conversation, tokenize=False, add_generation_prompt=False\n",
    "                )\n",
    "            else:\n",
    "                full_text = prompt_text\n",
    "\n",
    "            # Normalize text\n",
    "            prompt_text = unicodedata.normalize('NFKC', prompt_text)\n",
    "            full_text = unicodedata.normalize('NFKC', full_text)\n",
    "\n",
    "            images.append(img)\n",
    "            prompts.append(prompt_text)\n",
    "            full_texts.append(full_text)\n",
    "\n",
    "        # Encode prompt and full sequences\n",
    "        enc_prompt = self.processor(\n",
    "            images=images,\n",
    "            text=prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=cfg.MAX_SEQUENCE_LENGTH,\n",
    "        )\n",
    "        enc_full = self.processor(\n",
    "            images=images,\n",
    "            text=full_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=cfg.MAX_SEQUENCE_LENGTH,\n",
    "        )\n",
    "\n",
    "        if self.train:\n",
    "            labels = enc_full[\"input_ids\"].clone()\n",
    "            # mask pads\n",
    "            labels[enc_full[\"attention_mask\"] == 0] = -100\n",
    "            # mask prompt tokens\n",
    "            for i in range(labels.size(0)):\n",
    "                full_mask = enc_full[\"attention_mask\"][i].bool()\n",
    "                prompt_len = int(enc_prompt[\"attention_mask\"][i].sum().item())\n",
    "                nonpad_idx = torch.nonzero(full_mask, as_tuple=False).squeeze(-1)\n",
    "                prompt_idx = nonpad_idx[:prompt_len]\n",
    "                labels[i, prompt_idx] = -100\n",
    "            enc_full[\"labels\"] = labels\n",
    "\n",
    "        return enc_full\n",
    "\n",
    "\n",
    "print(\"✅ Dataset & DataCollator (with label masking) ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 7. Model & Processor 로드\n",
    "\n",
    "QLoRA 모델과 Processor를 로드합니다.\n",
    "\n",
    "### ✅ T4 호환 설정\n",
    "- Float16 (BFloat16 아님)\n",
    "- SDPA attention (FlashAttention 제거)\n",
    "- 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "﻿import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "def _fp8_supported():\n",
    "    try:\n",
    "        import transformer_engine.pytorch as te  # noqa: F401\n",
    "    except Exception:\n",
    "        return False\n",
    "    if not torch.cuda.is_available():\n",
    "        return False\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    if major < 9:  # Hopper(H100)=9.x, Ampere(A100)=8.0 -> FP8 誘몄???        return False\n",
    "    # PyTorch float8 ???議댁옱 ?щ?\n",
    "    has_float8 = hasattr(torch, \"float8_e4m3fn\") and hasattr(torch, \"float8_e5m2\")\n",
    "    return bool(has_float8)\n",
    "\n",
    "\n",
    "def create_model_and_processor(model_id, use_advanced=False):\n",
    "    \"\"\"Create 4-bit QLoRA model + processor with left padding and auto device_map.\n",
    "    For Qwen3-VL family we prefer AutoModelForImageTextToText (fallback to Vision2Seq as needed).\n",
    "    \"\"\"\n",
    "\n",
    "    compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_id,\n",
    "        min_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
    "        max_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if hasattr(processor, \"tokenizer\"):\n",
    "        processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    qopt = getattr(cfg, \"QUANTIZATION\", \"\").lower()\n",
    "    want_fp8 = (\"fp8\" in model_id.lower()) or (qopt == \"fp8\")\n",
    "    want_bnb4 = (qopt == \"bnb4\")\n",
    "    can_fp8 = _fp8_supported()\n",
    "    use_fp8 = bool(want_fp8 and can_fp8)\n",
    "\n",
    "    load_model_id = model_id\n",
    "    if want_fp8 and not can_fp8:\n",
    "        # FP8 誘몄????섍꼍(A100 ???먯꽌???쇰컲 媛以묒튂濡??대갚\n",
    "        if model_id.lower().endswith(\"-fp8\"):\n",
    "            load_model_id = model_id[:-4]  # \"-FP8\" ?쒓굅\n",
    "        print(f\"[warn] FP8 not supported on this GPU. Falling back to 4-bit for {load_model_id}\")\n",
    "\n",
    "    def _load_image_text_to_text():\n",
    "        return AutoModelForImageTextToText.from_pretrained(\n",
    "            load_model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "    def _load_vision2seq(**kwargs):\n",
    "        return AutoModelForVision2Seq.from_pretrained(\n",
    "            load_model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    if use_fp8:\n",
    "        try:\n",
    "            base_model = _load_image_text_to_text()\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq()\n",
    "    elif want_bnb4:\n",
    "        try:\n",
    "            base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                load_model_id,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq(quantization_config=bnb_config)\n",
    "    else:\n",
    "        # FP16/BF16 寃쎈줈: ?묒옄???놁씠 dtype 吏??        try:\n",
    "            base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                load_model_id,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq(torch_dtype=torch.float16)\n",
    "\n",
    "    if want_bnb4 and not use_fp8:\n",
    "        base_model = prepare_model_for_kbit_training(base_model)\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prefer SDPA attention for speed/stability on PyTorch 2.x\n",
    "    try:\n",
    "        if hasattr(base_model, \"config\"):\n",
    "            base_model.config.attn_implementation = \"sdpa\"\n",
    "        torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.LORA_R,\n",
    "        lora_alpha=cfg.LORA_ALPHA,\n",
    "        lora_dropout=cfg.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        target_modules=cfg.TARGET_MODULES,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # ensure eos/pad ids are set to avoid generation issues\n",
    "    try:\n",
    "        if hasattr(model, \"generation_config\") and hasattr(processor, \"tokenizer\"):\n",
    "            if model.generation_config.pad_token_id is None:\n",
    "                model.generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "            if model.generation_config.eos_token_id is None:\n",
    "                model.generation_config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "    except Exception:\n",
    "        pass\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "print(\"   Loading model & processor...\")\n",
    "model, processor = create_model_and_processor(\n",
    "    cfg.MODEL_ID,\n",
    "    use_advanced=cfg.USE_ADVANCED_MODEL\n",
    ")\n",
    "print(\"   Model ready.\")\n",
    "\n",
    "# Optional compile for speed (set USE_TORCH_COMPILE=1)\n",
    "import os as _os\n",
    "if _os.environ.get(\"USE_TORCH_COMPILE\", \"0\") == \"1\":\n",
    "    try:\n",
    "        model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)  # type: ignore[attr-defined]\n",
    "        print(\"   Model compiled with torch.compile\")\n",
    "    except Exception as _e:\n",
    "        print(f\"[warn] torch.compile skipped: {_e}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 理쒕? ?덉쟾 諛곗튂 ?ш린 ?먮룞 ?먯깋\n",
    "# -------------------------------\n",
    "import random\n",
    "\n",
    "\n",
    "def _build_tmp_batch(ds, collate_fn, bs: int):\n",
    "    idxs = [random.randrange(len(ds)) for _ in range(min(bs, len(ds)))]\n",
    "    samples = [ds[i] for i in idxs]\n",
    "    return collate_fn(samples)\n",
    "\n",
    "\n",
    "def auto_tune_per_device_batch(model, processor, df, max_probe_samples: int = 64):\n",
    "    if not torch.cuda.is_available():\n",
    "        return cfg.BATCH_SIZE\n",
    "\n",
    "    # ?섑뵆 ?곗씠?곗뀑/肄쒕젅?댄꽣 (?숈뒿 紐⑤뱶)\n",
    "    probe_n = min(max_probe_samples, len(df))\n",
    "    probe_df = df.sample(probe_n, random_state=cfg.SEED).reset_index(drop=True)\n",
    "    ds = VQADataset(probe_df, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "    collate_fn = DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "\n",
    "    # ?꾨낫援? ?꾩옱 ?ㅼ젙遺???먯쭊 利앷?\n",
    "    base = max(1, int(cfg.BATCH_SIZE))\n",
    "    candidates = sorted({1, 2, 3, 4, 6, 8, 12, 16, 24, 32, base})\n",
    "    candidates = [c for c in candidates if c <= 32]\n",
    "\n",
    "    best = 1\n",
    "    compute_dtype = torch.float16\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.USE_AMP)\n",
    "\n",
    "    for bs in candidates:\n",
    "        torch.cuda.empty_cache()\n",
    "        ok = True\n",
    "        try:\n",
    "            batch = _build_tmp_batch(ds, collate_fn, bs)\n",
    "            batch = {k: v.to('cuda') if hasattr(v, 'to') else v for k, v in batch.items()}\n",
    "            model.train()\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.USE_AMP, dtype=compute_dtype):\n",
    "                out = model(**batch)\n",
    "                loss = out.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            del batch, out, loss\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower():\n",
    "                ok = False\n",
    "            else:\n",
    "                raise\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            ok = False\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if ok:\n",
    "            best = bs\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # 紐⑺몴 ?좏슚 諛곗튂??留욎떠 ?꾩쟻 ?ㅽ뀦 ?ъ“??    import os\n",
    "    target_effective = int(os.environ.get(\"EFFECTIVE_BATCH_SIZE\", 16))\n",
    "    cfg.BATCH_SIZE = max(1, best)\n",
    "    cfg.GRAD_ACCUM_STEPS = max(1, target_effective // cfg.BATCH_SIZE)\n",
    "    print(\n",
    "        f\" Auto-tune: chosen per_device_batch={cfg.BATCH_SIZE} | grad_accum={cfg.GRAD_ACCUM_STEPS} | target_effective={target_effective}\"\n",
    "    )\n",
    "    return cfg.BATCH_SIZE\n",
    "\n",
    "\n",
    "try:\n",
    "    # train_df???욎꽑 ??먯꽌 濡쒕뱶??    if 'train_df' in globals() and len(train_df) > 0:\n",
    "        auto_tune_per_device_batch(model, processor, train_df)\n",
    "except Exception as _e:\n",
    "    print(f\"[warn] Auto-tune skipped: {_e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 8. Training Loop with Advanced Techniques\n",
    "\n",
    "고급 학습 기법을 적용한 학습 루프입니다.\n",
    "\n",
    "### ✨ 적용된 기법\n",
    "- ✅ **AMP** (Automatic Mixed Precision)\n",
    "- ✅ **EMA** (Exponential Moving Average)\n",
    "- ✅ **SWA** (Stochastic Weight Averaging)\n",
    "- ✅ **Cosine Warmup Scheduler**\n",
    "- ✅ **Gradient Clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AMP_DTYPE = None\n",
    "try:\n",
    "    import torch as _torch\n",
    "    AMP_DTYPE = _torch.float16\n",
    "except Exception:\n",
    "    AMP_DTYPE = None\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average\"\"\"\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self.register()\n",
    "    \n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    \n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (\n",
    "                    self.decay * self.shadow[name] +\n",
    "                    (1.0 - self.decay) * param.data\n",
    "                )\n",
    "                self.shadow[name] = new_average.clone()\n",
    "    \n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "    \n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "def train_one_fold(model, train_loader, valid_loader, fold=0):\n",
    "    \"\"\" Fold \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Fold {fold}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg.LEARNING_RATE,\n",
    "        weight_decay=cfg.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    num_training_steps = cfg.NUM_EPOCHS * math.ceil(len(train_loader) / cfg.GRAD_ACCUM_STEPS)\n",
    "    num_warmup_steps = int(num_training_steps * cfg.WARMUP_RATIO)\n",
    "    \n",
    "    if cfg.USE_COSINE_SCHEDULE:\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps, num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps, num_training_steps\n",
    "        )\n",
    "    \n",
    "    # AMP Scaler\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=cfg.USE_AMP)\n",
    "    \n",
    "    # EMA\n",
    "    ema = EMA(model, decay=cfg.EMA_DECAY) if cfg.USE_EMA else None\n",
    "    \n",
    "    # SWA\n",
    "    swa_model = None\n",
    "    if cfg.USE_SWA:\n",
    "        swa_model = AveragedModel(model)\n",
    "        swa_scheduler = SWALR(optimizer, swa_lr=cfg.LEARNING_RATE * 0.1)\n",
    "    \n",
    "    # \n",
    "    global_step = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(cfg.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{cfg.NUM_EPOCHS} [train]\",\n",
    "            unit=\"batch\"\n",
    "        )\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar, start=1):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward with AMP\n",
    "            with torch.amp.autocast('cuda', enabled=cfg.USE_AMP, dtype=AMP_DTYPE):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / cfg.GRAD_ACCUM_STEPS\n",
    "            \n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            if step % cfg.GRAD_ACCUM_STEPS == 0:\n",
    "                # Gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.MAX_GRAD_NORM)\n",
    "                \n",
    "                # Optimizer step\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Scheduler step\n",
    "                if cfg.USE_SWA and epoch >= cfg.SWA_START_EPOCH:\n",
    "                    swa_scheduler.step()\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "                \n",
    "                # EMA update\n",
    "                if cfg.USE_EMA and ema is not None:\n",
    "                    ema.update()\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # Progress\n",
    "                avg_loss = running_loss / cfg.GRAD_ACCUM_STEPS\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": f\"{avg_loss:.4f}\",\n",
    "                    \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "                })\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # SWA model update\n",
    "        if cfg.USE_SWA and swa_model is not None and epoch >= cfg.SWA_START_EPOCH:\n",
    "            swa_model.update_parameters(model)\n",
    "        \n",
    "        # Validation\n",
    "        if cfg.USE_EMA and ema is not None:\n",
    "            ema.apply_shadow()\n",
    "        \n",
    "        val_loss = validate(model, valid_loader)\n",
    "        \n",
    "        if cfg.USE_EMA and ema is not None:\n",
    "            ema.restore()\n",
    "        \n",
    "        print(f\"[Epoch {epoch+1}] Valid Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Best model \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            \n",
    "            if cfg.USE_EMA and ema is not None:\n",
    "                ema.apply_shadow()\n",
    "            \n",
    "            _to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "            _to_save.save_pretrained(save_path)\n",
    "            processor.save_pretrained(save_path)\n",
    "            \n",
    "            if cfg.USE_EMA and ema is not None:\n",
    "                ema.restore()\n",
    "            \n",
    "            print(f\"    Best model saved to {save_path}\")\n",
    "    \n",
    "    # SWA \n",
    "    if cfg.USE_SWA and swa_model is not None:\n",
    "        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n",
    "        save_path = f\"{cfg.SAVE_DIR}/fold{fold}_swa\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        (swa_model.module if hasattr(swa_model, \"module\") else swa_model).save_pretrained(save_path)\n",
    "        processor.save_pretrained(save_path)\n",
    "        print(f\"    SWA model saved to {save_path}\")\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "def validate(model, valid_loader):\n",
    "    \"\"\"Validation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=\"Validating\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.amp.autocast('cuda', enabled=cfg.USE_AMP, dtype=AMP_DTYPE):\n",
    "                outputs = model(**batch)\n",
    "                total_loss += outputs.loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / len(valid_loader)\n",
    "\n",
    "\n",
    "print(\" Training functions  \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 9. 실제 학습 실행\n",
    "\n",
    "K-Fold 또는 단일 모델 학습을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold \n",
    "if cfg.USE_KFOLD:\n",
    "    results = {}\n",
    "    \n",
    "    for fold in cfg.TRAIN_FOLDS:\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"Starting Fold {fold}/{cfg.N_FOLDS-1}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        # \n",
    "        train_subset = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
    "        valid_subset = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Train: {len(train_subset)}, Valid: {len(valid_subset)}\")\n",
    "        \n",
    "        # Dataset\n",
    "        train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "        valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "        \n",
    "        # DataLoader\n",
    "        dp_active = cfg.USE_DATAPARALLEL and torch.cuda.is_available() and torch.cuda.device_count() > 1\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL, augment_images=True),\n",
    "            num_workers=cfg.NUM_WORKERS,\n",
    "            drop_last=dp_active,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=bool(cfg.NUM_WORKERS)\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_ds,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL, augment_images=False),\n",
    "            num_workers=cfg.NUM_WORKERS,\n",
    "            drop_last=False,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=bool(cfg.NUM_WORKERS)\n",
    "        )\n",
    "        \n",
    "        # \n",
    "        best_loss = train_one_fold(model, train_loader, valid_loader, fold=fold)\n",
    "        results[fold] = best_loss\n",
    "        \n",
    "        print(f\"\\n Fold {fold} : Best Val Loss = {best_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"All Folds Training Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for fold, loss in results.items():\n",
    "        print(f\"Fold {fold}: {loss:.4f}\")\n",
    "    print(f\"Average: {np.mean(list(results.values())):.4f}\")\n",
    "\n",
    "else:\n",
    "    #   \n",
    "    train_subset = train_df[train_df['fold'] == -1].reset_index(drop=True)\n",
    "    valid_subset = train_df[train_df['fold'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "    valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "    \n",
    "    dp_active = cfg.USE_DATAPARALLEL and torch.cuda.is_available() and torch.cuda.device_count() > 1\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL, augment_images=True),\n",
    "        num_workers=cfg.NUM_WORKERS,\n",
    "        drop_last=dp_active,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=bool(cfg.NUM_WORKERS)\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_ds,\n",
    "        batch_size=cfg.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL, augment_images=False),\n",
    "        num_workers=cfg.NUM_WORKERS,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=bool(cfg.NUM_WORKERS)\n",
    "    )\n",
    "    \n",
    "    best_loss = train_one_fold(model, train_loader, valid_loader, fold=0)\n",
    "    print(f\"\\n Single model  : Best Val Loss = {best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔮 10. Inference with TTA\n",
    "\n",
    "Test-Time Augmentation을 활용한 추론을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "﻿import os\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "from transformers import LogitsProcessorList\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "import re\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "def extract_choice(text: str) -> str:\n",
    "    text = text.strip().lower()\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "    if lines:\n",
    "        last = lines[-1]\n",
    "        if last in [\"a\", \"b\", \"c\", \"d\"]:\n",
    "            return last\n",
    "    for tok in text.split():\n",
    "        if tok in [\"a\", \"b\", \"c\", \"d\"]:\n",
    "            return tok\n",
    "    return \"a\"\n",
    "\n",
    "\n",
    "def infer_single_fold(model_path, test_df, output_path):\n",
    "    \"\"\"Single-fold inference loading saved LoRA adapter on top of base model.\"\"\"\n",
    "\n",
    "    compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    # Load base and then attach adapters\n",
    "    qopt = getattr(cfg, \"QUANTIZATION\", \"\").lower()\n",
    "    want_fp8 = (\"fp8\" in cfg.MODEL_ID.lower()) or (qopt == \"fp8\")\n",
    "    want_bnb4 = (qopt == \"bnb4\")\n",
    "    can_fp8 = False\n",
    "    try:\n",
    "        import transformer_engine.pytorch as te  # noqa: F401\n",
    "        if torch.cuda.is_available():\n",
    "            major, minor = torch.cuda.get_device_capability(0)\n",
    "            can_fp8 = major >= 9 and hasattr(torch, \"float8_e4m3fn\") and hasattr(torch, \"float8_e5m2\")\n",
    "    except Exception:\n",
    "        can_fp8 = False\n",
    "\n",
    "    use_fp8 = bool(want_fp8 and can_fp8)\n",
    "    load_model_id = cfg.MODEL_ID\n",
    "    if want_fp8 and not can_fp8:\n",
    "        if load_model_id.lower().endswith(\"-fp8\"):\n",
    "            load_model_id = load_model_id[:-4]\n",
    "        print(f\"[warn] FP8 not supported on this GPU. Falling back to 4-bit for {load_model_id}\")\n",
    "\n",
    "    def _load_image_text_to_text():\n",
    "        return AutoModelForImageTextToText.from_pretrained(\n",
    "            load_model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    def _load_vision2seq(**kwargs):\n",
    "        return AutoModelForVision2Seq.from_pretrained(\n",
    "            load_model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    if use_fp8:\n",
    "        try:\n",
    "            base_model = _load_image_text_to_text()\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq()\n",
    "    elif want_bnb4:\n",
    "        try:\n",
    "            base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                load_model_id,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq(quantization_config=bnb_config)\n",
    "    else:\n",
    "        try:\n",
    "            base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                load_model_id,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq(torch_dtype=torch.float16)\n",
    "    model_infer = PeftModel.from_pretrained(base_model, model_path)\n",
    "    processor_infer = AutoProcessor.from_pretrained(\n",
    "        model_path,\n",
    "        min_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
    "        max_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if hasattr(processor_infer, \"tokenizer\"):\n",
    "        processor_infer.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    try:\n",
    "        if hasattr(base_model, \"config\"):\n",
    "            base_model.config.attn_implementation = \"sdpa\"\n",
    "        torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    model_infer.eval()\n",
    "\n",
    "    # Restrict to a/b/c/d tokens if possible\n",
    "    try:\n",
    "        allowed_tokens = [\n",
    "            processor_infer.tokenizer.convert_tokens_to_ids(t)\n",
    "            for t in [\"a\", \"b\", \"c\", \"d\"]\n",
    "        ]\n",
    "        allowed_tokens = None if any(x is None for x in allowed_tokens) else allowed_tokens\n",
    "    except Exception:\n",
    "        allowed_tokens = None\n",
    "\n",
    "    predictions = []\n",
    "    for i in tqdm(range(len(test_df)), desc=\"Inference\"):\n",
    "        row = test_df.iloc[i]\n",
    "        img_path = os.path.join(cfg.DATA_DIR, row[\"path\"]) if \"path\" in row else row.get(\"image_path\", \"\")\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n",
    "\n",
    "        user_text = build_mc_prompt(\n",
    "            str(row[\"question\"]),\n",
    "            str(row[\"a\"]), str(row[\"b\"]), str(row[\"c\"]), str(row[\"d\"])\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": user_text}\n",
    "            ]}\n",
    "        ]\n",
    "\n",
    "        text = processor_infer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        def _predict_one(pil_img):\n",
    "            _inputs = processor_infer(\n",
    "                text=[text], images=[pil_img], return_tensors=\"pt\"\n",
    "            )\n",
    "            _inputs = {k: v.to(model_infer.device) if hasattr(v, 'to') else v for k, v in _inputs.items()}\n",
    "            lp = None\n",
    "            if allowed_tokens is not None:\n",
    "                class _Allowed:\n",
    "                    def __init__(self, allowed):\n",
    "                        self.allowed = set(int(x) for x in allowed)\n",
    "                    def __call__(self, input_ids, scores):\n",
    "                        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "                        idx = torch.tensor(list(self.allowed), device=scores.device)\n",
    "                        mask.index_fill_(1, idx, 0.0)\n",
    "                        return scores + mask\n",
    "                lp = LogitsProcessorList([_Allowed(allowed_tokens)])\n",
    "            with torch.no_grad():\n",
    "                out_ids = model_infer.generate(\n",
    "                    **_inputs,\n",
    "                    max_new_tokens=(1 if allowed_tokens is not None else cfg.MAX_NEW_TOKENS),\n",
    "                    do_sample=cfg.DO_SAMPLE,\n",
    "                    temperature=cfg.TEMPERATURE if cfg.DO_SAMPLE else None,\n",
    "                    eos_token_id=processor_infer.tokenizer.eos_token_id,\n",
    "                    logits_processor=lp,\n",
    "                )\n",
    "            _txt = processor_infer.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "            return extract_choice(_txt)\n",
    "\n",
    "        if cfg.USE_TTA:\n",
    "            # Disable hflip when question/options imply directionality\n",
    "            orient_words = re.compile(r\"(left|right|\\b왼쪽\\b|\\b오른쪽\\b|\\b좌\\b|\\b우\\b|방향|대칭|mirror|mirrored)\", re.I)\n",
    "            qtxt = str(row.get(\"question\", \"\"))\n",
    "            otxt = \" \".join([str(row.get(k, \"\")) for k in [\"a\", \"b\", \"c\", \"d\"]])\n",
    "            is_dir_sensitive = bool(orient_words.search(qtxt) or orient_words.search(otxt))\n",
    "\n",
    "            votes = []\n",
    "            W, H = img.size\n",
    "            for s in cfg.TTA_SCALES:\n",
    "                new_img = img.resize((max(8, int(W*s)), max(8, int(H*s))))\n",
    "                votes.append(_predict_one(new_img))\n",
    "                if cfg.TTA_HFLIP and not is_dir_sensitive:\n",
    "                    votes.append(_predict_one(TF.hflip(new_img)))\n",
    "            answer = Counter(votes).most_common(1)[0][0] if votes else _predict_one(img)\n",
    "        else:\n",
    "            answer = _predict_one(img)\n",
    "        predictions.append(answer)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"answer\": predictions,\n",
    "    })\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\" Saved to {output_path}\")\n",
    "    return submission\n",
    "\n",
    "\n",
    "# Run inference across folds or single model\n",
    "predictions_all = []\n",
    "if cfg.USE_KFOLD:\n",
    "    for fold in cfg.TRAIN_FOLDS:\n",
    "        model_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n",
    "        output_path = f\"{cfg.OUTPUT_DIR}/submission_fold{fold}.csv\"\n",
    "        print(f\"\\n{'='*60}\\nInferencing Fold {fold}\\n{'='*60}\")\n",
    "        pred = infer_single_fold(model_path, test_df, output_path)\n",
    "        predictions_all.append(pred)\n",
    "else:\n",
    "    model_path = f\"{cfg.SAVE_DIR}/fold0_best\"\n",
    "    output_path = f\"{cfg.OUTPUT_DIR}/submission_single.csv\"\n",
    "    pred = infer_single_fold(model_path, test_df, output_path)\n",
    "    predictions_all.append(pred)\n",
    "\n",
    "print(\"\\n All inference complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 11. Ensemble\n",
    "\n",
    "여러 Fold의 예측을 앙상블합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if cfg.USE_KFOLD and len(predictions_all) > 1:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Ensemble (Weighted Voting)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Weighted Voting (weights=None이면 균등)\n",
    "    ensemble_preds = []\n",
    "    \n",
    "    if cfg.ENSEMBLE_WEIGHTS is None or len(cfg.ENSEMBLE_WEIGHTS) != len(predictions_all):\n",
    "        weights = [1.0/len(predictions_all)] * len(predictions_all)\n",
    "    else:\n",
    "        s = sum(cfg.ENSEMBLE_WEIGHTS); weights = [w/s for w in cfg.ENSEMBLE_WEIGHTS]\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "        score = {\"a\":0.0, \"b\":0.0, \"c\":0.0, \"d\":0.0}\n",
    "        for k, pred in enumerate(predictions_all):\n",
    "            ans = str(pred.iloc[i]['answer']).strip().lower()\n",
    "            if ans in score: score[ans] += weights[k]\n",
    "        chosen = max(score.items(), key=lambda x: x[1])[0]\n",
    "        ensemble_preds.append(chosen)\n",
    "    \n",
    "    # 최종 제출 파일\n",
    "    final_submission = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"answer\": ensemble_preds\n",
    "    })\n",
    "    \n",
    "    final_path = f\"{cfg.OUTPUT_DIR}/submission_ensemble.csv\"\n",
    "    final_submission.to_csv(final_path, index=False)\n",
    "    \n",
    "    print(f\"✅ Ensemble submission saved to {final_path}\")\n",
    "    print(f\"\\nAnswer Distribution:\")\n",
    "    print(final_submission['answer'].value_counts().sort_index())\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✅ Single model - No ensemble needed\")\n",
    "    final_submission = predictions_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 12. 결과 분석 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 답변 분포 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "answer_counts = final_submission['answer'].value_counts().sort_index()\n",
    "sns.barplot(x=answer_counts.index, y=answer_counts.values, palette='viridis', ax=ax)\n",
    "ax.set_title('Final Submission Answer Distribution', fontsize=14, weight='bold')\n",
    "ax.set_xlabel('Answer')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 비율 표시\n",
    "for i, (ans, count) in enumerate(answer_counts.items()):\n",
    "    percentage = count / len(final_submission) * 100\n",
    "    ax.text(i, count + 10, f\"{percentage:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 통계 출력\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Final Statistics\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total predictions: {len(final_submission)}\")\n",
    "print(f\"\\nAnswer counts:\")\n",
    "for ans, count in answer_counts.items():\n",
    "    print(f\"  {ans}: {count:5d} ({count/len(final_submission)*100:5.1f}%)\")\n",
    "\n",
    "# 제출 파일 샘플\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Sample Predictions\")\n",
    "print(f\"{'='*60}\")\n",
    "print(final_submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ 13. 최종 정리\n",
    "\n",
    "### 🎉 완료된 작업\n",
    "\n",
    "1. ✅ **환경 설정** - 패키지 설치 및 임포트\n",
    "2. ✅ **Config** - 하이퍼파라미터 통합 관리\n",
    "3. ✅ **데이터 로드 & EDA** - 탐색적 분석\n",
    "4. ✅ **Stratified K-Fold** - CV Splits 생성\n",
    "5. ✅ **Dataset & DataLoader** - 라벨 정렬 교정 적용\n",
    "6. ✅ **Model & Processor** - QLoRA 모델 로드 (T4 호환)\n",
    "7. ✅ **Training Loop** - AMP, EMA, SWA, Cosine Warmup 적용\n",
    "8. ✅ **Inference** - TTA 지원 추론\n",
    "9. ✅ **Ensemble** - Majority Voting\n",
    "10. ✅ **Results** - 시각화 및 통계\n",
    "\n",
    "### 🚀 다음 단계\n",
    "\n",
    "1. **하이퍼파라미터 튜닝**\n",
    "   - Learning rate, LoRA rank 조정\n",
    "   - Batch size, Grad accumulation 최적화\n",
    "\n",
    "2. **모델 크기 확대**\n",
    "   - 7B 모델 사용 (더 높은 정확도)\n",
    "   - Image size 증가 (512, 768)\n",
    "\n",
    "3. **고급 기법 활성화**\n",
    "   - TTA scales 추가\n",
    "   - SWA 적용\n",
    "   - 데이터 증강 활성화\n",
    "\n",
    "4. **에폭 증가**\n",
    "   - NUM_EPOCHS = 3~5\n",
    "\n",
    "### 📌 Important Notes\n",
    "\n",
    "- **T4 호환**: Float16, SDPA attention 사용\n",
    "- **라벨 정렬**: Assistant 메시지에 정답 포함 (핵심!)\n",
    "- **재현성**: Seed 42 고정\n",
    "- **메모리**: Gradient checkpointing, 4-bit QLoRA\n",
    "\n",
    "---\n",
    "\n",
    "**🤖 Generated for SSAFY AI Project 2025**\n",
    "\n",
    "**📧 Contact**: GitHub Issues\n",
    "\n",
    "**⭐ 행운을 빕니다!**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8556585,
     "sourceId": 13477844,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}