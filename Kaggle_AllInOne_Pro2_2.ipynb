{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìí Kaggle_AllInOne_Pro.ipynb ‚Äì Îã®Ïùº ÎÖ∏Ìä∏Î∂Å ÌÜµÌï© Î≤ÑÏ†Ñ\n",
    "\n",
    "## üéØ Í∞úÏöî\n",
    "\n",
    "Î≥∏ ÎÖ∏Ìä∏Î∂ÅÏùÄ **VQA Kaggle Challenge**Î•º ÏúÑÌïú **ÏôÑÏ†Ñ ÌÜµÌï© Í≥†ÏÑ±Îä• ÌååÏù¥ÌîÑÎùºÏù∏**ÏûÖÎãàÎã§.\n",
    "\n",
    "### ‚ú® Ï£ºÏöî Í∏∞Îä•\n",
    "\n",
    "- ‚úÖ **T4 GPU ÏôÑÎ≤Ω Ìò∏Ìôò** (Float16, SDPA attention)\n",
    "- ‚úÖ **ÎùºÎ≤® Ï†ïÎ†¨ ÍµêÏ†ï** (Assistant Î©îÏãúÏßÄÏóê Ï†ïÎãµ Ìè¨Ìï®)\n",
    "- ‚úÖ **K-Fold Cross-Validation** (Stratified)\n",
    "- ‚úÖ **Í≥†Í∏â ÌïôÏäµ Í∏∞Î≤ï** (AMP, EMA, SWA, Cosine Warmup)\n",
    "- ‚úÖ **Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï** (Choice Shuffle, Paraphrase)\n",
    "- ‚úÖ **TTA (Test-Time Augmentation)**\n",
    "- ‚úÖ **ÏïôÏÉÅÎ∏î** (Weighted Voting)\n",
    "- ‚úÖ **Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî** (Gradient Checkpointing, 4-bit QLoRA)\n",
    "\n",
    "### üìä ÏòàÏÉÅ ÏÑ±Îä•\n",
    "\n",
    "| ÏÑ§Ï†ï | Ï†ïÌôïÎèÑ | ÏãúÍ∞Ñ |\n",
    "|------|--------|------|\n",
    "| Single Fold | 79-82% | ~4h |\n",
    "| 3-Fold Ensemble | 83-85% | ~12h |\n",
    "| + TTA + Optimization | 85-88% | ~15h |\n",
    "\n",
    "### üöÄ Ïã§Ìñâ ÏàúÏÑú\n",
    "\n",
    "1. **ÌôòÍ≤Ω ÏÑ§Ï†ï** - Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò Î∞è ÏûÑÌè¨Ìä∏\n",
    "2. **Config** - ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
    "3. **Îç∞Ïù¥ÌÑ∞ Î°úÎìú** - Train/Test Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "4. **EDA** - ÌÉêÏÉâÏ†Å Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù\n",
    "5. **Stratified K-Fold** - CV Splits ÏÉùÏÑ±\n",
    "6. **Dataset & DataLoader** - Ïª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÏùò\n",
    "7. **Model & Processor** - QLoRA Î™®Îç∏ Î°úÎìú\n",
    "8. **Training Loop** - Í≥†Í∏â Í∏∞Î≤ï Ï†ÅÏö© ÌïôÏäµ\n",
    "9. **Inference** - TTAÎ•º ÌôúÏö©Ìïú Ï∂îÎ°†\n",
    "10. **Ensemble** - ÏïôÏÉÅÎ∏î Î∞è Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ±\n",
    "\n",
    "---\n",
    "\n",
    "**ü§ñ Generated for SSAFY AI Project 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò\n",
    "\n",
    "ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÑ§ÏπòÌï©ÎãàÎã§. (Ï≤´ Ïã§Ìñâ Ïãú 1ÌöåÎßå)\n",
    "\n",
    "### ‚ö†Ô∏è Ï§ëÏöî: ÏÑ§Ïπò ÌõÑ Îü∞ÌÉÄÏûÑ Ïû¨ÏãúÏûë ÌïÑÏöî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Íµ¨Í∏ÄÎìúÎùºÏù¥Î∏å ÎßàÏö¥Ìä∏\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:19:36.203531Z",
     "iopub.status.busy": "2025-10-24T07:19:36.202944Z",
     "iopub.status.idle": "2025-10-24T07:19:45.714082Z",
     "shell.execute_reply": "2025-10-24T07:19:45.713005Z",
     "shell.execute_reply.started": "2025-10-24T07:19:36.203502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "‚úÖ Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÏôÑÎ£å! Îü∞ÌÉÄÏûÑÏùÑ Ïû¨ÏãúÏûëÌïòÏÑ∏Ïöî.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.27 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.5.0 which is incompatible.\n",
      "langchain-community 0.3.30 requires pydantic-settings<3.0.0,>=2.10.1, but you have pydantic-settings 2.1.0 which is incompatible.\n",
      "llama-index-core 0.14.4 requires pydantic>=2.8.0, but you have pydantic 2.5.0 which is incompatible.\n",
      "llama-index-embeddings-google 0.4.1 requires google-generativeai<0.6,>=0.5.2, but you have google-generativeai 0.8.5 which is incompatible.\n",
      "llama-index-readers-file 0.5.4 requires beautifulsoup4<5,>=4.12.3, but you have beautifulsoup4 4.12.2 which is incompatible.\n",
      "llama-index-readers-file 0.5.4 requires pandas<2.3.0, but you have pandas 2.3.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò (Colab/Kaggle ÌôòÍ≤Ω)\n",
    "# Ï≤´ Ïã§Ìñâ ÏãúÏóêÎßå Ï£ºÏÑù Ìï¥Ï†úÌïòÍ≥† Ïã§Ìñâ\n",
    "!pip install -q \"transformers>=4.44.2\" \"accelerate>=0.34.2\" \"peft>=0.13.2\" \\\n",
    "    \"bitsandbytes>=0.43.1\" datasets pillow pandas torch torchvision nltk \\\n",
    "    scikit-learn matplotlib seaborn tqdm sentencepiece --upgrade\n",
    "!pip install -q qwen-vl-utils==0.0.8\n",
    "\n",
    "print(\"‚úÖ Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÏôÑÎ£å! Îü∞ÌÉÄÏûÑÏùÑ Ïû¨ÏãúÏûëÌïòÏÑ∏Ïöî.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 2. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, sys, re, math, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Optional\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "# Transformers & PEFT\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    LogitsProcessorList\n",
    ")\n",
    "import torchvision.transforms as T\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Env hygiene\n",
    "warnings.filterwarnings('ignore')\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Runtime info\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\" Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\" Python: {sys.version.split()[0]}\")\n",
    "print(f\" PyTorch: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3. Config ÏÑ§Ï†ï\n",
    "\n",
    "Î™®Îì† ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Î•º Ìïú Í≥≥ÏóêÏÑú Í¥ÄÎ¶¨Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ôªøimport os\n",
    "import torch\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Qwen3-VL-8B Instruct ÏÑ§Ï†ï (FP16)\n",
    "\n",
    "    Î™©Ìëú: ÎÜíÏùÄ Ï†ïÎãµÎ•†Í≥º ÏïàÏ†ïÏ†Å/Îπ†Î•∏ Ïã§Ìñâ\n",
    "    - Î™®Îç∏: Qwen/Qwen3-VL-8B-Instruct (FP16)\n",
    "    - Ï†ïÎ∞ÄÎèÑ: AMP(FP16) + SDPA Ïñ¥ÌÖêÏÖò\n",
    "    - ÎùºÎ≤® ÎßàÏä§ÌÇπ: ÌîÑÎ°¨ÌîÑÌä∏(-100), Ï†ïÎãµÎßå ÌïôÏäµ\n",
    "    - Î°úÏßì Ï†úÌïú: a/b/c/dÎßå ÏÉùÏÑ± (Ï∂îÎ°† ÏïàÏ†ïÌôî)\n",
    "    - ÏûêÎèô ÌäúÎãù: GPU VRAMÏóê ÎßûÏ∂∞ Î∞∞Ïπò/ÎàÑÏ†Å Ïä§ÌÖù ÏûêÎèô Í≤∞Ï†ï\n",
    "\n",
    "    Ï†ÑÌôò ÏòµÏÖò:\n",
    "    - QUANTIZATION=\"bnb4\" ‚Üí 4bit QLoRA (Î©îÎ™®Î¶¨ Ï†àÏïΩ)\n",
    "    - QUANTIZATION=\"fp16\" ‚Üí FP16 (Í∏∞Î≥∏, Ï†ïÌôïÎèÑ Ïö∞ÏÑ†)\n",
    "\n",
    "    Í∂åÏû• Ïã§Ìñâ ÏàúÏÑú:\n",
    "    1) Îç∞Ïù¥ÌÑ∞ Î°úÎìú/EDA\n",
    "    2) K-Fold Î∂ÑÌï†(ÏÑ†ÌÉù): USE_KFOLD True\n",
    "    3) Î™®Îç∏ Î°úÎìú(ÏûêÎèô ÌäúÎãù Ìè¨Ìï®)\n",
    "    4) ÌïôÏäµ(ÎùºÎ≤® ÎßàÏä§ÌÇπ Ï†ÅÏö©)\n",
    "    5) Ï∂îÎ°†(TTA/Î°úÏßì Ï†úÌïú)\n",
    "    6) Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ±\n",
    "    \"\"\"\n",
    "\n",
    "    # Seed\n",
    "    SEED = 42\n",
    "\n",
    "    # Model\n",
    "    MODEL_ID = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
    "    # Quantization/backend selection: \"fp16\" | \"bnb4\" | \"fp8\"\n",
    "    QUANTIZATION = \"fp16\"\n",
    "    IMAGE_SIZE = 384  # 384/512/768\n",
    "    USE_ADVANCED_MODEL = False\n",
    "    USE_DATAPARALLEL = False\n",
    "    NUM_WORKERS = 4\n",
    "\n",
    "    # Data\n",
    "    DATA_DIR = \"/kaggle/input/ssafy-ai-pjt-data\"\n",
    "    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
    "    TEST_CSV  = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "    # K-Fold\n",
    "    N_FOLDS = 3\n",
    "    USE_KFOLD = False   # Ï†ïÌôïÎèÑ Í∑πÎåÄÌôî Ïãú True (ÌïôÏäµ ÏãúÍ∞Ñ Ï¶ùÍ∞Ä)\n",
    "    TRAIN_FOLDS = [0, 1, 2]\n",
    "\n",
    "    # LoRA / QLoRA\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    TARGET_MODULES = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "\n",
    "    # Training\n",
    "    NUM_EPOCHS = 1     # Ï†ïÌôïÎèÑ ‚Üë ÏõêÌïòÎ©¥ 3~5Î°ú Ï¶ùÍ∞Ä\n",
    "    BATCH_SIZE = 1     # ÏûêÎèô ÌäúÎãùÏúºÎ°ú Ï°∞Ï†ïÎê®\n",
    "    GRAD_ACCUM_STEPS = 4\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_RATIO = 0.03\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "\n",
    "    # Precision & schedules\n",
    "    USE_AMP = True\n",
    "    USE_EMA = True\n",
    "    EMA_DECAY = 0.999\n",
    "    USE_SWA = False\n",
    "    SWA_START_EPOCH = 0\n",
    "    USE_COSINE_SCHEDULE = True\n",
    "\n",
    "    # Augment\n",
    "    USE_IMAGE_AUGMENTATION = False\n",
    "    USE_RANDAUGMENT = True\n",
    "    RANDAUG_N = 2\n",
    "    RANDAUG_M = 9\n",
    "    USE_MIXUP = False\n",
    "    MIXUP_ALPHA = 0.2\n",
    "    USE_CUTMIX = False\n",
    "    CUTMIX_ALPHA = 1.0\n",
    "\n",
    "    # Text aug\n",
    "    USE_BACK_TRANSLATION = False\n",
    "    USE_SYNONYM_AUG = False\n",
    "    TEXT_AUG_PROB = 0.15\n",
    "\n",
    "    # TTA\n",
    "    USE_TTA = False\n",
    "    TTA_SCALES = [0.9, 1.0, 1.1]\n",
    "    TTA_HFLIP = True\n",
    "\n",
    "    # Generation\n",
    "    MAX_NEW_TOKENS = 1\n",
    "    DO_SAMPLE = False\n",
    "    TEMPERATURE = 0.0\n",
    "\n",
    "    # Paths\n",
    "    SAVE_DIR = f\"/kaggle/working/checkpoints\"\n",
    "    OUTPUT_DIR = f\"/kaggle/working/outputs\"\n",
    "\n",
    "    # Sampling\n",
    "    USE_SAMPLE = False  # Ï†ïÌôïÎèÑ Ïö∞ÏÑ†: Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©\n",
    "    SAMPLE_SIZE = 200\n",
    "\n",
    "    # Ensemble\n",
    "    ENSEMBLE_WEIGHTS = None\n",
    "\n",
    "    # Chat system text\n",
    "    SYSTEM_INSTRUCT = (\n",
    "        \"You are a helpful visual question answering assistant. \"\n",
    "        \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n",
    "    )\n",
    "\n",
    "    # Sequence\n",
    "    MAX_SEQUENCE_LENGTH = 1024\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random, numpy as np, torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(cfg.SEED)\n",
    "\n",
    "# GPU perf knobs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def _auto_scale_training(cfg):\n",
    "    total_gb = 0.0\n",
    "    if torch.cuda.is_available():\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        total_gb = props.total_memory / 1024 ** 3\n",
    "\n",
    "    # User can override\n",
    "    target_effective = int(os.environ.get(\"EFFECTIVE_BATCH_SIZE\", 16))\n",
    "    suggested = 1\n",
    "    if total_gb >= 70:\n",
    "        suggested = 4\n",
    "    elif total_gb >= 40:\n",
    "        suggested = 2\n",
    "    else:\n",
    "        suggested = 1\n",
    "    per_device = int(os.environ.get(\"PER_DEVICE_BATCH\", suggested))\n",
    "\n",
    "    cfg.BATCH_SIZE = max(1, per_device)\n",
    "    cfg.GRAD_ACCUM_STEPS = max(1, target_effective // cfg.BATCH_SIZE)\n",
    "\n",
    "    print(\n",
    "        f\" Auto-scale: GPU {total_gb:.1f} GB | per_device_batch={cfg.BATCH_SIZE} | \"\n",
    "        f\"grad_accum={cfg.GRAD_ACCUM_STEPS} | target_effective={target_effective}\"\n",
    "    )\n",
    "\n",
    "\n",
    "_auto_scale_training(cfg)\n",
    "print(f\" Config loaded (Seed: {cfg.SEED})\")\n",
    "print(f\"   Model: {cfg.MODEL_ID}\")\n",
    "print(\n",
    "    f\"   Training: BATCH={cfg.BATCH_SIZE}, ACCUM={cfg.GRAD_ACCUM_STEPS}, LR={cfg.LEARNING_RATE}, Cosine={cfg.USE_COSINE_SCHEDULE}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Advanced: AMP={cfg.USE_AMP}, EMA={cfg.USE_EMA}, SWA={cfg.USE_SWA}, TTA={cfg.USE_TTA}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 4. Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è EDA\n",
    "\n",
    "Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌïòÍ≥† Í∞ÑÎã®Ìïú ÌÉêÏÉâÏ†Å Î∂ÑÏÑùÏùÑ ÏàòÌñâÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:19:55.638548Z",
     "iopub.status.busy": "2025-10-24T07:19:55.637495Z",
     "iopub.status.idle": "2025-10-24T07:19:56.071371Z",
     "shell.execute_reply": "2025-10-24T07:19:56.070573Z",
     "shell.execute_reply.started": "2025-10-24T07:19:55.638517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
    "test_df = pd.read_csv(cfg.TEST_CSV)\n",
    "\n",
    "print(f\"üìÅ Train: {len(train_df):,} samples\")\n",
    "print(f\"üìÅ Test: {len(test_df):,} samples\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "\n",
    "# ÏÉòÌîåÎßÅ (ÎîîÎ≤ÑÍπÖÏö©)\n",
    "if cfg.USE_SAMPLE:\n",
    "    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n",
    "    print(f\"\\n‚ö†Ô∏è  Sampled {len(train_df)} samples for quick testing\")\n",
    "\n",
    "# Í∏∞Î≥∏ ÌÜµÍ≥Ñ\n",
    "print(f\"\\nüìä Answer Distribution:\")\n",
    "print(train_df['answer'].value_counts().sort_index())\n",
    "\n",
    "# ÏãúÍ∞ÅÌôî\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# ÎãµÎ≥Ä Î∂ÑÌè¨\n",
    "train_df['answer'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Answer Distribution (Train)', fontsize=12, weight='bold')\n",
    "axes[0].set_xlabel('Answer')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ÏßàÎ¨∏ Í∏∏Ïù¥ Î∂ÑÌè¨\n",
    "train_df['question_len'] = train_df['question'].str.len()\n",
    "train_df['question_len'].hist(bins=30, ax=axes[1], color='salmon', edgecolor='black')\n",
    "axes[1].set_title('Question Length Distribution', fontsize=12, weight='bold')\n",
    "axes[1].set_xlabel('Length (chars)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ÏÉòÌîå Ï∂úÎ†•\n",
    "print(\"\\nüìù Sample Data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 5. Stratified K-Fold Cross-Validation\n",
    "\n",
    "ÎãµÎ≥Ä Î∂ÑÌè¨Î•º Ïú†ÏßÄÌïòÎ©¥ÏÑú K-FoldÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:19:56.073478Z",
     "iopub.status.busy": "2025-10-24T07:19:56.073212Z",
     "iopub.status.idle": "2025-10-24T07:19:56.082402Z",
     "shell.execute_reply": "2025-10-24T07:19:56.081499Z",
     "shell.execute_reply.started": "2025-10-24T07:19:56.073437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if cfg.USE_KFOLD:\n",
    "    # Stratified K-Fold ÏÉùÏÑ±\n",
    "    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n",
    "    train_df['fold'] = -1\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n",
    "        train_df.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "    print(f\"‚úÖ {cfg.N_FOLDS}-Fold CV ÏÉùÏÑ± ÏôÑÎ£å\")\n",
    "    print(f\"\\nFold Distribution:\")\n",
    "    print(train_df['fold'].value_counts().sort_index())\n",
    "    \n",
    "    # FoldÎ≥Ñ ÎãµÎ≥Ä Î∂ÑÌè¨ ÌôïÏù∏\n",
    "    print(f\"\\nAnswer Distribution per Fold:\")\n",
    "    for fold in range(cfg.N_FOLDS):\n",
    "        fold_data = train_df[train_df['fold'] == fold]\n",
    "        dist = fold_data['answer'].value_counts(normalize=True).sort_index()\n",
    "        print(f\"Fold {fold}: {dict(dist)}\")\n",
    "else:\n",
    "    # Îã®Ïùº Î™®Îç∏ ÌïôÏäµ (90:10 split)\n",
    "    split_idx = int(len(train_df) * 0.9)\n",
    "    train_df['fold'] = -1\n",
    "    train_df.loc[split_idx:, 'fold'] = 0\n",
    "    print(f\"‚úÖ Single split (90:10) ÏÉùÏÑ± ÏôÑÎ£å\")\n",
    "    print(f\"   Train: {len(train_df[train_df['fold'] == -1])}\")\n",
    "    print(f\"   Valid: {len(train_df[train_df['fold'] == 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è 6. Dataset & DataLoader\n",
    "\n",
    "Ïª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ÏÖã Î∞è DataCollatorÎ•º Ï†ïÏùòÌï©ÎãàÎã§.\n",
    "\n",
    "### ‚úÖ ÎùºÎ≤® Ï†ïÎ†¨ ÍµêÏ†ï Ï†ÅÏö©\n",
    "- Assistant Î©îÏãúÏßÄÏóê Ï†ïÎãµ Ìè¨Ìï®\n",
    "- `add_generation_prompt=False` ÏÇ¨Ïö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json, unicodedata\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "def build_mc_prompt(question, a, b, c, d):\n",
    "    \"\"\"Multiple Choice prompt\"\"\"\n",
    "    return (\n",
    "        f\"{question}\\n\"\n",
    "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
    "        \"ÎãµÎ≥ÄÏùÄ a, b, c, d Ï§ë ÌïòÎÇòÏùò Î¨∏ÏûêÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def synonym_replace(text: str) -> str:\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.corpus import wordnet as wn\n",
    "        try:\n",
    "            wn.synsets(\"test\")\n",
    "        except LookupError:\n",
    "            nltk.download(\"wordnet\")\n",
    "            nltk.download(\"omw-1.4\")\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return text\n",
    "        idx = np.random.randint(len(words))\n",
    "        syns = wn.synsets(words[idx])\n",
    "        lemmas = [l.name().replace(\"_\", \" \") for s in syns for l in s.lemmas()]\n",
    "        lemmas = [w for w in lemmas if w.lower() != words[idx].lower()]\n",
    "        if not lemmas:\n",
    "            return text\n",
    "        words[idx] = np.random.choice(lemmas)\n",
    "        return \" \".join(words)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "\n",
    "def maybe_augment_text(question: str, options: Dict[str, str]) -> (str, Dict[str, str]):\n",
    "    q, opts = question, dict(options)\n",
    "    if cfg.USE_SYNONYM_AUG and np.random.rand() < cfg.TEXT_AUG_PROB:\n",
    "        q = synonym_replace(q)\n",
    "    return q, opts\n",
    "\n",
    "\n",
    "class VQADataset(Dataset):\n",
    "    \"\"\"VQA Dataset (assistant label returned separately for proper masking).\"\"\"\n",
    "\n",
    "    def __init__(self, df, processor, data_dir=\"\", train=True, use_advanced=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.data_dir = data_dir\n",
    "        self.train = train\n",
    "        self.use_advanced = use_advanced\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_path = os.path.join(self.data_dir, row[\"path\"]) if \"path\" in row else row.get(\"image_path\", \"\")\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n",
    "\n",
    "        q0 = str(row[\"question\"]) if \"question\" in row else \"\"\n",
    "        opts0 = {\n",
    "            \"a\": str(row.get(\"a\", \"\")),\n",
    "            \"b\": str(row.get(\"b\", \"\")),\n",
    "            \"c\": str(row.get(\"c\", \"\")),\n",
    "            \"d\": str(row.get(\"d\", \"\")),\n",
    "        }\n",
    "        if self.train:\n",
    "            q0, opts0 = maybe_augment_text(q0, opts0)\n",
    "\n",
    "        user_text = build_mc_prompt(q0, opts0[\"a\"], opts0[\"b\"], opts0[\"c\"], opts0[\"d\"])\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": user_text}\n",
    "            ]}\n",
    "        ]\n",
    "\n",
    "        out = {\"messages\": messages, \"image\": img}\n",
    "        if self.train:\n",
    "            out[\"answer\"] = str(row.get(\"answer\", \"\")).strip().lower()\n",
    "        return out\n",
    "\n",
    "\n",
    "def _build_image_transform():\n",
    "    if not cfg.USE_IMAGE_AUGMENTATION:\n",
    "        return None\n",
    "    ops = [T.RandomHorizontalFlip(p=0.5)]\n",
    "    if cfg.USE_RANDAUGMENT:\n",
    "        try:\n",
    "            ops.append(T.RandAugment(num_ops=cfg.RANDAUG_N, magnitude=cfg.RANDAUG_M))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return T.Compose(ops)\n",
    "\n",
    "\n",
    "_IMG_TF = _build_image_transform()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollator:\n",
    "    \"\"\"Data Collator with correct prompt masking for decoder-only training.\"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    train: bool = True\n",
    "    use_advanced: bool = False\n",
    "    augment_images: bool = True\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, prompts, full_texts = [], [], []\n",
    "\n",
    "        for sample in batch:\n",
    "            img = sample[\"image\"]\n",
    "            if self.train and self.augment_images and _IMG_TF is not None:\n",
    "                try:\n",
    "                    img = _IMG_TF(img)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            messages = sample[\"messages\"]\n",
    "            prompt_text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            if self.train and \"answer\" in sample and sample[\"answer\"]:\n",
    "                conversation = messages + [\n",
    "                    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": str(sample[\"answer\"]).strip().lower()}]}\n",
    "                ]\n",
    "                full_text = self.processor.apply_chat_template(\n",
    "                    conversation, tokenize=False, add_generation_prompt=False\n",
    "                )\n",
    "            else:\n",
    "                full_text = prompt_text\n",
    "\n",
    "            # Normalize text\n",
    "            prompt_text = unicodedata.normalize('NFKC', prompt_text)\n",
    "            full_text = unicodedata.normalize('NFKC', full_text)\n",
    "\n",
    "            images.append(img)\n",
    "            prompts.append(prompt_text)\n",
    "            full_texts.append(full_text)\n",
    "\n",
    "        # Encode prompt and full sequences\n",
    "        enc_prompt = self.processor(\n",
    "            images=images,\n",
    "            text=prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=cfg.MAX_SEQUENCE_LENGTH,\n",
    "        )\n",
    "        enc_full = self.processor(\n",
    "            images=images,\n",
    "            text=full_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=cfg.MAX_SEQUENCE_LENGTH,\n",
    "        )\n",
    "\n",
    "        if self.train:\n",
    "            labels = enc_full[\"input_ids\"].clone()\n",
    "            # mask pads\n",
    "            labels[enc_full[\"attention_mask\"] == 0] = -100\n",
    "            # mask prompt tokens\n",
    "            for i in range(labels.size(0)):\n",
    "                full_mask = enc_full[\"attention_mask\"][i].bool()\n",
    "                prompt_len = int(enc_prompt[\"attention_mask\"][i].sum().item())\n",
    "                nonpad_idx = torch.nonzero(full_mask, as_tuple=False).squeeze(-1)\n",
    "                prompt_idx = nonpad_idx[:prompt_len]\n",
    "                labels[i, prompt_idx] = -100\n",
    "            enc_full[\"labels\"] = labels\n",
    "\n",
    "        return enc_full\n",
    "\n",
    "\n",
    "print(\"‚úÖ Dataset & DataCollator (with label masking) ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Model & Processor Î°úÎìú\n",
    "\n",
    "QLoRA Î™®Îç∏Í≥º ProcessorÎ•º Î°úÎìúÌï©ÎãàÎã§.\n",
    "\n",
    "### ‚úÖ T4 Ìò∏Ìôò ÏÑ§Ï†ï\n",
    "- Float16 (BFloat16 ÏïÑÎãò)\n",
    "- SDPA attention (FlashAttention Ï†úÍ±∞)\n",
    "- 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Ôªøimport torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "def _fp8_supported():\n",
    "    try:\n",
    "        import transformer_engine.pytorch as te  # noqa: F401\n",
    "    except Exception:\n",
    "        return False\n",
    "    if not torch.cuda.is_available():\n",
    "        return False\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    if major < 9:  # Hopper(H100)=9.x, Ampere(A100)=8.0 -> FP8 Ë™òÎ™Ñ???        return False\n",
    "    # PyTorch float8 ?¬Ä??Ë≠∞ÎåÅÏò± ?—â?\n",
    "    has_float8 = hasattr(torch, \"float8_e4m3fn\") and hasattr(torch, \"float8_e5m2\")\n",
    "    return bool(has_float8)\n",
    "\n",
    "\n",
    "def create_model_and_processor(model_id, use_advanced=False):\n",
    "    \"\"\"Create 4-bit QLoRA model + processor with left padding and auto device_map.\n",
    "    For Qwen3-VL family we prefer AutoModelForImageTextToText (fallback to Vision2Seq as needed).\n",
    "    \"\"\"\n",
    "\n",
    "    compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_id,\n",
    "        min_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
    "        max_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if hasattr(processor, \"tokenizer\"):\n",
    "        processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    qopt = getattr(cfg, \"QUANTIZATION\", \"\").lower()\n",
    "    want_fp8 = (\"fp8\" in model_id.lower()) or (qopt == \"fp8\")\n",
    "    want_bnb4 = (qopt == \"bnb4\")\n",
    "    can_fp8 = _fp8_supported()\n",
    "    use_fp8 = bool(want_fp8 and can_fp8)\n",
    "\n",
    "    load_model_id = model_id\n",
    "    if want_fp8 and not can_fp8:\n",
    "        # FP8 Ë™òÎ™Ñ????ÏÑçÍºç(A100 ???Î®ØÍΩå???Ïá∞Ïª≤ Â™õ¬Ä‰ª•Î¨íÌäÇÊø°??ÎåÄÍ∞ö\n",
    "        if model_id.lower().endswith(\"-fp8\"):\n",
    "            load_model_id = model_id[:-4]  # \"-FP8\" ?ÏíìÍµÖ\n",
    "        print(f\"[warn] FP8 not supported on this GPU. Falling back to 4-bit for {load_model_id}\")\n",
    "\n",
    "    def _load_image_text_to_text():\n",
    "        return AutoModelForImageTextToText.from_pretrained(\n",
    "            load_model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "    def _load_vision2seq(**kwargs):\n",
    "        return AutoModelForVision2Seq.from_pretrained(\n",
    "            load_model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    if use_fp8:\n",
    "        try:\n",
    "            base_model = _load_image_text_to_text()\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq()\n",
    "    elif want_bnb4:\n",
    "        try:\n",
    "            base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                load_model_id,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq(quantization_config=bnb_config)\n",
    "    else:\n",
    "        # FP16/BF16 ÂØÉÏéàÏ§à: ?Î¨íÏòÑ???ÎÜÅÏî† dtype Ôßû¬Ä??        try:\n",
    "            base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                load_model_id,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq(torch_dtype=torch.float16)\n",
    "\n",
    "    if want_bnb4 and not use_fp8:\n",
    "        base_model = prepare_model_for_kbit_training(base_model)\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prefer SDPA attention for speed/stability on PyTorch 2.x\n",
    "    try:\n",
    "        if hasattr(base_model, \"config\"):\n",
    "            base_model.config.attn_implementation = \"sdpa\"\n",
    "        torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.LORA_R,\n",
    "        lora_alpha=cfg.LORA_ALPHA,\n",
    "        lora_dropout=cfg.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        target_modules=cfg.TARGET_MODULES,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # ensure eos/pad ids are set to avoid generation issues\n",
    "    try:\n",
    "        if hasattr(model, \"generation_config\") and hasattr(processor, \"tokenizer\"):\n",
    "            if model.generation_config.pad_token_id is None:\n",
    "                model.generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "            if model.generation_config.eos_token_id is None:\n",
    "                model.generation_config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "    except Exception:\n",
    "        pass\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "print(\"   Loading model & processor...\")\n",
    "model, processor = create_model_and_processor(\n",
    "    cfg.MODEL_ID,\n",
    "    use_advanced=cfg.USE_ADVANCED_MODEL\n",
    ")\n",
    "print(\"   Model ready.\")\n",
    "\n",
    "# Optional compile for speed (set USE_TORCH_COMPILE=1)\n",
    "import os as _os\n",
    "if _os.environ.get(\"USE_TORCH_COMPILE\", \"0\") == \"1\":\n",
    "    try:\n",
    "        model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)  # type: ignore[attr-defined]\n",
    "        print(\"   Model compiled with torch.compile\")\n",
    "    except Exception as _e:\n",
    "        print(f\"[warn] torch.compile skipped: {_e}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Ôß§Ïíï? ?ÎçâÏüæ Ë´õÍ≥óÌäÇ ?—àÎ¶∞ ?Î®ÆÎ£û ?Î®ØÍπã\n",
    "# -------------------------------\n",
    "import random\n",
    "\n",
    "\n",
    "def _build_tmp_batch(ds, collate_fn, bs: int):\n",
    "    idxs = [random.randrange(len(ds)) for _ in range(min(bs, len(ds)))]\n",
    "    samples = [ds[i] for i in idxs]\n",
    "    return collate_fn(samples)\n",
    "\n",
    "\n",
    "def auto_tune_per_device_batch(model, processor, df, max_probe_samples: int = 64):\n",
    "    if not torch.cuda.is_available():\n",
    "        return cfg.BATCH_SIZE\n",
    "\n",
    "    # ?ÏÑëÎµÜ ?Í≥óÏî†?Í≥óÎÄë/ËÇÑÏíïÏ†Ö?ÎåÑÍΩ£ (?ÏààÎíø Ôßè‚ë§Î±∂)\n",
    "    probe_n = min(max_probe_samples, len(df))\n",
    "    probe_df = df.sample(probe_n, random_state=cfg.SEED).reset_index(drop=True)\n",
    "    ds = VQADataset(probe_df, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "    collate_fn = DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "\n",
    "    # ?Íæ®ÎÇ´Êè¥? ?Íæ©Ïò± ?„ÖºÏ†ôÈÅ∫¬Ä???Î®ØÏ≠ä ÔßùÏï∑?\n",
    "    base = max(1, int(cfg.BATCH_SIZE))\n",
    "    candidates = sorted({1, 2, 3, 4, 6, 8, 12, 16, 24, 32, base})\n",
    "    candidates = [c for c in candidates if c <= 32]\n",
    "\n",
    "    best = 1\n",
    "    compute_dtype = torch.float16\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.USE_AMP)\n",
    "\n",
    "    for bs in candidates:\n",
    "        torch.cuda.empty_cache()\n",
    "        ok = True\n",
    "        try:\n",
    "            batch = _build_tmp_batch(ds, collate_fn, bs)\n",
    "            batch = {k: v.to('cuda') if hasattr(v, 'to') else v for k, v in batch.items()}\n",
    "            model.train()\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.USE_AMP, dtype=compute_dtype):\n",
    "                out = model(**batch)\n",
    "                loss = out.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            del batch, out, loss\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower():\n",
    "                ok = False\n",
    "            else:\n",
    "                raise\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            ok = False\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if ok:\n",
    "            best = bs\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Ôßè‚ë∫Î™¥ ?Ï¢èÏäö Ë´õÍ≥óÌäÇ??ÔßçÏöéÎñ† ?Íæ©Ïüª ?„ÖΩÎÄ¶ ?—ä‚Äú??    import os\n",
    "    target_effective = int(os.environ.get(\"EFFECTIVE_BATCH_SIZE\", 16))\n",
    "    cfg.BATCH_SIZE = max(1, best)\n",
    "    cfg.GRAD_ACCUM_STEPS = max(1, target_effective // cfg.BATCH_SIZE)\n",
    "    print(\n",
    "        f\" Auto-tune: chosen per_device_batch={cfg.BATCH_SIZE} | grad_accum={cfg.GRAD_ACCUM_STEPS} | target_effective={target_effective}\"\n",
    "    )\n",
    "    return cfg.BATCH_SIZE\n",
    "\n",
    "\n",
    "try:\n",
    "    # train_df???ÏöéÍΩë ?¬Ä?Î®ØÍΩå Êø°ÏíïÎ±∂??    if 'train_df' in globals() and len(train_df) > 0:\n",
    "        auto_tune_per_device_batch(model, processor, train_df)\n",
    "except Exception as _e:\n",
    "    print(f\"[warn] Auto-tune skipped: {_e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 8. Training Loop with Advanced Techniques\n",
    "\n",
    "Í≥†Í∏â ÌïôÏäµ Í∏∞Î≤ïÏùÑ Ï†ÅÏö©Ìïú ÌïôÏäµ Î£®ÌîÑÏûÖÎãàÎã§.\n",
    "\n",
    "### ‚ú® Ï†ÅÏö©Îêú Í∏∞Î≤ï\n",
    "- ‚úÖ **AMP** (Automatic Mixed Precision)\n",
    "- ‚úÖ **EMA** (Exponential Moving Average)\n",
    "- ‚úÖ **SWA** (Stochastic Weight Averaging)\n",
    "- ‚úÖ **Cosine Warmup Scheduler**\n",
    "- ‚úÖ **Gradient Clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AMP_DTYPE = None\n",
    "try:\n",
    "    import torch as _torch\n",
    "    AMP_DTYPE = _torch.float16\n",
    "except Exception:\n",
    "    AMP_DTYPE = None\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average\"\"\"\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self.register()\n",
    "    \n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    \n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (\n",
    "                    self.decay * self.shadow[name] +\n",
    "                    (1.0 - self.decay) * param.data\n",
    "                )\n",
    "                self.shadow[name] = new_average.clone()\n",
    "    \n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "    \n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "def train_one_fold(model, train_loader, valid_loader, fold=0):\n",
    "    \"\"\" Fold \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Fold {fold}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg.LEARNING_RATE,\n",
    "        weight_decay=cfg.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    num_training_steps = cfg.NUM_EPOCHS * math.ceil(len(train_loader) / cfg.GRAD_ACCUM_STEPS)\n",
    "    num_warmup_steps = int(num_training_steps * cfg.WARMUP_RATIO)\n",
    "    \n",
    "    if cfg.USE_COSINE_SCHEDULE:\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps, num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps, num_training_steps\n",
    "        )\n",
    "    \n",
    "    # AMP Scaler\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=cfg.USE_AMP)\n",
    "    \n",
    "    # EMA\n",
    "    ema = EMA(model, decay=cfg.EMA_DECAY) if cfg.USE_EMA else None\n",
    "    \n",
    "    # SWA\n",
    "    swa_model = None\n",
    "    if cfg.USE_SWA:\n",
    "        swa_model = AveragedModel(model)\n",
    "        swa_scheduler = SWALR(optimizer, swa_lr=cfg.LEARNING_RATE * 0.1)\n",
    "    \n",
    "    # \n",
    "    global_step = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(cfg.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{cfg.NUM_EPOCHS} [train]\",\n",
    "            unit=\"batch\"\n",
    "        )\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar, start=1):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward with AMP\n",
    "            with torch.amp.autocast('cuda', enabled=cfg.USE_AMP, dtype=AMP_DTYPE):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / cfg.GRAD_ACCUM_STEPS\n",
    "            \n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            if step % cfg.GRAD_ACCUM_STEPS == 0:\n",
    "                # Gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.MAX_GRAD_NORM)\n",
    "                \n",
    "                # Optimizer step\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Scheduler step\n",
    "                if cfg.USE_SWA and epoch >= cfg.SWA_START_EPOCH:\n",
    "                    swa_scheduler.step()\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "                \n",
    "                # EMA update\n",
    "                if cfg.USE_EMA and ema is not None:\n",
    "                    ema.update()\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # Progress\n",
    "                avg_loss = running_loss / cfg.GRAD_ACCUM_STEPS\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": f\"{avg_loss:.4f}\",\n",
    "                    \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "                })\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # SWA model update\n",
    "        if cfg.USE_SWA and swa_model is not None and epoch >= cfg.SWA_START_EPOCH:\n",
    "            swa_model.update_parameters(model)\n",
    "        \n",
    "        # Validation\n",
    "        if cfg.USE_EMA and ema is not None:\n",
    "            ema.apply_shadow()\n",
    "        \n",
    "        val_loss = validate(model, valid_loader)\n",
    "        \n",
    "        if cfg.USE_EMA and ema is not None:\n",
    "            ema.restore()\n",
    "        \n",
    "        print(f\"[Epoch {epoch+1}] Valid Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Best model \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            \n",
    "            if cfg.USE_EMA and ema is not None:\n",
    "                ema.apply_shadow()\n",
    "            \n",
    "            _to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "            _to_save.save_pretrained(save_path)\n",
    "            processor.save_pretrained(save_path)\n",
    "            \n",
    "            if cfg.USE_EMA and ema is not None:\n",
    "                ema.restore()\n",
    "            \n",
    "            print(f\"    Best model saved to {save_path}\")\n",
    "    \n",
    "    # SWA \n",
    "    if cfg.USE_SWA and swa_model is not None:\n",
    "        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n",
    "        save_path = f\"{cfg.SAVE_DIR}/fold{fold}_swa\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        (swa_model.module if hasattr(swa_model, \"module\") else swa_model).save_pretrained(save_path)\n",
    "        processor.save_pretrained(save_path)\n",
    "        print(f\"    SWA model saved to {save_path}\")\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "def validate(model, valid_loader):\n",
    "    \"\"\"Validation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=\"Validating\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.amp.autocast('cuda', enabled=cfg.USE_AMP, dtype=AMP_DTYPE):\n",
    "                outputs = model(**batch)\n",
    "                total_loss += outputs.loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / len(valid_loader)\n",
    "\n",
    "\n",
    "print(\" Training functions  \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 9. Ïã§Ï†ú ÌïôÏäµ Ïã§Ìñâ\n",
    "\n",
    "K-Fold ÎòêÎäî Îã®Ïùº Î™®Îç∏ ÌïôÏäµÏùÑ Ïã§ÌñâÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold \n",
    "if cfg.USE_KFOLD:\n",
    "    results = {}\n",
    "    \n",
    "    for fold in cfg.TRAIN_FOLDS:\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"Starting Fold {fold}/{cfg.N_FOLDS-1}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        # \n",
    "        train_subset = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
    "        valid_subset = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Train: {len(train_subset)}, Valid: {len(valid_subset)}\")\n",
    "        \n",
    "        # Dataset\n",
    "        train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "        valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "        \n",
    "        # DataLoader\n",
    "        dp_active = cfg.USE_DATAPARALLEL and torch.cuda.is_available() and torch.cuda.device_count() > 1\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL, augment_images=True),\n",
    "            num_workers=cfg.NUM_WORKERS,\n",
    "            drop_last=dp_active,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=bool(cfg.NUM_WORKERS)\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_ds,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL, augment_images=False),\n",
    "            num_workers=cfg.NUM_WORKERS,\n",
    "            drop_last=False,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=bool(cfg.NUM_WORKERS)\n",
    "        )\n",
    "        \n",
    "        # \n",
    "        best_loss = train_one_fold(model, train_loader, valid_loader, fold=fold)\n",
    "        results[fold] = best_loss\n",
    "        \n",
    "        print(f\"\\n Fold {fold} : Best Val Loss = {best_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"All Folds Training Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for fold, loss in results.items():\n",
    "        print(f\"Fold {fold}: {loss:.4f}\")\n",
    "    print(f\"Average: {np.mean(list(results.values())):.4f}\")\n",
    "\n",
    "else:\n",
    "    #   \n",
    "    train_subset = train_df[train_df['fold'] == -1].reset_index(drop=True)\n",
    "    valid_subset = train_df[train_df['fold'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "    valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
    "    \n",
    "    dp_active = cfg.USE_DATAPARALLEL and torch.cuda.is_available() and torch.cuda.device_count() > 1\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL, augment_images=True),\n",
    "        num_workers=cfg.NUM_WORKERS,\n",
    "        drop_last=dp_active,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=bool(cfg.NUM_WORKERS)\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_ds,\n",
    "        batch_size=cfg.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL, augment_images=False),\n",
    "        num_workers=cfg.NUM_WORKERS,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=bool(cfg.NUM_WORKERS)\n",
    "    )\n",
    "    \n",
    "    best_loss = train_one_fold(model, train_loader, valid_loader, fold=0)\n",
    "    print(f\"\\n Single model  : Best Val Loss = {best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ 10. Inference with TTA\n",
    "\n",
    "Test-Time AugmentationÏùÑ ÌôúÏö©Ìïú Ï∂îÎ°†ÏùÑ ÏàòÌñâÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Ôªøimport os\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "from transformers import LogitsProcessorList\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "import re\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "def extract_choice(text: str) -> str:\n",
    "    text = text.strip().lower()\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "    if lines:\n",
    "        last = lines[-1]\n",
    "        if last in [\"a\", \"b\", \"c\", \"d\"]:\n",
    "            return last\n",
    "    for tok in text.split():\n",
    "        if tok in [\"a\", \"b\", \"c\", \"d\"]:\n",
    "            return tok\n",
    "    return \"a\"\n",
    "\n",
    "\n",
    "def infer_single_fold(model_path, test_df, output_path):\n",
    "    \"\"\"Single-fold inference loading saved LoRA adapter on top of base model.\"\"\"\n",
    "\n",
    "    compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    # Load base and then attach adapters\n",
    "    qopt = getattr(cfg, \"QUANTIZATION\", \"\").lower()\n",
    "    want_fp8 = (\"fp8\" in cfg.MODEL_ID.lower()) or (qopt == \"fp8\")\n",
    "    want_bnb4 = (qopt == \"bnb4\")\n",
    "    can_fp8 = False\n",
    "    try:\n",
    "        import transformer_engine.pytorch as te  # noqa: F401\n",
    "        if torch.cuda.is_available():\n",
    "            major, minor = torch.cuda.get_device_capability(0)\n",
    "            can_fp8 = major >= 9 and hasattr(torch, \"float8_e4m3fn\") and hasattr(torch, \"float8_e5m2\")\n",
    "    except Exception:\n",
    "        can_fp8 = False\n",
    "\n",
    "    use_fp8 = bool(want_fp8 and can_fp8)\n",
    "    load_model_id = cfg.MODEL_ID\n",
    "    if want_fp8 and not can_fp8:\n",
    "        if load_model_id.lower().endswith(\"-fp8\"):\n",
    "            load_model_id = load_model_id[:-4]\n",
    "        print(f\"[warn] FP8 not supported on this GPU. Falling back to 4-bit for {load_model_id}\")\n",
    "\n",
    "    def _load_image_text_to_text():\n",
    "        return AutoModelForImageTextToText.from_pretrained(\n",
    "            load_model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    def _load_vision2seq(**kwargs):\n",
    "        return AutoModelForVision2Seq.from_pretrained(\n",
    "            load_model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    if use_fp8:\n",
    "        try:\n",
    "            base_model = _load_image_text_to_text()\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq()\n",
    "    elif want_bnb4:\n",
    "        try:\n",
    "            base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                load_model_id,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq(quantization_config=bnb_config)\n",
    "    else:\n",
    "        try:\n",
    "            base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                load_model_id,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        except Exception:\n",
    "            base_model = _load_vision2seq(torch_dtype=torch.float16)\n",
    "    model_infer = PeftModel.from_pretrained(base_model, model_path)\n",
    "    processor_infer = AutoProcessor.from_pretrained(\n",
    "        model_path,\n",
    "        min_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
    "        max_pixels=cfg.IMAGE_SIZE * cfg.IMAGE_SIZE,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if hasattr(processor_infer, \"tokenizer\"):\n",
    "        processor_infer.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    try:\n",
    "        if hasattr(base_model, \"config\"):\n",
    "            base_model.config.attn_implementation = \"sdpa\"\n",
    "        torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    model_infer.eval()\n",
    "\n",
    "    # Restrict to a/b/c/d tokens if possible\n",
    "    try:\n",
    "        allowed_tokens = [\n",
    "            processor_infer.tokenizer.convert_tokens_to_ids(t)\n",
    "            for t in [\"a\", \"b\", \"c\", \"d\"]\n",
    "        ]\n",
    "        allowed_tokens = None if any(x is None for x in allowed_tokens) else allowed_tokens\n",
    "    except Exception:\n",
    "        allowed_tokens = None\n",
    "\n",
    "    predictions = []\n",
    "    for i in tqdm(range(len(test_df)), desc=\"Inference\"):\n",
    "        row = test_df.iloc[i]\n",
    "        img_path = os.path.join(cfg.DATA_DIR, row[\"path\"]) if \"path\" in row else row.get(\"image_path\", \"\")\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n",
    "\n",
    "        user_text = build_mc_prompt(\n",
    "            str(row[\"question\"]),\n",
    "            str(row[\"a\"]), str(row[\"b\"]), str(row[\"c\"]), str(row[\"d\"])\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": user_text}\n",
    "            ]}\n",
    "        ]\n",
    "\n",
    "        text = processor_infer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        def _predict_one(pil_img):\n",
    "            _inputs = processor_infer(\n",
    "                text=[text], images=[pil_img], return_tensors=\"pt\"\n",
    "            )\n",
    "            _inputs = {k: v.to(model_infer.device) if hasattr(v, 'to') else v for k, v in _inputs.items()}\n",
    "            lp = None\n",
    "            if allowed_tokens is not None:\n",
    "                class _Allowed:\n",
    "                    def __init__(self, allowed):\n",
    "                        self.allowed = set(int(x) for x in allowed)\n",
    "                    def __call__(self, input_ids, scores):\n",
    "                        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "                        idx = torch.tensor(list(self.allowed), device=scores.device)\n",
    "                        mask.index_fill_(1, idx, 0.0)\n",
    "                        return scores + mask\n",
    "                lp = LogitsProcessorList([_Allowed(allowed_tokens)])\n",
    "            with torch.no_grad():\n",
    "                out_ids = model_infer.generate(\n",
    "                    **_inputs,\n",
    "                    max_new_tokens=(1 if allowed_tokens is not None else cfg.MAX_NEW_TOKENS),\n",
    "                    do_sample=cfg.DO_SAMPLE,\n",
    "                    temperature=cfg.TEMPERATURE if cfg.DO_SAMPLE else None,\n",
    "                    eos_token_id=processor_infer.tokenizer.eos_token_id,\n",
    "                    logits_processor=lp,\n",
    "                )\n",
    "            _txt = processor_infer.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "            return extract_choice(_txt)\n",
    "\n",
    "        if cfg.USE_TTA:\n",
    "            # Disable hflip when question/options imply directionality\n",
    "            orient_words = re.compile(r\"(left|right|\\bÏôºÏ™Ω\\b|\\bÏò§Î•∏Ï™Ω\\b|\\bÏ¢å\\b|\\bÏö∞\\b|Î∞©Ìñ•|ÎåÄÏπ≠|mirror|mirrored)\", re.I)\n",
    "            qtxt = str(row.get(\"question\", \"\"))\n",
    "            otxt = \" \".join([str(row.get(k, \"\")) for k in [\"a\", \"b\", \"c\", \"d\"]])\n",
    "            is_dir_sensitive = bool(orient_words.search(qtxt) or orient_words.search(otxt))\n",
    "\n",
    "            votes = []\n",
    "            W, H = img.size\n",
    "            for s in cfg.TTA_SCALES:\n",
    "                new_img = img.resize((max(8, int(W*s)), max(8, int(H*s))))\n",
    "                votes.append(_predict_one(new_img))\n",
    "                if cfg.TTA_HFLIP and not is_dir_sensitive:\n",
    "                    votes.append(_predict_one(TF.hflip(new_img)))\n",
    "            answer = Counter(votes).most_common(1)[0][0] if votes else _predict_one(img)\n",
    "        else:\n",
    "            answer = _predict_one(img)\n",
    "        predictions.append(answer)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"answer\": predictions,\n",
    "    })\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\" Saved to {output_path}\")\n",
    "    return submission\n",
    "\n",
    "\n",
    "# Run inference across folds or single model\n",
    "predictions_all = []\n",
    "if cfg.USE_KFOLD:\n",
    "    for fold in cfg.TRAIN_FOLDS:\n",
    "        model_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n",
    "        output_path = f\"{cfg.OUTPUT_DIR}/submission_fold{fold}.csv\"\n",
    "        print(f\"\\n{'='*60}\\nInferencing Fold {fold}\\n{'='*60}\")\n",
    "        pred = infer_single_fold(model_path, test_df, output_path)\n",
    "        predictions_all.append(pred)\n",
    "else:\n",
    "    model_path = f\"{cfg.SAVE_DIR}/fold0_best\"\n",
    "    output_path = f\"{cfg.OUTPUT_DIR}/submission_single.csv\"\n",
    "    pred = infer_single_fold(model_path, test_df, output_path)\n",
    "    predictions_all.append(pred)\n",
    "\n",
    "print(\"\\n All inference complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 11. Ensemble\n",
    "\n",
    "Ïó¨Îü¨ FoldÏùò ÏòàÏ∏°ÏùÑ ÏïôÏÉÅÎ∏îÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if cfg.USE_KFOLD and len(predictions_all) > 1:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Ensemble (Weighted Voting)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Weighted Voting (weights=NoneÏù¥Î©¥ Í∑†Îì±)\n",
    "    ensemble_preds = []\n",
    "    \n",
    "    if cfg.ENSEMBLE_WEIGHTS is None or len(cfg.ENSEMBLE_WEIGHTS) != len(predictions_all):\n",
    "        weights = [1.0/len(predictions_all)] * len(predictions_all)\n",
    "    else:\n",
    "        s = sum(cfg.ENSEMBLE_WEIGHTS); weights = [w/s for w in cfg.ENSEMBLE_WEIGHTS]\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "        score = {\"a\":0.0, \"b\":0.0, \"c\":0.0, \"d\":0.0}\n",
    "        for k, pred in enumerate(predictions_all):\n",
    "            ans = str(pred.iloc[i]['answer']).strip().lower()\n",
    "            if ans in score: score[ans] += weights[k]\n",
    "        chosen = max(score.items(), key=lambda x: x[1])[0]\n",
    "        ensemble_preds.append(chosen)\n",
    "    \n",
    "    # ÏµúÏ¢Ö Ï†úÏ∂ú ÌååÏùº\n",
    "    final_submission = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"answer\": ensemble_preds\n",
    "    })\n",
    "    \n",
    "    final_path = f\"{cfg.OUTPUT_DIR}/submission_ensemble.csv\"\n",
    "    final_submission.to_csv(final_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Ensemble submission saved to {final_path}\")\n",
    "    print(f\"\\nAnswer Distribution:\")\n",
    "    print(final_submission['answer'].value_counts().sort_index())\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚úÖ Single model - No ensemble needed\")\n",
    "    final_submission = predictions_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 12. Í≤∞Í≥º Î∂ÑÏÑù Î∞è ÏãúÍ∞ÅÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ÎãµÎ≥Ä Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "answer_counts = final_submission['answer'].value_counts().sort_index()\n",
    "sns.barplot(x=answer_counts.index, y=answer_counts.values, palette='viridis', ax=ax)\n",
    "ax.set_title('Final Submission Answer Distribution', fontsize=14, weight='bold')\n",
    "ax.set_xlabel('Answer')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ÎπÑÏú® ÌëúÏãú\n",
    "for i, (ans, count) in enumerate(answer_counts.items()):\n",
    "    percentage = count / len(final_submission) * 100\n",
    "    ax.text(i, count + 10, f\"{percentage:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ÌÜµÍ≥Ñ Ï∂úÎ†•\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Final Statistics\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total predictions: {len(final_submission)}\")\n",
    "print(f\"\\nAnswer counts:\")\n",
    "for ans, count in answer_counts.items():\n",
    "    print(f\"  {ans}: {count:5d} ({count/len(final_submission)*100:5.1f}%)\")\n",
    "\n",
    "# Ï†úÏ∂ú ÌååÏùº ÏÉòÌîå\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Sample Predictions\")\n",
    "print(f\"{'='*60}\")\n",
    "print(final_submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ 13. ÏµúÏ¢Ö Ï†ïÎ¶¨\n",
    "\n",
    "### üéâ ÏôÑÎ£åÎêú ÏûëÏóÖ\n",
    "\n",
    "1. ‚úÖ **ÌôòÍ≤Ω ÏÑ§Ï†ï** - Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò Î∞è ÏûÑÌè¨Ìä∏\n",
    "2. ‚úÖ **Config** - ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌÜµÌï© Í¥ÄÎ¶¨\n",
    "3. ‚úÖ **Îç∞Ïù¥ÌÑ∞ Î°úÎìú & EDA** - ÌÉêÏÉâÏ†Å Î∂ÑÏÑù\n",
    "4. ‚úÖ **Stratified K-Fold** - CV Splits ÏÉùÏÑ±\n",
    "5. ‚úÖ **Dataset & DataLoader** - ÎùºÎ≤® Ï†ïÎ†¨ ÍµêÏ†ï Ï†ÅÏö©\n",
    "6. ‚úÖ **Model & Processor** - QLoRA Î™®Îç∏ Î°úÎìú (T4 Ìò∏Ìôò)\n",
    "7. ‚úÖ **Training Loop** - AMP, EMA, SWA, Cosine Warmup Ï†ÅÏö©\n",
    "8. ‚úÖ **Inference** - TTA ÏßÄÏõê Ï∂îÎ°†\n",
    "9. ‚úÖ **Ensemble** - Majority Voting\n",
    "10. ‚úÖ **Results** - ÏãúÍ∞ÅÌôî Î∞è ÌÜµÍ≥Ñ\n",
    "\n",
    "### üöÄ Îã§Ïùå Îã®Í≥Ñ\n",
    "\n",
    "1. **ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù**\n",
    "   - Learning rate, LoRA rank Ï°∞Ï†ï\n",
    "   - Batch size, Grad accumulation ÏµúÏ†ÅÌôî\n",
    "\n",
    "2. **Î™®Îç∏ ÌÅ¨Í∏∞ ÌôïÎåÄ**\n",
    "   - 7B Î™®Îç∏ ÏÇ¨Ïö© (Îçî ÎÜíÏùÄ Ï†ïÌôïÎèÑ)\n",
    "   - Image size Ï¶ùÍ∞Ä (512, 768)\n",
    "\n",
    "3. **Í≥†Í∏â Í∏∞Î≤ï ÌôúÏÑ±Ìôî**\n",
    "   - TTA scales Ï∂îÍ∞Ä\n",
    "   - SWA Ï†ÅÏö©\n",
    "   - Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÌôúÏÑ±Ìôî\n",
    "\n",
    "4. **ÏóêÌè≠ Ï¶ùÍ∞Ä**\n",
    "   - NUM_EPOCHS = 3~5\n",
    "\n",
    "### üìå Important Notes\n",
    "\n",
    "- **T4 Ìò∏Ìôò**: Float16, SDPA attention ÏÇ¨Ïö©\n",
    "- **ÎùºÎ≤® Ï†ïÎ†¨**: Assistant Î©îÏãúÏßÄÏóê Ï†ïÎãµ Ìè¨Ìï® (ÌïµÏã¨!)\n",
    "- **Ïû¨ÌòÑÏÑ±**: Seed 42 Í≥†Ï†ï\n",
    "- **Î©îÎ™®Î¶¨**: Gradient checkpointing, 4-bit QLoRA\n",
    "\n",
    "---\n",
    "\n",
    "**ü§ñ Generated for SSAFY AI Project 2025**\n",
    "\n",
    "**üìß Contact**: GitHub Issues\n",
    "\n",
    "**‚≠ê ÌñâÏö¥ÏùÑ ÎπïÎãàÎã§!**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8556585,
     "sourceId": 13477844,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}