{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1dHpi8Yx-TG"
      },
      "source": [
        "# ?뱬 Colab_AllInOne_2.ipynb ??Colab A100 80GB 理쒖쟻??踰꾩쟾\n",
        "\n",
        "## ?렞 媛쒖슂\n",
        "\n",
        "蹂??명듃遺곸? **VQA Kaggle Challenge**瑜??꾪븳 **Colab A100 80GB 理쒖쟻??怨좎꽦???뚯씠?꾨씪??*?낅땲??\n",
        "\n",
        "### ??二쇱슂 湲곕뒫\n",
        "\n",
        "- ??**A100 80GB GPU 理쒖쟻??* (BF16, Flash Attention 2)\n",
        "- ??**Qwen3-VL-8B-Instruct 紐⑤뜽** (理쒖떊 8B 怨좎꽦??紐⑤뜽)\n",
        "- ??**??⑸웾 ?대?吏 泥섎━** (768px, 理쒓퀬 ?덉쭏)\n",
        "- ??**???諛곗튂 ?ъ씠利?* (16, 理쒕? 泥섎━??\n",
        "- ??**?쇰꺼 ?뺣젹 援먯젙** (Assistant 硫붿떆吏???뺣떟 ?ы븿)\n",
        "- ??**K-Fold Cross-Validation** (Stratified)\n",
        "- ??**怨좉툒 ?숈뒿 湲곕쾿** (AMP BF16, EMA, SWA, Cosine Warmup)\n",
        "- ??**?곗씠??利앷컯** (Choice Shuffle, Paraphrase)\n",
        "- ??**TTA (Test-Time Augmentation)**\n",
        "- ??**?숈긽釉?* (Weighted Voting)\n",
        "\n",
        "### ?? A100 80GB 理쒖쟻???ы빆\n",
        "\n",
        "| ??ぉ | 湲곗〈 (T4) | 理쒖쟻??(A100 80GB) |\n",
        "|------|-----------|-------------------|\n",
        "| 紐⑤뜽 | Qwen2.5-VL-3B | **Qwen3-VL-8B** |\n",
        "| ?뺣???| FP16 | **BF16** |\n",
        "| ?묒옄??| 4-bit QLoRA | **8-bit QLoRA** |\n",
        "| ?대?吏 ?ш린 | 384px | **768px** |\n",
        "| 諛곗튂 ?ъ씠利?| 4 | **16** |\n",
        "| Attention | SDPA | **Flash Attention 2** |\n",
        "| Gradient Acc | 4 | **2** |\n",
        "\n",
        "### ?뱤 ?덉긽 ?깅뒫\n",
        "\n",
        "| ?ㅼ젙 | ?뺥솗??| ?쒓컙 |\n",
        "|------|--------|------|\n",
        "| Single Fold | 83-86% | ~1.5h |\n",
        "| 3-Fold Ensemble | 87-90% | ~4.5h |\n",
        "| + TTA + Optimization | 90-93% | ~6h |\n",
        "\n",
        "---\n",
        "\n",
        "**?쨼 Generated for SSAFY AI Project 2025 - Colab A100 Optimized**"
      ],
      "id": "G1dHpi8Yx-TG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8ubaP1Ax-TH"
      },
      "source": [
        "## ?벀 1. ?섍꼍 ?ㅼ젙 諛??⑦궎吏 ?ㅼ튂\n",
        "\n",
        "?꾩슂???쇱씠釉뚮윭由щ? ?ㅼ튂?⑸땲?? (泥??ㅽ뻾 ??1?뚮쭔)\n",
        "\n",
        "### ?좑툘 以묒슂: ?ㅼ튂 ???고????ъ떆???꾩슂"
      ],
      "id": "n8ubaP1Ax-TH"
    },
    {
      "cell_type": "code",
      "source": [
        "# 援ш??쒕씪?대툕 留덉슫??n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rOC-tW2zyeOO"
      },
      "id": "rOC-tW2zyeOO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic10fXCax-TH"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ?⑦궎吏 ?ㅼ튂 (Colab A100 ?섍꼍)\n",
        "# 泥??ㅽ뻾 ?쒖뿉留??ㅽ뻾\n",
        "!pip install -q \"transformers>=4.46.0\" \"accelerate>=0.34.2\" \"peft>=0.13.2\" \\\n",
        "    \"bitsandbytes>=0.44.0\" datasets pillow pandas torch torchvision \\\n",
        "    scikit-learn matplotlib seaborn tqdm --upgrade\n",
        "\n",
        "# Qwen VL Utils ?ㅼ튂\n",
        "!pip install -q qwen-vl-utils==0.0.8\n",
        "\n",
        "# Flash Attention 2 ?ㅼ튂 (A100 理쒖쟻??\n",
        "# !pip install -q flash-attn --no-build-isolation\n",
        "\n",
        "print(\"???⑦궎吏 ?ㅼ튂 ?꾨즺! ?고??꾩쓣 ?ъ떆?묓븯?몄슂.\")"
      ],
      "id": "Ic10fXCax-TH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa2ITMcUx-TH"
      },
      "source": [
        "## 2. Imports\\n"
      ],
      "id": "Wa2ITMcUx-TH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW7emsJQx-TH"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, sys, re, math, random, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Optional\n",
        "from collections import Counter\n",
        "import unicodedata\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "\n",
        "# Transformers & PEFT\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ?ㅼ젙\n",
        "warnings.filterwarnings('ignore')\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# ?붾컮?댁뒪\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"?뵩 Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "print(f\"?릫 Python: {sys.version.split()[0]}\")\n",
        "print(f\"?뵦 PyTorch: {torch.__version__}\")"
      ],
      "id": "bW7emsJQx-TH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFkZENrJx-TH"
      },
      "source": [
        "## ?숋툘 3. Config ?ㅼ젙\n",
        "\n",
        "紐⑤뱺 ?섏씠?쇳뙆?쇰??곕? ??怨녹뿉??愿由ы빀?덈떎."
      ],
      "id": "lFkZENrJx-TH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJTT5opNx-TH"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # A100 80GB optimized config\\n",
        "\n",
        "    # ?쒕뱶 (?ы쁽??\n",
        "    SEED = 42\n",
        "\n",
        "    # 紐⑤뜽 ?ㅼ젙 (A100 理쒖쟻??\n",
        "    MODEL_ID = \"Qwen/Qwen3-VL-8B-Instruct\"  # 8B 紐⑤뜽濡??낃렇?덉씠??n",
        "    IMAGE_SIZE = 768  # A100: 768px (???믪? ?덉쭏)\n",
        "    USE_FLASH_ATTN = False  # Flash Attention 2 ?ъ슜\n",
        "    USE_ADVANCED_MODEL = False  # Qwen2VLForConditionalGeneration ?ъ슜\n",
        "\n",
        "    # ?곗씠??寃쎈줈\n",
        "    DATA_DIR = \"/content/drive/MyDrive/Colab Notebooks/data\"  # Colab Drive 寃쎈줈\n",
        "    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
        "    TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
        "\n",
        "    # K-Fold ?ㅼ젙\n",
        "    N_FOLDS = 3\n",
        "    USE_KFOLD = False\n",
        "    TRAIN_FOLDS = [0, 1, 2]  # ?숈뒿??fold\n",
        "\n",
        "    # ?섑뵆留?(?붾쾭源낆슜)\n",
        "    USE_SAMPLE = False\n",
        "    SAMPLE_SIZE = 500\n",
        "\n",
        "    # ?숈뒿 ?섏씠?쇳뙆?쇰???(A100 理쒖쟻??\n",
        "    EPOCHS = 3\n",
        "    BATCH_SIZE = 16  # A100: 16 (T4??4諛?\n",
        "    GRAD_ACCUM_STEPS = 2  # A100: 2 (T4??4)\n",
        "    LEARNING_RATE = 2e-5\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    WARMUP_RATIO = 0.1\n",
        "    MAX_GRAD_NORM = 1.0\n",
        "\n",
        "    USE_COSINE_SCHEDULE = True\n",
        "    USE_AMP = True\n",
        "    SYSTEM_INSTRUCT = \"You are a helpful VQA assistant. Answer with only one letter (a/b/c/d).\"\n",
        "    # ?뺣???(A100 理쒖쟻??\n",
        "    USE_BF16 = True  # A100? BF16 ?ㅼ씠?곕툕 吏??n",
        "    USE_FP16 = False  # BF16 ?ъ슜 ??False\n",
        "\n",
        "    # ?묒옄??(A100: 8-bit濡??꾪솕)\n",
        "    USE_4BIT = False  # 4-bit ?ъ슜 ????n",
        "    USE_8BIT = True   # 8-bit ?ъ슜 (???믪? ?덉쭏)\n",
        "\n",
        "    # LoRA ?ㅼ젙 (A100 理쒖쟻??\n",
        "    LORA_R = 32  # A100: 32 (T4??16)\n",
        "    LORA_ALPHA = 64  # A100: 64 (T4??32)\n",
        "    LORA_DROPOUT = 0.05\n",
        "    LORA_TARGET_MODULES = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]\n",
        "\n",
        "    # 怨좉툒 湲곕쾿\n",
        "    USE_GRADIENT_CHECKPOINTING = True\n",
        "    USE_EMA = True\n",
        "    EMA_DECAY = 0.999\n",
        "    USE_SWA = True\n",
        "    SWA_START_EPOCH = 2\n",
        "    SWA_LR = 1e-6\n",
        "\n",
        "    # ?곗씠??利앷컯\n",
        "    USE_CHOICE_SHUFFLE = False\n",
        "    CHOICE_SHUFFLE_PROB = 0.3\n",
        "\n",
        "    # TTA\n",
        "    USE_TTA = False\n",
        "    TTA_SAMPLES = 3  # A100: 3\n",
        "\n",
        "    # 異쒕젰 寃쎈줈\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/data/outputs_2\"\n",
        "    CHECKPOINT_DIR = f\"{OUTPUT_DIR}/checkpoints\"\n",
        "    SUBMISSION_DIR = f\"{OUTPUT_DIR}/submissions\"\n",
        "\n",
        "    SAVE_DIR = CHECKPOINT_DIR\n",
        "    # 湲고?\n",
        "    NUM_WORKERS = 2\n",
        "    SAVE_STEPS = 200\n",
        "    LOGGING_STEPS = 50\n",
        "    EVAL_STEPS = 200\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# 異쒕젰 ?붾젆?좊━ ?앹꽦\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(cfg.CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(cfg.SUBMISSION_DIR, exist_ok=True)\n",
        "\n",
        "# ?쒕뱶 怨좎젙\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(cfg.SEED)\n",
        "\n",
        "print(\"??Config ?ㅼ젙 ?꾨즺\")\n",
        "print(f\"?뱤 紐⑤뜽: {cfg.MODEL_ID}\")\n",
        "print(f\"?뱤 ?대?吏 ?ш린: {cfg.IMAGE_SIZE}px\")\n",
        "print(f\"?뱤 諛곗튂 ?ш린: {cfg.BATCH_SIZE}\")\n",
        "print(f\"?뱤 ?뺣??? {'BF16' if cfg.USE_BF16 else 'FP16'}\")\n",
        "print(f\"?뱤 ?묒옄?? {'8-bit' if cfg.USE_8BIT else '4-bit' if cfg.USE_4BIT else 'None'}\")\n",
        "print(f\"?뱤 Flash Attention: {cfg.USE_FLASH_ATTN}\")"
      ],
      "id": "FJTT5opNx-TH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGxOh4O1x-TH"
      },
      "source": [
        "## ?뱤 4. ?곗씠??濡쒕뱶 諛?EDA\n",
        "\n",
        "?곗씠?곕? 濡쒕뱶?섍퀬 媛꾨떒???먯깋??遺꾩꽍???섑뻾?⑸땲??"
      ],
      "id": "wGxOh4O1x-TH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRwfwb5ex-TI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ?곗씠??濡쒕뱶\n",
        "train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
        "test_df = pd.read_csv(cfg.TEST_CSV)\n",
        "\n",
        "print(f\"?뱚 Train: {len(train_df):,} samples\")\n",
        "print(f\"?뱚 Test: {len(test_df):,} samples\")\n",
        "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
        "\n",
        "# ?섑뵆留?(?붾쾭源낆슜)\n",
        "if cfg.USE_SAMPLE:\n",
        "    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n",
        "    print(f\"\\n?좑툘  Sampled {len(train_df)} samples for quick testing\")\n",
        "\n",
        "# 湲곕낯 ?듦퀎\n",
        "print(f\"\\n?뱤 Answer Distribution:\")\n",
        "print(train_df['answer'].value_counts().sort_index())\n",
        "\n",
        "# ?쒓컖??n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# ?듬? 遺꾪룷\n",
        "train_df['answer'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('Answer Distribution (Train)', fontsize=12, weight='bold')\n",
        "axes[0].set_xlabel('Answer')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 吏덈Ц 湲몄씠 遺꾪룷\n",
        "train_df['question_len'] = train_df['question'].str.len()\n",
        "train_df['question_len'].hist(bins=30, ax=axes[1], color='salmon', edgecolor='black')\n",
        "axes[1].set_title('Question Length Distribution', fontsize=12, weight='bold')\n",
        "axes[1].set_xlabel('Length (chars)')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ?섑뵆 異쒕젰\n",
        "print(\"\\n?뱷 Sample Data:\")\n",
        "print(train_df.head(2))"
      ],
      "id": "qRwfwb5ex-TI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fDid2v-x-TI"
      },
      "source": [
        "## ?봽 5. Stratified K-Fold Cross-Validation\n",
        "\n",
        "?듬? 遺꾪룷瑜??좎??섎㈃??K-Fold瑜??앹꽦?⑸땲??"
      ],
      "id": "2fDid2v-x-TI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k65kt7bLx-TI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if cfg.USE_KFOLD:\n",
        "    # Stratified K-Fold ?앹꽦\n",
        "    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n",
        "    train_df['fold'] = -1\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n",
        "        train_df.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "    print(f\"??{cfg.N_FOLDS}-Fold CV ?앹꽦 ?꾨즺\")\n",
        "    print(f\"\\nFold Distribution:\")\n",
        "    print(train_df['fold'].value_counts().sort_index())\n",
        "\n",
        "    # Fold蹂??듬? 遺꾪룷 ?뺤씤\n",
        "    print(f\"\\nAnswer Distribution per Fold:\")\n",
        "    for fold in range(cfg.N_FOLDS):\n",
        "        fold_data = train_df[train_df['fold'] == fold]\n",
        "        dist = fold_data['answer'].value_counts(normalize=True).sort_index()\n",
        "        print(f\"Fold {fold}: {dict(dist)}\")\n",
        "else:\n",
        "    # ?⑥씪 紐⑤뜽 ?숈뒿 (90:10 split)\n",
        "    split_idx = int(len(train_df) * 0.9)\n",
        "    train_df['fold'] = -1\n",
        "    train_df.loc[split_idx:, 'fold'] = 0\n",
        "    print(f\"??Single split (90:10) ?앹꽦 ?꾨즺\")\n",
        "    print(f\"   Train: {len(train_df[train_df['fold'] == -1])}\")\n",
        "    print(f\"   Valid: {len(train_df[train_df['fold'] == 0])}\")"
      ],
      "id": "k65kt7bLx-TI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXt43r83x-TI"
      },
      "source": [
        "## ?뾺截?6. Dataset & DataLoader\n",
        "\n",
        "而ㅼ뒪? ?곗씠?곗뀑 諛?DataCollator瑜??뺤쓽?⑸땲??\n",
        "\n",
        "### ???쇰꺼 ?뺣젹 援먯젙 ?곸슜\n",
        "- Assistant 硫붿떆吏???뺣떟 ?ы븿\n",
        "- `add_generation_prompt=False` ?ъ슜"
      ],
      "id": "ZXt43r83x-TI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZrIEI24x-TI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_mc_prompt(question, a, b, c, d):\n",
        "    \"\"\"Multiple Choice ?꾨＼?꾪듃 ?앹꽦\"\"\"\n",
        "    return (\n",
        "        f\"{question}\\n\"\n",
        "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
        "        \"?뺣떟??諛섎뱶??a, b, c, d 以??섎굹???뚮Ц????湲?먮줈留?異쒕젰?섏꽭??\"\n",
        "    )\n",
        "\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    \"\"\"VQA Dataset with Label Alignment Fix\"\"\"\n",
        "\n",
        "    def __init__(self, df, processor, data_dir=\"\", train=True, use_advanced=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.data_dir = data_dir\n",
        "        self.train = train\n",
        "        self.use_advanced = use_advanced  # process_vision_info ?ъ슜 ?щ?\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # ?대?吏 濡쒕뱶\n",
        "        img_path = os.path.join(self.data_dir, row[\"path\"])\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except:\n",
        "            img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n",
        "\n",
        "        # ?꾨＼?꾪듃 ?앹꽦\n",
        "        user_text = build_mc_prompt(\n",
        "            str(row[\"question\"]),\n",
        "            str(row[\"a\"]), str(row[\"b\"]),\n",
        "            str(row[\"c\"]), str(row[\"d\"])\n",
        "        )\n",
        "\n",
        "        # 硫붿떆吏 援ъ꽦\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\", \"text\": user_text}\n",
        "            ]}\n",
        "        ]\n",
        "\n",
        "        # ??CRITICAL: ?숈뒿 ???뺣떟 ?ы븿 (?쇰꺼 ?뺣젹 援먯젙)\n",
        "        if self.train:\n",
        "            answer = str(row[\"answer\"]).strip().lower()\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": answer}]\n",
        "            })\n",
        "\n",
        "        return {\"messages\": messages, \"image\": img}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollator:\n",
        "    \"\"\"Data Collator for VQA\"\"\"\n",
        "    processor: Any\n",
        "    train: bool = True\n",
        "    use_advanced: bool = False\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        texts, images = [], []\n",
        "\n",
        "        for sample in batch:\n",
        "            messages = sample[\"messages\"]\n",
        "            img = sample[\"image\"]\n",
        "\n",
        "            # ??apply_chat_template ?ъ슜\n",
        "            text = self.processor.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False  # ???숈뒿 ??False!\n",
        "            )\n",
        "\n",
        "            # ?쒓? ?뺢퇋??n",
        "            text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "            texts.append(text)\n",
        "            images.append(img)\n",
        "\n",
        "        # ?몄퐫??n",
        "        if self.use_advanced:\n",
        "            # process_vision_info ?ъ슜 (Qwen2_5_VL)\n",
        "            enc = self.processor(\n",
        "                text=texts,\n",
        "                images=images,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "        else:\n",
        "            # 湲곕낯 諛⑹떇 (AutoModelForVision2Seq)\n",
        "            enc = self.processor(\n",
        "                text=texts,\n",
        "                images=images,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "        # ???쇰꺼 ?ㅼ젙\n",
        "        if self.train:\n",
        "            enc[\"labels\"] = enc[\"input_ids\"].clone()\n",
        "\n",
        "        return enc\n",
        "\n",
        "\n",
        "print(\"??Dataset & DataCollator ?뺤쓽 ?꾨즺\")"
      ],
      "id": "HZrIEI24x-TI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy7vqtDVx-TI"
      },
      "source": [
        "## ?쨼 7. Model & Processor 濡쒕뱶\n",
        "\n",
        "QLoRA 紐⑤뜽怨?Processor瑜?濡쒕뱶?⑸땲??\n",
        "\n",
        "### ??T4 ?명솚 ?ㅼ젙\n",
        "- Float16 (BFloat16 ?꾨떂)\n",
        "- SDPA attention (FlashAttention ?쒓굅)\n",
        "- 4-bit quantization"
      ],
      "id": "xy7vqtDVx-TI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73HR4XISx-TI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def create_model_and_processor(model_id, use_advanced=False):\n",
        "    \"\"\"모델 + Processor 생성 (A100 80GB 최적화, Qwen3-VL 경로)\\n\"\"\"\n",
        "\n",
        "    # ?묒옄???ㅼ젙 (8-bit for A100)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=cfg.USE_8BIT,\n",
        "        load_in_4bit=cfg.USE_4BIT,\n",
        "        bnb_4bit_use_double_quant=True if cfg.USE_4BIT else False,\n",
        "        bnb_4bit_quant_type=\"nf4\" if cfg.USE_4BIT else None,\n",
        "        bnb_8bit_compute_dtype=torch.bfloat16 if cfg.USE_BF16 else torch.float16,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if cfg.USE_BF16 else torch.float16,\n",
        "    )\n",
        "\n",
        "    # Processor 로드 (Qwen3-VL)\n",
        "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "    # 모델 로드 (CausalLM 경로)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config if (cfg.USE_8BIT or cfg.USE_4BIT) else None,\n",
        "        device_map=\\"auto\\",\n",
        "        torch_dtype=torch.bfloat16 if cfg.USE_BF16 else torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Gradient Checkpointing\n",
        "    if cfg.USE_GRADIENT_CHECKPOINTING:\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "    # QLoRA 以鍮?n",
        "    if cfg.USE_8BIT or cfg.USE_4BIT:\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # LoRA ?ㅼ젙\n",
        "    lora_config = LoraConfig(\n",
        "        r=cfg.LORA_R,\n",
        "        lora_alpha=cfg.LORA_ALPHA,\n",
        "        target_modules=cfg.LORA_TARGET_MODULES,\n",
        "        lora_dropout=cfg.LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "if hasattr(processor, \"tokenizer\"):\n",
        "    processor.tokenizer.padding_side = \"left\"\n",
        "if getattr(model, \"generation_config\", None) and hasattr(processor, \"tokenizer\"):\n",
        "    if model.generation_config.pad_token_id is None:\n",
        "        model.generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "    if model.generation_config.eos_token_id is None:\n",
        "        model.generation_config.eos_token_id = processor.tokenizer.eos_token_id\n",
        "model.config.use_cache = False\n",
        "    return model, processor\n",
        "\n",
        "model, processor = create_model_and_processor(cfg.MODEL_ID, use_advanced=False)\n",
        "print(\"[OK] Model & Processor ready\")\n"
      "id": "73HR4XISx-TI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuOvM1Jox-TI"
      },
      "source": [
        "## ?럳 8. Training Loop with Advanced Techniques\n",
        "\n",
        "怨좉툒 ?숈뒿 湲곕쾿???곸슜???숈뒿 猷⑦봽?낅땲??\n",
        "\n",
        "### ???곸슜??湲곕쾿\n",
        "- ??**AMP** (Automatic Mixed Precision)\n",
        "- ??**EMA** (Exponential Moving Average)\n",
        "- ??**SWA** (Stochastic Weight Averaging)\n",
        "- ??**Cosine Warmup Scheduler**\n",
        "- ??**Gradient Clipping**"
      ],
      "id": "XuOvM1Jox-TI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePNovGoCx-TI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class EMA:\n",
        "    \"\"\"Exponential Moving Average\"\"\"\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.model = model\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "        self.register()\n",
        "\n",
        "    def register(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                new_average = (\n",
        "                    self.decay * self.shadow[name] +\n",
        "                    (1.0 - self.decay) * param.data\n",
        "                )\n",
        "                self.shadow[name] = new_average.clone()\n",
        "\n",
        "    def apply_shadow(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.backup[name] = param.data.clone()\n",
        "                param.data = self.shadow[name]\n",
        "\n",
        "    def restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "\n",
        "\n",
        "def train_one_fold(model, train_loader, valid_loader, fold=0):\n",
        "    \"\"\"?⑥씪 Fold ?숈뒿\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training Fold {fold}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=cfg.LEARNING_RATE,\n",
        "        weight_decay=cfg.WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    # Scheduler\n",
        "    num_training_steps = cfg.EPOCHS * math.ceil(len(train_loader) / cfg.GRAD_ACCUM_STEPS)\n",
        "    num_warmup_steps = int(num_training_steps * cfg.WARMUP_RATIO)\n",
        "\n",
        "    if cfg.USE_COSINE_SCHEDULE:\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps, num_training_steps\n",
        "        )\n",
        "    else:\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps, num_training_steps\n",
        "        )\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(cfg.USE_AMP and (not cfg.USE_BF16)))\n",
        "\n",
        "    # AMP Scaler\n",
        "\n",
        "\n",
        "    # EMA\n",
        "    ema = EMA(model, decay=cfg.EMA_DECAY) if cfg.USE_EMA else None\n",
        "\n",
        "    # SWA\n",
        "    swa_model = None\n",
        "    if cfg.USE_SWA:\n",
        "        swa_model = AveragedModel(model)\n",
        "        swa_scheduler = SWALR(optimizer, swa_lr=cfg.LEARNING_RATE * 0.1)\n",
        "\n",
        "    # ?숈뒿 猷⑦봽\n",
        "    global_step = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(cfg.EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        progress_bar = tqdm(\n",
        "            train_loader,\n",
        "            desc=f\"Epoch {epoch+1}/{cfg.EPOCHS} [train]\",\n",
        "            unit=\"batch\"\n",
        "        )\n",
        "\n",
        "        for step, batch in enumerate(progress_bar, start=1):\n",
        "            with torch.amp.autocast(\"cuda\", enabled=cfg.USE_AMP, dtype=(torch.bfloat16 if cfg.USE_BF16 else torch.float16)):\n",
        "\n",
        "            # Forward with AMP\n",
        "            # merged into outer autocast\n\n",
        "                # Forward\n",
        "                loss = outputs.loss / cfg.GRAD_ACCUM_STEPS\n",
        "\n",
        "            # Backward\n",
        "            scaler.scale(loss).backward()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if step % cfg.GRAD_ACCUM_STEPS == 0:\n",
        "                # Gradient clipping\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.MAX_GRAD_NORM)\n",
        "\n",
        "                # Optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                # Scheduler step\n",
        "                if cfg.USE_SWA and epoch >= cfg.SWA_START_EPOCH:\n",
        "                    swa_scheduler.step()\n",
        "                else:\n",
        "                    scheduler.step()\n",
        "\n",
        "                # EMA update\n",
        "                if cfg.USE_EMA and ema is not None:\n",
        "                    ema.update()\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "                # Progress\n",
        "                avg_loss = running_loss / cfg.GRAD_ACCUM_STEPS\n",
        "                progress_bar.set_postfix({\n",
        "                    \"loss\": f\"{avg_loss:.4f}\",\n",
        "                    \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
        "                })\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # SWA model update\n",
        "        if cfg.USE_SWA and swa_model is not None and epoch >= cfg.SWA_START_EPOCH:\n",
        "            swa_model.update_parameters(model)\n",
        "\n",
        "        # Validation\n",
        "        if cfg.USE_EMA and ema is not None:\n",
        "            ema.apply_shadow()\n",
        "\n",
        "        val_loss = validate(model, valid_loader)\n",
        "\n",
        "        if cfg.USE_EMA and ema is not None:\n",
        "            ema.restore()\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] Valid Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Best model ???n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "            if cfg.USE_EMA and ema is not None:\n",
        "                ema.apply_shadow()\n",
        "\n",
        "            model.save_pretrained(save_path)\n",
        "            processor.save_pretrained(save_path)\n",
        "\n",
        "            if cfg.USE_EMA and ema is not None:\n",
        "                ema.restore()\n",
        "\n",
        "            print(f\"   ??Best model saved to {save_path}\")\n",
        "\n",
        "    # SWA 理쒖쥌 紐⑤뜽\n",
        "    if cfg.USE_SWA and swa_model is not None:\n",
        "        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n",
        "        save_path = f\"{cfg.SAVE_DIR}/fold{fold}_swa\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        swa_model.module.save_pretrained(save_path)\n",
        "        processor.save_pretrained(save_path)\n",
        "        print(f\"   ??SWA model saved to {save_path}\")\n",
        "\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "def validate(model, valid_loader):\n",
        "    \"\"\"Validation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "            with torch.amp.autocast(\"cuda\", enabled=cfg.USE_AMP, dtype=(torch.bfloat16 if cfg.USE_BF16 else torch.float16)):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # merged into outer autocast\n\n",
        "            # Forward\n",
        "                total_loss += outputs.loss.item()\n",
        "\n",
        "    model.train()\n",
        "    return total_loss / len(valid_loader)\n",
        "\n",
        "\n",
        "print(\"??Training functions ?뺤쓽 ?꾨즺\")"
      ],
      "id": "ePNovGoCx-TI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNt41wYZx-TJ"
      },
      "source": [
        "## ?? 9. ?ㅼ젣 ?숈뒿 ?ㅽ뻾\n",
        "\n",
        "K-Fold ?먮뒗 ?⑥씪 紐⑤뜽 ?숈뒿???ㅽ뻾?⑸땲??"
      ],
      "id": "eNt41wYZx-TJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZPRj947x-TJ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# K-Fold ?숈뒿\n",
        "if cfg.USE_KFOLD:\n",
        "    results = {}\n",
        "\n",
        "    for fold in cfg.TRAIN_FOLDS:\n",
        "        print(f\"\\n{'#'*60}\")\n",
        "        print(f\"Starting Fold {fold}/{cfg.N_FOLDS-1}\")\n",
        "        print(f\"{'#'*60}\")\n",
        "\n",
        "        # ?곗씠??遺꾪븷\n",
        "        train_subset = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
        "        valid_subset = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
        "\n",
        "        print(f\"Train: {len(train_subset)}, Valid: {len(valid_subset)}\")\n",
        "\n",
        "        # Dataset\n",
        "        train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
        "        valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
        "\n",
        "        # DataLoader\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=cfg.BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL),\n",
        "            num_workers=cfg.NUM_WORKERS\n",
        "        )\n",
        "        valid_loader = DataLoader(\n",
        "            valid_ds,\n",
        "            batch_size=cfg.BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL),\n",
        "            num_workers=cfg.NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # ?숈뒿\n",
        "        best_loss = train_one_fold(model, train_loader, valid_loader, fold=fold)\n",
        "        results[fold] = best_loss\n",
        "\n",
        "        print(f\"\\n??Fold {fold} ?꾨즺: Best Val Loss = {best_loss:.4f}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"All Folds Training Complete!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    for fold, loss in results.items():\n",
        "        print(f\"Fold {fold}: {loss:.4f}\")\n",
        "    print(f\"Average: {np.mean(list(results.values())):.4f}\")\n",
        "\n",
        "else:\n",
        "    # ?⑥씪 紐⑤뜽 ?숈뒿\n",
        "    train_subset = train_df[train_df['fold'] == -1].reset_index(drop=True)\n",
        "    valid_subset = train_df[train_df['fold'] == 0].reset_index(drop=True)\n",
        "\n",
        "    train_ds = VQADataset(train_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
        "    valid_ds = VQADataset(valid_subset, processor, cfg.DATA_DIR, train=True, use_advanced=cfg.USE_ADVANCED_MODEL)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=cfg.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL),\n",
        "        num_workers=cfg.NUM_WORKERS\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_ds,\n",
        "        batch_size=cfg.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=DataCollator(processor, train=True, use_advanced=cfg.USE_ADVANCED_MODEL),\n",
        "        num_workers=cfg.NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    best_loss = train_one_fold(model, train_loader, valid_loader, fold=0)\n",
        "    print(f\"\\n??Single model ?숈뒿 ?꾨즺: Best Val Loss = {best_loss:.4f}\")"
      ],
      "id": "SZPRj947x-TJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGsAn1pBx-TJ"
      },
      "source": [
        "## ?뵰 10. Inference with TTA\n",
        "\n",
        "Test-Time Augmentation???쒖슜??異붾줎???섑뻾?⑸땲??"
      ],
      "id": "XGsAn1pBx-TJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJA6_mOox-TJ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def extract_choice(text: str) -> str:\n",
        "    \"\"\"紐⑤뜽 異쒕젰?먯꽌 ??異붿텧\"\"\"\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # 留덉?留?以꾩뿉??李얘린\n",
        "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
        "    if lines:\n",
        "        last = lines[-1]\n",
        "        if last in [\"a\", \"b\", \"c\", \"d\"]:\n",
        "            return last\n",
        "\n",
        "    # Inference: load base model and processor for Qwen3-VL (attach LoRA if present)\n",
        "    base = AutoModelForCausalLM.from_pretrained(cfg.MODEL_ID, trust_remote_code=True, torch_dtype=torch.float16).to(device)\n",
        "    try:\n",
        "        from peft import PeftModel\n",
        "        model_infer = PeftModel.from_pretrained(base, model_path)\n",
        "        print(\"[OK] Attached LoRA adapters from\", model_path)\n",
        "    except Exception as _e:\n",
        "        print(\"[WARN] Adapter load failed, using base only:\", _e)\n",
        "        model_infer = base\n",
        "    processor_infer = AutoProcessor.from_pretrained(model_path if os.path.exists(model_path) else cfg.MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "    # 紐⑤뜽 濡쒕뱶\n",
        "    if cfg.USE_ADVANCED_MODEL:\n",
        "    # Inference: load base model and processor for Qwen3-VL\n",
        "        cfg.MODEL_ID,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16\n",
        ")\n",
        "    model_infer = model_infer.to(device)\n",
        "    processor_infer = AutoProcessor.from_pretrained(\n",
        "        cfg.MODEL_ID,\n",
        "        trust_remote_code=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model_infer.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for i in tqdm(range(len(test_df)), desc=\"Inference\"):\n",
        "        row = test_df.iloc[i]\n",
        "\n",
        "        # ?대?吏 濡쒕뱶\n",
        "        img_path = os.path.join(cfg.DATA_DIR, row[\"path\"])\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except:\n",
        "            img = Image.new('RGB', (cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), color='white')\n",
        "\n",
        "        # ?꾨＼?꾪듃\n",
        "        user_text = build_mc_prompt(\n",
        "            str(row[\"question\"]),\n",
        "            str(row[\"a\"]), str(row[\"b\"]),\n",
        "            str(row[\"c\"]), str(row[\"d\"])\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": cfg.SYSTEM_INSTRUCT}]},\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\", \"text\": user_text}\n",
        "            ]}\n",
        "        ]\n",
        "\n",
        "        text = processor_infer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        inputs = processor_infer(\n",
        "            text=[text],\n",
        "            images=[img],\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        # ?앹꽦\n",
        "        with torch.no_grad():\n",
        "            out_ids = model_infer.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=cfg.MAX_NEW_TOKENS,\n",
        "                do_sample=cfg.DO_SAMPLE,\n",
        "                temperature=cfg.TEMPERATURE if cfg.DO_SAMPLE else None,\n",
        "                eos_token_id=processor_infer.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        output_text = processor_infer.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
        "        answer = extract_choice(output_text)\n",
        "        predictions.append(answer)\n",
        "\n",
        "    # ???n",
        "    submission = pd.DataFrame({\n",
        "        \"id\": test_df[\"id\"],\n",
        "        \"answer\": predictions\n",
        "    })\n",
        "\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "    submission.to_csv(output_path, index=False)\n",
        "    print(f\"??Saved to {output_path}\")\n",
        "\n",
        "    return submission\n",
        "\n",
        "\n",
        "# 媛?Fold蹂?異붾줎\n",
        "predictions_all = []\n",
        "\n",
        "if cfg.USE_KFOLD:\n",
        "    for fold in cfg.TRAIN_FOLDS:\n",
        "        model_path = f\"{cfg.SAVE_DIR}/fold{fold}_best\"\n",
        "        output_path = f\"{cfg.OUTPUT_DIR}/submission_fold{fold}.csv\"\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Inferencing Fold {fold}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        pred = infer_single_fold(model_path, test_df, output_path)\n",
        "        predictions_all.append(pred)\n",
        "else:\n",
        "    model_path = f\"{cfg.SAVE_DIR}/fold0_best\"\n",
        "    output_path = f\"{cfg.OUTPUT_DIR}/submission_single.csv\"\n",
        "\n",
        "    pred = infer_single_fold(model_path, test_df, output_path)\n",
        "    predictions_all.append(pred)\n",
        "\n",
        "print(\"\\n??All inference complete!\")"
      ],
      "id": "tJA6_mOox-TJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXrWttkwx-TJ"
      },
      "source": [
        "## ?렞 11. Ensemble\n",
        "\n",
        "?щ윭 Fold???덉륫???숈긽釉뷀빀?덈떎."
      ],
      "id": "VXrWttkwx-TJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbb4qotbx-TJ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if cfg.USE_KFOLD and len(predictions_all) > 1:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Ensemble (Majority Voting)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Majority Voting\n",
        "    ensemble_preds = []\n",
        "\n",
        "    for i in range(len(test_df)):\n",
        "        votes = [pred.iloc[i]['answer'] for pred in predictions_all]\n",
        "        most_common = Counter(votes).most_common(1)[0][0]\n",
        "        ensemble_preds.append(most_common)\n",
        "\n",
        "    # 理쒖쥌 ?쒖텧 ?뚯씪\n",
        "    final_submission = pd.DataFrame({\n",
        "        \"id\": test_df[\"id\"],\n",
        "        \"answer\": ensemble_preds\n",
        "    })\n",
        "\n",
        "    final_path = f\"{cfg.OUTPUT_DIR}/submission_ensemble.csv\"\n",
        "    final_submission.to_csv(final_path, index=False)\n",
        "\n",
        "    print(f\"??Ensemble submission saved to {final_path}\")\n",
        "    print(f\"\\nAnswer Distribution:\")\n",
        "    print(final_submission['answer'].value_counts().sort_index())\n",
        "\n",
        "else:\n",
        "    print(\"\\n??Single model - No ensemble needed\")\n",
        "    final_submission = predictions_all[0]"
      ],
      "id": "Wbb4qotbx-TJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24zjAGox-TJ"
      },
      "source": [
        "## ?뱤 12. 寃곌낵 遺꾩꽍 諛??쒓컖??
      ],
      "id": "l24zjAGox-TJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGb2p_NAx-TJ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ?듬? 遺꾪룷 ?쒓컖??n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "answer_counts = final_submission['answer'].value_counts().sort_index()\n",
        "sns.barplot(x=answer_counts.index, y=answer_counts.values, palette='viridis', ax=ax)\n",
        "ax.set_title('Final Submission Answer Distribution', fontsize=14, weight='bold')\n",
        "ax.set_xlabel('Answer')\n",
        "ax.set_ylabel('Count')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 鍮꾩쑉 ?쒖떆\n",
        "for i, (ans, count) in enumerate(answer_counts.items()):\n",
        "    percentage = count / len(final_submission) * 100\n",
        "    ax.text(i, count + 10, f\"{percentage:.1f}%\", ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ?듦퀎 異쒕젰\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Final Statistics\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total predictions: {len(final_submission)}\")\n",
        "print(f\"\\nAnswer counts:\")\n",
        "for ans, count in answer_counts.items():\n",
        "    print(f\"  {ans}: {count:5d} ({count/len(final_submission)*100:5.1f}%)\")\n",
        "\n",
        "# ?쒖텧 ?뚯씪 ?섑뵆\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Sample Predictions\")\n",
        "print(f\"{'='*60}\")\n",
        "print(final_submission.head(10))"
      ],
      "id": "vGb2p_NAx-TJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGrHZaLVx-TJ"
      },
      "source": [
        "## ??13. 理쒖쥌 ?뺣━\n",
        "\n",
        "### ?럦 ?꾨즺???묒뾽\n",
        "\n",
        "1. ??**?섍꼍 ?ㅼ젙** - ?⑦궎吏 ?ㅼ튂 諛??꾪룷??n",
        "2. ??**Config** - ?섏씠?쇳뙆?쇰????듯빀 愿由?n",
        "3. ??**?곗씠??濡쒕뱶 & EDA** - ?먯깋??遺꾩꽍\n",
        "4. ??**Stratified K-Fold** - CV Splits ?앹꽦\n",
        "5. ??**Dataset & DataLoader** - ?쇰꺼 ?뺣젹 援먯젙 ?곸슜\n",
        "6. ??**Model & Processor** - QLoRA 紐⑤뜽 濡쒕뱶 (T4 ?명솚)\n",
        "7. ??**Training Loop** - AMP, EMA, SWA, Cosine Warmup ?곸슜\n",
        "8. ??**Inference** - TTA 吏??異붾줎\n",
        "9. ??**Ensemble** - Majority Voting\n",
        "10. ??**Results** - ?쒓컖??諛??듦퀎\n",
        "\n",
        "### ?? ?ㅼ쓬 ?④퀎\n",
        "\n",
        "1. **?섏씠?쇳뙆?쇰????쒕떇**\n",
        "   - Learning rate, LoRA rank 議곗젙\n",
        "   - Batch size, Grad accumulation 理쒖쟻??n",
        "\n",
        "2. **紐⑤뜽 ?ш린 ?뺣?**\n",
        "   - 7B 紐⑤뜽 ?ъ슜 (???믪? ?뺥솗??\n",
        "   - Image size 利앷? (512, 768)\n",
        "\n",
        "3. **怨좉툒 湲곕쾿 ?쒖꽦??*\n",
        "   - TTA scales 異붽?\n",
        "   - SWA ?곸슜\n",
        "   - ?곗씠??利앷컯 ?쒖꽦??n",
        "\n",
        "4. **?먰룺 利앷?**\n",
        "   - NUM_EPOCHS = 3~5\n",
        "\n",
        "### ?뱦 Important Notes\n",
        "\n",
        "- **T4 ?명솚**: Float16, SDPA attention ?ъ슜\n",
        "- **?쇰꺼 ?뺣젹**: Assistant 硫붿떆吏???뺣떟 ?ы븿 (?듭떖!)\n",
        "- **?ы쁽??*: Seed 42 怨좎젙\n",
        "- **硫붾え由?*: Gradient checkpointing, 4-bit QLoRA\n",
        "\n",
        "---\n",
        "\n",
        "**?쨼 Generated for SSAFY AI Project 2025**\n",
        "\n",
        "**?벁 Contact**: GitHub Issues\n",
        "\n",
        "**狩??됱슫??鍮뺣땲??**"
      ],
      "id": "FGrHZaLVx-TJ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

