{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Kaggle_Qwen3_30B_MultiGPU.ipynb\n",
        "\n",
        "## ğŸ¯ íŠ¹ì§•\n",
        "\n",
        "### ğŸ“Š ëª¨ë¸\n",
        "- **Qwen/Qwen2.5-VL-30B-A3B-Instruct** (30B íŒŒë¼ë¯¸í„°)\n",
        "- 4-bit Quantization (í•„ìˆ˜)\n",
        "- Multi-GPU ë³‘ë ¬ ì²˜ë¦¬ (T4 * 2)\n",
        "\n",
        "### ğŸ”§ ìµœì í™”\n",
        "- âœ… **Model Parallelism**: ëª¨ë¸ì„ ì—¬ëŸ¬ GPUì— ë¶„ì‚°\n",
        "- âœ… **Memory Optimization**: CPU offloading, gradient checkpointing\n",
        "- âœ… **4-bit QLoRA**: ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”\n",
        "- âœ… **Batch Size 1**: OOM ë°©ì§€\n",
        "- âœ… **Gradient Accumulation**: íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸°\n",
        "- âœ… **Mixed Precision**: Float16 + BFloat16\n",
        "- âœ… **Accelerate Integration**: ìë™ ë³‘ë ¬ ì²˜ë¦¬\n",
        "\n",
        "### âš ï¸ ì¤‘ìš” ì‚¬í•­\n",
        "- T4 * 2 (ì´ 32GB) í™˜ê²½ ìµœì í™”\n",
        "- EMA/SWA ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ë¶€ì¡±)\n",
        "- Inferenceë„ ë³‘ë ¬ ì²˜ë¦¬\n",
        "- ë§¤ìš° ë†’ì€ gradient accumulation\n",
        "\n",
        "**ğŸ¤– SSAFY AI Project 2025 - Qwen3-30B Multi-GPU Edition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "# !pip install -q transformers>=4.45.0 accelerate>=0.34.0 peft>=0.13.0 bitsandbytes>=0.43.0\n",
        "# !pip install -q datasets pillow pandas torch torchvision scikit-learn matplotlib seaborn tqdm scipy\n",
        "# !pip install -q qwen-vl-utils==0.0.8\n",
        "# !pip install -q deepspeed  # Optional: DeepSpeed for advanced optimization\n",
        "\n",
        "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ! ëŸ°íƒ€ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ & GPU í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, re, math, random, warnings, json, logging\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "import unicodedata\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
        "from accelerate.utils import set_seed as accelerate_set_seed\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# GPU í™•ì¸\n",
        "print(\"ğŸ” GPU í™•ì¸:\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"   ì‚¬ìš© ê°€ëŠ¥ GPU: {gpu_count}ê°œ\")\n",
        "    for i in range(gpu_count):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
        "        print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "    total_memory = sum(torch.cuda.get_device_properties(i).total_memory for i in range(gpu_count)) / 1e9\n",
        "    print(f\"   ì´ GPU ë©”ëª¨ë¦¬: {total_memory:.1f} GB\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"   âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "    raise RuntimeError(\"ì´ ë…¸íŠ¸ë¶ì€ GPUê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.\")\n",
        "\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "print(f\"   Transformers: {__import__('transformers').__version__}\")\n",
        "print(f\"   Accelerate: {__import__('accelerate').__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ 3. Config ì„¤ì • (Qwen3-30B Multi-GPU ìµœì í™”)\n",
        "\n",
        "### ğŸ¯ í•µì‹¬ ì„¤ì •\n",
        "- **ë©”ëª¨ë¦¬ ìµœì í™”**: 4-bit, CPU offload, gradient checkpointing\n",
        "- **ë³‘ë ¬ ì²˜ë¦¬**: Model parallelism across 2 GPUs\n",
        "- **ì•ˆì •ì„±**: ë§¤ìš° ì‘ì€ ë°°ì¹˜, ë†’ì€ gradient accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # ============ ì‹œë“œ ============\n",
        "    SEED = 42\n",
        "    \n",
        "    # ============ ëª¨ë¸ ============\n",
        "    MODEL_ID = \"Qwen/Qwen2.5-VL-30B-A3B-Instruct\"  # 30B ëª¨ë¸\n",
        "    IMAGE_SIZE = 384  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 384ë¡œ ì‹œì‘ (í•„ìš”ì‹œ 512)\n",
        "    \n",
        "    # ============ ë°ì´í„° ============\n",
        "    DATA_DIR = \"/content\"\n",
        "    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
        "    TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
        "    \n",
        "    # ============ K-Fold ============\n",
        "    N_FOLDS = 3\n",
        "    USE_KFOLD = True\n",
        "    TRAIN_FOLDS = [0, 1, 2]\n",
        "    \n",
        "    # ============ QLoRA (ë©”ëª¨ë¦¬ ìµœì í™”) ============\n",
        "    LORA_R = 8  # 30B ëª¨ë¸ì—ëŠ” ì‘ì€ rank ì‚¬ìš©\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # í•„ìˆ˜ ëª¨ë“ˆë§Œ\n",
        "    \n",
        "    # ============ í•™ìŠµ (ë©”ëª¨ë¦¬ ìµœì í™”) ============\n",
        "    NUM_EPOCHS = 2  # 30B ëª¨ë¸ì€ ì ì€ epochë„ ì¶©ë¶„\n",
        "    BATCH_SIZE = 1  # í•„ìˆ˜: OOM ë°©ì§€\n",
        "    GRAD_ACCUM_STEPS = 16  # ë†’ì€ accumulationìœ¼ë¡œ íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° í™•ë³´\n",
        "    LEARNING_RATE = 5e-5  # í° ëª¨ë¸ì€ ì‘ì€ LR\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    WARMUP_RATIO = 0.1  # ë” ê¸´ warmup\n",
        "    MAX_GRAD_NORM = 0.5  # ë” ì‘ì€ gradient clipping\n",
        "    \n",
        "    # ============ ë©”ëª¨ë¦¬ ìµœì í™” ============\n",
        "    USE_AMP = True  # Mixed precision\n",
        "    USE_GRADIENT_CHECKPOINTING = True  # í•„ìˆ˜\n",
        "    USE_CPU_OFFLOAD = True  # Optimizer statesë¥¼ CPUë¡œ\n",
        "    USE_8BIT_OPTIMIZER = False  # bitsandbytes 8-bit Adam (ì„ íƒ)\n",
        "    \n",
        "    # ============ ê³ ê¸‰ ê¸°ë²• (ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ ë¹„í™œì„±í™”) ============\n",
        "    USE_EMA = False  # ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë¹„í™œì„±í™”\n",
        "    USE_SWA = False  # ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë¹„í™œì„±í™”\n",
        "    USE_COSINE_SCHEDULE = True\n",
        "    \n",
        "    # ============ Early Stopping ============\n",
        "    USE_EARLY_STOPPING = False\n",
        "    EARLY_STOPPING_PATIENCE = 1\n",
        "    \n",
        "    # ============ TTA ============\n",
        "    USE_TTA = False  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë¹„í™œì„±í™” (í•„ìš”ì‹œ í™œì„±í™”)\n",
        "    TTA_SCALES = [1.0]  # Single scale\n",
        "    \n",
        "    # ============ ì¶”ë¡  ============\n",
        "    USE_DIRECT_LOGIT_DECODE = True\n",
        "    USE_BATCH_INFERENCE = False  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    INFER_BATCH_SIZE = 1\n",
        "    MAX_NEW_TOKENS = 8\n",
        "    \n",
        "    # ============ Temperature Scaling ============\n",
        "    USE_TEMPERATURE_SCALING = True\n",
        "    \n",
        "    # ============ ì•™ìƒë¸” ============\n",
        "    ENSEMBLE_METHOD = \"prob\"\n",
        "    FOLD_WEIGHTS = None\n",
        "    \n",
        "    # ============ ì €ì¥ ============\n",
        "    SAVE_DIR = f\"{DATA_DIR}/checkpoints_30b\"\n",
        "    OUTPUT_DIR = f\"{DATA_DIR}/outputs_30b\"\n",
        "    LOG_DIR = f\"{DATA_DIR}/logs_30b\"\n",
        "    SAVE_EVERY_EPOCH = False  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    \n",
        "    # ============ ìƒ˜í”Œë§ ============\n",
        "    USE_SAMPLE = False\n",
        "    SAMPLE_SIZE = 100\n",
        "    \n",
        "    # ============ í”„ë¡¬í”„íŠ¸ ============\n",
        "    SYSTEM_INSTRUCT = (\n",
        "        \"You are a helpful visual question answering assistant. \"\n",
        "        \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n",
        "    )\n",
        "    \n",
        "    # ============ ë¡œê¹… ============\n",
        "    LOG_LEVEL = logging.INFO\n",
        "    LOG_TO_FILE = True\n",
        "    \n",
        "    # ============ Multi-GPU ì„¤ì • ============\n",
        "    USE_MULTI_GPU = True  # ìë™ìœ¼ë¡œ ì—¬ëŸ¬ GPU ì‚¬ìš©\n",
        "    MAX_MEMORY_PER_GPU = {0: \"14GB\", 1: \"14GB\"}  # T4 * 2 (ê° 16GB ì¤‘ 14GB ì‚¬ìš©)\n",
        "    DEVICE_MAP = \"auto\"  # ìë™ ëª¨ë¸ ë³‘ë ¬í™”\n",
        "    \n",
        "    # ============ Quantization ============\n",
        "    LOAD_IN_4BIT = True  # í•„ìˆ˜: 4-bit quantization\n",
        "    LOAD_IN_8BIT = False\n",
        "    BNB_4BIT_COMPUTE_DTYPE = torch.float16  # T4ëŠ” BF16 ë¯¸ì§€ì›\n",
        "    BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "    BNB_4BIT_USE_DOUBLE_QUANT = True\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "for dir_path in [cfg.SAVE_DIR, cfg.OUTPUT_DIR, cfg.LOG_DIR]:\n",
        "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ë¡œê¹… ì„¤ì •\n",
        "def setup_logging():\n",
        "    logger = logging.getLogger('VQA_30B')\n",
        "    logger.setLevel(cfg.LOG_LEVEL)\n",
        "    logger.handlers.clear()\n",
        "    \n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    \n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "    \n",
        "    if cfg.LOG_TO_FILE:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        file_handler = logging.FileHandler(f\"{cfg.LOG_DIR}/training_30b_{timestamp}.log\")\n",
        "        file_handler.setFormatter(formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "    \n",
        "    return logger\n",
        "\n",
        "logger = setup_logging()\n",
        "\n",
        "# ì‹œë“œ ê³ ì •\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    accelerate_set_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(cfg.SEED)\n",
        "\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(\"ğŸš€ Qwen3-VL-30B Multi-GPU Configuration\")\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(f\"Model: {cfg.MODEL_ID}\")\n",
        "logger.info(f\"Image Size: {cfg.IMAGE_SIZE}\")\n",
        "logger.info(f\"Epochs: {cfg.NUM_EPOCHS}, Batch: {cfg.BATCH_SIZE}, Grad Accum: {cfg.GRAD_ACCUM_STEPS}\")\n",
        "logger.info(f\"Effective Batch Size: {cfg.BATCH_SIZE * cfg.GRAD_ACCUM_STEPS}\")\n",
        "logger.info(f\"LoRA R: {cfg.LORA_R}, LR: {cfg.LEARNING_RATE}\")\n",
        "logger.info(f\"Multi-GPU: {cfg.USE_MULTI_GPU}, Device Map: {cfg.DEVICE_MAP}\")\n",
        "logger.info(f\"4-bit Quantization: {cfg.LOAD_IN_4BIT}\")\n",
        "logger.info(f\"Gradient Checkpointing: {cfg.USE_GRADIENT_CHECKPOINTING}\")\n",
        "logger.info(f\"CPU Offload: {cfg.USE_CPU_OFFLOAD}\")\n",
        "logger.info(\"=\"*60)\n",
        "print(f\"\\nğŸ“ ë¡œê·¸ ì €ì¥: {cfg.LOG_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¾ 4. ë©”ëª¨ë¦¬ ìµœì í™” ìœ í‹¸ë¦¬í‹°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_gpu_memory(prefix=\"\"):\n",
        "    \"\"\"ëª¨ë“  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return\n",
        "    \n",
        "    logger.info(f\"{'='*60}\")\n",
        "    logger.info(f\"{prefix} GPU Memory Status\")\n",
        "    logger.info(f\"{'='*60}\")\n",
        "    \n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
        "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
        "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
        "        logger.info(f\"GPU {i}: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB, Total={total:.1f}GB\")\n",
        "    \n",
        "    logger.info(f\"{'='*60}\")\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            with torch.cuda.device(i):\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "    logger.info(\"ğŸ’¾ Memory cleared\")\n",
        "\n",
        "def estimate_model_memory(model):\n",
        "    \"\"\"ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\"\"\"\n",
        "    param_size = 0\n",
        "    buffer_size = 0\n",
        "    \n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    \n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    \n",
        "    total_size_gb = (param_size + buffer_size) / 1e9\n",
        "    logger.info(f\"ğŸ“Š Estimated model size: {total_size_gb:.2f} GB\")\n",
        "    return total_size_gb\n",
        "\n",
        "# ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ\n",
        "print_gpu_memory(\"Initial\")\n",
        "clear_memory()\n",
        "print_gpu_memory(\"After clearing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š 5. ë°ì´í„° ë¡œë“œ & EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
        "    test_df = pd.read_csv(cfg.TEST_CSV)\n",
        "    logger.info(f\"ğŸ“ Train: {len(train_df):,} samples\")\n",
        "    logger.info(f\"ğŸ“ Test: {len(test_df):,} samples\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    raise\n",
        "\n",
        "# ë°ì´í„° ê²€ì¦\n",
        "required_cols = ['question', 'a', 'b', 'c', 'd', 'answer']\n",
        "missing_cols = set(required_cols) - set(train_df.columns)\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}\")\n",
        "\n",
        "img_col = 'path' if 'path' in train_df.columns else 'image'\n",
        "logger.info(f\"ğŸ“· ì´ë¯¸ì§€ ì»¬ëŸ¼: {img_col}\")\n",
        "\n",
        "if cfg.USE_SAMPLE:\n",
        "    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n",
        "    logger.warning(f\"âš ï¸  Sampled {len(train_df)} samples for testing\")\n",
        "\n",
        "logger.info(f\"\\nğŸ“Š Answer Distribution:\")\n",
        "answer_dist = train_df['answer'].value_counts().sort_index()\n",
        "for ans, count in answer_dist.items():\n",
        "    logger.info(f\"   {ans}: {count:4d} ({count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "# ì‹œê°í™”\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "answer_dist.plot(kind='bar', ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('Answer Distribution', fontsize=12, weight='bold')\n",
        "axes[0].set_xlabel('Answer')\n",
        "axes[0].set_ylabel('Count')\n",
        "\n",
        "train_df['question_len'] = train_df['question'].str.len()\n",
        "train_df['question_len'].hist(bins=30, ax=axes[1], color='salmon')\n",
        "axes[1].set_title('Question Length', fontsize=12, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{cfg.LOG_DIR}/data_dist_30b.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "logger.info(\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ 6. Stratified K-Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if cfg.USE_KFOLD:\n",
        "    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n",
        "    train_df['fold'] = -1\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n",
        "        train_df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "    logger.info(f\"âœ… {cfg.N_FOLDS}-Fold CV ìƒì„±\")\n",
        "    for fold in range(cfg.N_FOLDS):\n",
        "        fold_count = (train_df['fold'] == fold).sum()\n",
        "        logger.info(f\"   Fold {fold}: {fold_count:4d} samples\")\n",
        "else:\n",
        "    split_idx = int(len(train_df) * 0.9)\n",
        "    train_df['fold'] = -1\n",
        "    train_df.loc[split_idx:, 'fold'] = 0\n",
        "    logger.info(\"âœ… Single split (90:10)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
