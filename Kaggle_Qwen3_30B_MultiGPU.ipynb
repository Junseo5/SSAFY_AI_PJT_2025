{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Kaggle_Qwen3_30B_MultiGPU.ipynb\n",
        "\n",
        "## 🎯 특징\n",
        "\n",
        "### 📊 모델\n",
        "- **Qwen/Qwen2.5-VL-30B-A3B-Instruct** (30B 파라미터)\n",
        "- 4-bit Quantization (필수)\n",
        "- Multi-GPU 병렬 처리 (T4 * 2)\n",
        "\n",
        "### 🔧 최적화\n",
        "- ✅ **Model Parallelism**: 모델을 여러 GPU에 분산\n",
        "- ✅ **Memory Optimization**: CPU offloading, gradient checkpointing\n",
        "- ✅ **4-bit QLoRA**: 메모리 효율 극대화\n",
        "- ✅ **Batch Size 1**: OOM 방지\n",
        "- ✅ **Gradient Accumulation**: 효과적인 배치 크기\n",
        "- ✅ **Mixed Precision**: Float16 + BFloat16\n",
        "- ✅ **Accelerate Integration**: 자동 병렬 처리\n",
        "\n",
        "### ⚠️ 중요 사항\n",
        "- T4 * 2 (총 32GB) 환경 최적화\n",
        "- EMA/SWA 비활성화 (메모리 부족)\n",
        "- Inference도 병렬 처리\n",
        "- 매우 높은 gradient accumulation\n",
        "\n",
        "**🤖 SSAFY AI Project 2025 - Qwen3-30B Multi-GPU Edition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📦 1. 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 패키지 설치\n",
        "# !pip install -q transformers>=4.45.0 accelerate>=0.34.0 peft>=0.13.0 bitsandbytes>=0.43.0\n",
        "# !pip install -q datasets pillow pandas torch torchvision scikit-learn matplotlib seaborn tqdm scipy\n",
        "# !pip install -q qwen-vl-utils==0.0.8\n",
        "# !pip install -q deepspeed  # Optional: DeepSpeed for advanced optimization\n",
        "\n",
        "print(\"✅ 패키지 설치 완료! 런타임 재시작하세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 2. 라이브러리 임포트 & GPU 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, re, math, random, warnings, json, logging\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "import unicodedata\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
        "from accelerate.utils import set_seed as accelerate_set_seed\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# GPU 확인\n",
        "print(\"🔍 GPU 확인:\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"   사용 가능 GPU: {gpu_count}개\")\n",
        "    for i in range(gpu_count):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
        "        print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "    total_memory = sum(torch.cuda.get_device_properties(i).total_memory for i in range(gpu_count)) / 1e9\n",
        "    print(f\"   총 GPU 메모리: {total_memory:.1f} GB\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"   ❌ GPU를 사용할 수 없습니다!\")\n",
        "    raise RuntimeError(\"이 노트북은 GPU가 필수입니다.\")\n",
        "\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "print(f\"   Transformers: {__import__('transformers').__version__}\")\n",
        "print(f\"   Accelerate: {__import__('accelerate').__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ 3. Config 설정 (Qwen3-30B Multi-GPU 최적화)\n",
        "\n",
        "### 🎯 핵심 설정\n",
        "- **메모리 최적화**: 4-bit, CPU offload, gradient checkpointing\n",
        "- **병렬 처리**: Model parallelism across 2 GPUs\n",
        "- **안정성**: 매우 작은 배치, 높은 gradient accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # ============ 시드 ============\n",
        "    SEED = 42\n",
        "    \n",
        "    # ============ 모델 ============\n",
        "    MODEL_ID = \"Qwen/Qwen2.5-VL-30B-A3B-Instruct\"  # 30B 모델\n",
        "    IMAGE_SIZE = 384  # 메모리 절약을 위해 384로 시작 (필요시 512)\n",
        "    \n",
        "    # ============ 데이터 ============\n",
        "    DATA_DIR = \"/content\"\n",
        "    TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
        "    TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
        "    \n",
        "    # ============ K-Fold ============\n",
        "    N_FOLDS = 3\n",
        "    USE_KFOLD = True\n",
        "    TRAIN_FOLDS = [0, 1, 2]\n",
        "    \n",
        "    # ============ QLoRA (메모리 최적화) ============\n",
        "    LORA_R = 8  # 30B 모델에는 작은 rank 사용\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # 필수 모듈만\n",
        "    \n",
        "    # ============ 학습 (메모리 최적화) ============\n",
        "    NUM_EPOCHS = 2  # 30B 모델은 적은 epoch도 충분\n",
        "    BATCH_SIZE = 1  # 필수: OOM 방지\n",
        "    GRAD_ACCUM_STEPS = 16  # 높은 accumulation으로 효과적 배치 크기 확보\n",
        "    LEARNING_RATE = 5e-5  # 큰 모델은 작은 LR\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    WARMUP_RATIO = 0.1  # 더 긴 warmup\n",
        "    MAX_GRAD_NORM = 0.5  # 더 작은 gradient clipping\n",
        "    \n",
        "    # ============ 메모리 최적화 ============\n",
        "    USE_AMP = True  # Mixed precision\n",
        "    USE_GRADIENT_CHECKPOINTING = True  # 필수\n",
        "    USE_CPU_OFFLOAD = True  # Optimizer states를 CPU로\n",
        "    USE_8BIT_OPTIMIZER = False  # bitsandbytes 8-bit Adam (선택)\n",
        "    \n",
        "    # ============ 고급 기법 (메모리 고려하여 비활성화) ============\n",
        "    USE_EMA = False  # 메모리 부족으로 비활성화\n",
        "    USE_SWA = False  # 메모리 부족으로 비활성화\n",
        "    USE_COSINE_SCHEDULE = True\n",
        "    \n",
        "    # ============ Early Stopping ============\n",
        "    USE_EARLY_STOPPING = False\n",
        "    EARLY_STOPPING_PATIENCE = 1\n",
        "    \n",
        "    # ============ TTA ============\n",
        "    USE_TTA = False  # 메모리 절약을 위해 비활성화 (필요시 활성화)\n",
        "    TTA_SCALES = [1.0]  # Single scale\n",
        "    \n",
        "    # ============ 추론 ============\n",
        "    USE_DIRECT_LOGIT_DECODE = True\n",
        "    USE_BATCH_INFERENCE = False  # 메모리 절약\n",
        "    INFER_BATCH_SIZE = 1\n",
        "    MAX_NEW_TOKENS = 8\n",
        "    \n",
        "    # ============ Temperature Scaling ============\n",
        "    USE_TEMPERATURE_SCALING = True\n",
        "    \n",
        "    # ============ 앙상블 ============\n",
        "    ENSEMBLE_METHOD = \"prob\"\n",
        "    FOLD_WEIGHTS = None\n",
        "    \n",
        "    # ============ 저장 ============\n",
        "    SAVE_DIR = f\"{DATA_DIR}/checkpoints_30b\"\n",
        "    OUTPUT_DIR = f\"{DATA_DIR}/outputs_30b\"\n",
        "    LOG_DIR = f\"{DATA_DIR}/logs_30b\"\n",
        "    SAVE_EVERY_EPOCH = False  # 메모리 절약\n",
        "    \n",
        "    # ============ 샘플링 ============\n",
        "    USE_SAMPLE = False\n",
        "    SAMPLE_SIZE = 100\n",
        "    \n",
        "    # ============ 프롬프트 ============\n",
        "    SYSTEM_INSTRUCT = (\n",
        "        \"You are a helpful visual question answering assistant. \"\n",
        "        \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n",
        "    )\n",
        "    \n",
        "    # ============ 로깅 ============\n",
        "    LOG_LEVEL = logging.INFO\n",
        "    LOG_TO_FILE = True\n",
        "    \n",
        "    # ============ Multi-GPU 설정 ============\n",
        "    USE_MULTI_GPU = True  # 자동으로 여러 GPU 사용\n",
        "    MAX_MEMORY_PER_GPU = {0: \"14GB\", 1: \"14GB\"}  # T4 * 2 (각 16GB 중 14GB 사용)\n",
        "    DEVICE_MAP = \"auto\"  # 자동 모델 병렬화\n",
        "    \n",
        "    # ============ Quantization ============\n",
        "    LOAD_IN_4BIT = True  # 필수: 4-bit quantization\n",
        "    LOAD_IN_8BIT = False\n",
        "    BNB_4BIT_COMPUTE_DTYPE = torch.float16  # T4는 BF16 미지원\n",
        "    BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "    BNB_4BIT_USE_DOUBLE_QUANT = True\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# 디렉토리 생성\n",
        "for dir_path in [cfg.SAVE_DIR, cfg.OUTPUT_DIR, cfg.LOG_DIR]:\n",
        "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 로깅 설정\n",
        "def setup_logging():\n",
        "    logger = logging.getLogger('VQA_30B')\n",
        "    logger.setLevel(cfg.LOG_LEVEL)\n",
        "    logger.handlers.clear()\n",
        "    \n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    \n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "    \n",
        "    if cfg.LOG_TO_FILE:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        file_handler = logging.FileHandler(f\"{cfg.LOG_DIR}/training_30b_{timestamp}.log\")\n",
        "        file_handler.setFormatter(formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "    \n",
        "    return logger\n",
        "\n",
        "logger = setup_logging()\n",
        "\n",
        "# 시드 고정\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    accelerate_set_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(cfg.SEED)\n",
        "\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(\"🚀 Qwen3-VL-30B Multi-GPU Configuration\")\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(f\"Model: {cfg.MODEL_ID}\")\n",
        "logger.info(f\"Image Size: {cfg.IMAGE_SIZE}\")\n",
        "logger.info(f\"Epochs: {cfg.NUM_EPOCHS}, Batch: {cfg.BATCH_SIZE}, Grad Accum: {cfg.GRAD_ACCUM_STEPS}\")\n",
        "logger.info(f\"Effective Batch Size: {cfg.BATCH_SIZE * cfg.GRAD_ACCUM_STEPS}\")\n",
        "logger.info(f\"LoRA R: {cfg.LORA_R}, LR: {cfg.LEARNING_RATE}\")\n",
        "logger.info(f\"Multi-GPU: {cfg.USE_MULTI_GPU}, Device Map: {cfg.DEVICE_MAP}\")\n",
        "logger.info(f\"4-bit Quantization: {cfg.LOAD_IN_4BIT}\")\n",
        "logger.info(f\"Gradient Checkpointing: {cfg.USE_GRADIENT_CHECKPOINTING}\")\n",
        "logger.info(f\"CPU Offload: {cfg.USE_CPU_OFFLOAD}\")\n",
        "logger.info(\"=\"*60)\n",
        "print(f\"\\n📝 로그 저장: {cfg.LOG_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 💾 4. 메모리 최적화 유틸리티"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_gpu_memory(prefix=\"\"):\n",
        "    \"\"\"모든 GPU 메모리 사용량 출력\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return\n",
        "    \n",
        "    logger.info(f\"{'='*60}\")\n",
        "    logger.info(f\"{prefix} GPU Memory Status\")\n",
        "    logger.info(f\"{'='*60}\")\n",
        "    \n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
        "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
        "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
        "        logger.info(f\"GPU {i}: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB, Total={total:.1f}GB\")\n",
        "    \n",
        "    logger.info(f\"{'='*60}\")\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"메모리 정리\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            with torch.cuda.device(i):\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "    logger.info(\"💾 Memory cleared\")\n",
        "\n",
        "def estimate_model_memory(model):\n",
        "    \"\"\"모델 메모리 사용량 추정\"\"\"\n",
        "    param_size = 0\n",
        "    buffer_size = 0\n",
        "    \n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    \n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    \n",
        "    total_size_gb = (param_size + buffer_size) / 1e9\n",
        "    logger.info(f\"📊 Estimated model size: {total_size_gb:.2f} GB\")\n",
        "    return total_size_gb\n",
        "\n",
        "# 초기 메모리 상태\n",
        "print_gpu_memory(\"Initial\")\n",
        "clear_memory()\n",
        "print_gpu_memory(\"After clearing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 5. 데이터 로드 & EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    train_df = pd.read_csv(cfg.TRAIN_CSV)\n",
        "    test_df = pd.read_csv(cfg.TEST_CSV)\n",
        "    logger.info(f\"📁 Train: {len(train_df):,} samples\")\n",
        "    logger.info(f\"📁 Test: {len(test_df):,} samples\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"❌ 데이터 로드 실패: {e}\")\n",
        "    raise\n",
        "\n",
        "# 데이터 검증\n",
        "required_cols = ['question', 'a', 'b', 'c', 'd', 'answer']\n",
        "missing_cols = set(required_cols) - set(train_df.columns)\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"❌ 필수 컬럼 누락: {missing_cols}\")\n",
        "\n",
        "img_col = 'path' if 'path' in train_df.columns else 'image'\n",
        "logger.info(f\"📷 이미지 컬럼: {img_col}\")\n",
        "\n",
        "if cfg.USE_SAMPLE:\n",
        "    train_df = train_df.sample(n=min(cfg.SAMPLE_SIZE, len(train_df)), random_state=cfg.SEED).reset_index(drop=True)\n",
        "    logger.warning(f\"⚠️  Sampled {len(train_df)} samples for testing\")\n",
        "\n",
        "logger.info(f\"\\n📊 Answer Distribution:\")\n",
        "answer_dist = train_df['answer'].value_counts().sort_index()\n",
        "for ans, count in answer_dist.items():\n",
        "    logger.info(f\"   {ans}: {count:4d} ({count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "# 시각화\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "answer_dist.plot(kind='bar', ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('Answer Distribution', fontsize=12, weight='bold')\n",
        "axes[0].set_xlabel('Answer')\n",
        "axes[0].set_ylabel('Count')\n",
        "\n",
        "train_df['question_len'] = train_df['question'].str.len()\n",
        "train_df['question_len'].hist(bins=30, ax=axes[1], color='salmon')\n",
        "axes[1].set_title('Question Length', fontsize=12, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{cfg.LOG_DIR}/data_dist_30b.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "logger.info(\"✅ 데이터 로드 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 6. Stratified K-Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if cfg.USE_KFOLD:\n",
        "    skf = StratifiedKFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=cfg.SEED)\n",
        "    train_df['fold'] = -1\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['answer'])):\n",
        "        train_df.loc[val_idx, 'fold'] = fold\n",
        "    \n",
        "    logger.info(f\"✅ {cfg.N_FOLDS}-Fold CV 생성\")\n",
        "    for fold in range(cfg.N_FOLDS):\n",
        "        fold_count = (train_df['fold'] == fold).sum()\n",
        "        logger.info(f\"   Fold {fold}: {fold_count:4d} samples\")\n",
        "else:\n",
        "    split_idx = int(len(train_df) * 0.9)\n",
        "    train_df['fold'] = -1\n",
        "    train_df.loc[split_idx:, 'fold'] = 0\n",
        "    logger.info(\"✅ Single split (90:10)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
