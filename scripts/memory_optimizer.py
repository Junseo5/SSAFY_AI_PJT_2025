"""
Kaggle VQA Challenge - GPU Memory Optimizer

This script provides GPU memory management and optimization for T4 GPUs:
- Memory monitoring
- Automatic configuration based on available memory
- Memory-efficient training arguments
"""

import torch
import gc
from typing import Dict
from transformers import TrainingArguments


class GPUMemoryManager:
    """GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""

    def __init__(self):
        self.peak_memory = 0.0

    @staticmethod
    def clear_cache():
        """
        GPU ìºì‹œ ì •ë¦¬

        Usage:
            GPUMemoryManager.clear_cache()
        """
        gc.collect()
        torch.cuda.empty_cache()

        if torch.cuda.is_available():
            torch.cuda.synchronize()

    @staticmethod
    def get_memory_stats() -> Dict[str, float]:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš© í†µê³„

        Returns:
            dict: {
                'allocated': float (GB),
                'reserved': float (GB),
                'max_allocated': float (GB)
            }
        """
        if not torch.cuda.is_available():
            return {}

        stats = {
            'allocated': torch.cuda.memory_allocated() / 1e9,
            'reserved': torch.cuda.memory_reserved() / 1e9,
            'max_allocated': torch.cuda.max_memory_allocated() / 1e9
        }

        return stats

    @staticmethod
    def print_memory_stats():
        """ë©”ëª¨ë¦¬ í†µê³„ ì¶œë ¥"""
        stats = GPUMemoryManager.get_memory_stats()

        if stats:
            print(f"\n{'â”€'*60}")
            print("GPU Memory Statistics")
            print(f"{'â”€'*60}")
            print(f"  Allocated:     {stats['allocated']:.2f} GB")
            print(f"  Reserved:      {stats['reserved']:.2f} GB")
            print(f"  Max Allocated: {stats['max_allocated']:.2f} GB")
            print(f"{'â”€'*60}\n")
        else:
            print("âŒ CUDA not available")

    @staticmethod
    def get_available_memory() -> float:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬ ë°˜í™˜

        Returns:
            float: ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ (GB)
        """
        if not torch.cuda.is_available():
            return 0.0

        # ì „ì²´ ë©”ëª¨ë¦¬
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9

        # í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬
        allocated = torch.cuda.memory_allocated() / 1e9

        # ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬
        available = total_memory - allocated

        return available

    @staticmethod
    def optimize_training_config(available_memory_gb: float = None) -> Dict:
        """
        ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  í•™ìŠµ ì„¤ì •

        Args:
            available_memory_gb: ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ (GB)
                                Noneì´ë©´ ìë™ ê°ì§€

        Returns:
            dict: {
                'batch_size': int,
                'gradient_accumulation_steps': int,
                'use_gradient_checkpointing': bool,
                'compute_dtype': torch.dtype
            }

        âœ… CRITICAL: T4 í˜¸í™˜ì„±ì„ ìœ„í•´ torch.float16 ì‚¬ìš©
        """
        if available_memory_gb is None:
            if torch.cuda.is_available():
                available_memory_gb = GPUMemoryManager.get_available_memory()
            else:
                available_memory_gb = 15  # ê¸°ë³¸ê°’ (T4 single)

        print(f"ğŸ“Š Optimizing for {available_memory_gb:.1f} GB available memory")

        # T4 GPU Ã— 2 (30GB)
        if available_memory_gb >= 25:
            config = {
                'batch_size': 8,
                'gradient_accumulation_steps': 1,
                'use_gradient_checkpointing': False,
                'compute_dtype': torch.float16,  # âœ… T4 compatible
                'max_seq_length': 1024
            }
            print("  â†’ Using T4Ã—2 configuration (high performance)")

        # T4 GPU Ã— 1.5 (20-25GB)
        elif available_memory_gb >= 18:
            config = {
                'batch_size': 6,
                'gradient_accumulation_steps': 1,
                'use_gradient_checkpointing': True,
                'compute_dtype': torch.float16,  # âœ… T4 compatible
                'max_seq_length': 768
            }
            print("  â†’ Using T4Ã—1.5 configuration (balanced)")

        # T4 GPU Ã— 1 (15GB) - ê¸°ë³¸
        elif available_memory_gb >= 12:
            config = {
                'batch_size': 4,
                'gradient_accumulation_steps': 2,
                'use_gradient_checkpointing': True,
                'compute_dtype': torch.float16,  # âœ… T4 compatible
                'max_seq_length': 512
            }
            print("  â†’ Using T4Ã—1 configuration (standard)")

        # ì €ë©”ëª¨ë¦¬ (< 12GB)
        else:
            config = {
                'batch_size': 2,
                'gradient_accumulation_steps': 4,
                'use_gradient_checkpointing': True,
                'compute_dtype': torch.float16,  # âœ… T4 compatible
                'max_seq_length': 512
            }
            print("  â†’ Using low-memory configuration")
            print("  âš ï¸  Warning: Very limited memory. Consider closing other processes.")

        # ìœ íš¨ ë°°ì¹˜ í¬ê¸°
        effective_batch_size = config['batch_size'] * config['gradient_accumulation_steps']
        print(f"  â†’ Effective batch size: {effective_batch_size}")

        return config

    def monitor_training(self, callback_interval: int = 100):
        """
        í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì½œë°± ìƒì„±

        Args:
            callback_interval: ëª¨ë‹ˆí„°ë§ ê°„ê²© (steps)

        Returns:
            Callable: ì½œë°± í•¨ìˆ˜

        Usage:
            manager = GPUMemoryManager()
            callback = manager.monitor_training(interval=100)

            # í•™ìŠµ ë£¨í”„ ë‚´ì—ì„œ:
            callback(step=current_step)
        """
        def callback(step: int):
            if step % callback_interval == 0:
                stats = self.get_memory_stats()
                self.peak_memory = max(self.peak_memory, stats.get('allocated', 0))

                print(f"\nStep {step}:")
                print(f"  Memory: {stats['allocated']:.2f} GB / {stats['reserved']:.2f} GB")
                print(f"  Peak:   {self.peak_memory:.2f} GB")

                # ë©”ëª¨ë¦¬ ê²½ê³  (T4 15GBì˜ 87%)
                if stats['allocated'] > 13:
                    print("  âš ï¸  WARNING: Memory usage is high!")
                    self.clear_cache()
                    print("  â†’ Cache cleared")

        return callback


def create_memory_efficient_training_args(
    output_dir: str = 'checkpoints',
    available_memory_gb: float = None,
    num_epochs: int = 3,
    learning_rate: float = 2e-5
) -> TrainingArguments:
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ ì„¤ì • ìƒì„±

    Args:
        output_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬
        available_memory_gb: ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ (GB), Noneì´ë©´ ìë™
        num_epochs: í•™ìŠµ ì—í­ ìˆ˜
        learning_rate: í•™ìŠµë¥ 

    Returns:
        TrainingArguments: ë©”ëª¨ë¦¬ ìµœì í™”ëœ í•™ìŠµ ì„¤ì •

    âœ… CRITICAL: T4 í˜¸í™˜ì„±ì„ ìœ„í•´ fp16=True, bf16=False
    """
    # ë©”ëª¨ë¦¬ ê¸°ë°˜ ìµœì  ì„¤ì •
    manager = GPUMemoryManager()
    config = manager.optimize_training_config(available_memory_gb)

    print(f"\n{'='*60}")
    print("Creating Memory-Efficient Training Arguments")
    print(f"{'='*60}\n")

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,

        # Batch size (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        per_device_train_batch_size=config['batch_size'],
        per_device_eval_batch_size=config['batch_size'],
        gradient_accumulation_steps=config['gradient_accumulation_steps'],

        # Memory optimization
        gradient_checkpointing=config['use_gradient_checkpointing'],
        gradient_checkpointing_kwargs={'use_reentrant': False},  # ì•ˆì •ì„±

        # âœ… CRITICAL: T4 í˜¸í™˜ì„±
        fp16=True,              # âœ… Float16 ì‚¬ìš©
        bf16=False,             # âœ… BFloat16 ë¯¸ì‚¬ìš© (T4 unsupported)

        # Optimizer (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        optim="paged_adamw_8bit",

        # Data loading (ë©”ëª¨ë¦¬ ì ˆì•½)
        dataloader_pin_memory=False,
        dataloader_num_workers=2,

        # Gradient clipping
        max_grad_norm=1.0,

        # Learning rate
        learning_rate=learning_rate,
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,

        # Weight decay
        weight_decay=0.01,

        # Label smoothing
        label_smoothing_factor=0.05,

        # Logging
        logging_steps=10,
        logging_first_step=True,

        # Evaluation
        eval_strategy="steps",
        eval_steps=50,

        # Saving
        save_strategy="steps",
        save_steps=100,
        save_total_limit=2,  # ìµœëŒ€ 2ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€

        # Best model
        load_best_model_at_end=True,
        metric_for_best_model="eval_accuracy",

        # WandB
        report_to="wandb",

        # Reproducibility
        seed=42,
        data_seed=42,

        # Disable unnecessary features
        push_to_hub=False,
    )

    print("Training Arguments:")
    print(f"  Batch size: {config['batch_size']}")
    print(f"  Gradient accumulation: {config['gradient_accumulation_steps']}")
    print(f"  Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}")
    print(f"  Gradient checkpointing: {config['use_gradient_checkpointing']}")
    print(f"  Precision: FP16 (T4 compatible)")
    print(f"  Optimizer: paged_adamw_8bit")
    print(f"{'='*60}\n")

    return training_args


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*60)
    print("GPU Memory Optimizer")
    print("="*60 + "\n")

    # GPU ì •ë³´
    if torch.cuda.is_available():
        print(f"CUDA Available: âœ“")
        print(f"Device: {torch.cuda.get_device_name(0)}")
        print(f"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("CUDA Available: âœ—")
        print("Running on CPU")

    # ë©”ëª¨ë¦¬ í†µê³„
    manager = GPUMemoryManager()
    manager.print_memory_stats()

    # ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬
    available = manager.get_available_memory()
    print(f"Available Memory: {available:.2f} GB\n")

    # ìµœì  ì„¤ì •
    config = manager.optimize_training_config()

    print("\nOptimal Configuration:")
    for key, value in config.items():
        print(f"  {key:30s}: {value}")

    # í•™ìŠµ ì„¤ì • ì˜ˆì‹œ
    print("\n" + "="*60)
    print("Example: Creating Training Arguments")
    print("="*60 + "\n")

    training_args = create_memory_efficient_training_args(
        output_dir='checkpoints/example',
        available_memory_gb=15,  # T4 single
        num_epochs=3,
        learning_rate=2e-5
    )

    print("âœ… Training arguments created successfully")


if __name__ == "__main__":
    main()
