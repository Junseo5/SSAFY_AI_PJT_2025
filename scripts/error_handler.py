"""
Kaggle VQA Challenge - Error Handler

This script provides robust error handling for:
- GPU Out of Memory (OOM)
- Tokenization errors (Korean text)
- Model loading failures
- Safe inference with retries
"""

import logging
import torch
import time
import unicodedata
from typing import Callable, Any
from functools import wraps

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class VQAErrorHandler:
    """VQA í”„ë¡œì íŠ¸ ì „ìš© ì—ëŸ¬ í•¸ë“¤ëŸ¬"""

    @staticmethod
    def handle_gpu_oom(func: Callable) -> Callable:
        """
        GPU OOM ìë™ ë³µêµ¬ ë°ì½”ë ˆì´í„°

        Args:
            func: ë˜í•‘í•  í•¨ìˆ˜

        Returns:
            Callable: ë˜í•‘ëœ í•¨ìˆ˜

        Usage:
            @VQAErrorHandler.handle_gpu_oom
            def train_epoch(model, dataloader, batch_size=4):
                ...
        """
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except RuntimeError as e:
                error_msg = str(e).lower()

                if "out of memory" in error_msg or "oom" in error_msg:
                    logger.warning("âš ï¸  GPU OOM detected. Clearing cache...")

                    # GPU ìºì‹œ ì •ë¦¬
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

                    # Batch size ì¤„ì´ê¸° ì‹œë„
                    if 'batch_size' in kwargs and kwargs['batch_size'] > 1:
                        new_batch_size = max(1, kwargs['batch_size'] // 2)
                        logger.info(
                            f"ğŸ”„ Reducing batch_size: {kwargs['batch_size']} â†’ {new_batch_size}"
                        )
                        kwargs['batch_size'] = new_batch_size

                        # ì¬ì‹œë„
                        try:
                            return func(*args, **kwargs)
                        except RuntimeError as e2:
                            logger.error(f"âŒ Retry failed even with reduced batch size: {e2}")
                            raise
                    else:
                        logger.error("âŒ Cannot reduce batch_size further")
                        raise
                else:
                    # ë‹¤ë¥¸ RuntimeErrorëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
                    raise

        return wrapper

    @staticmethod
    def handle_tokenization_error(func: Callable) -> Callable:
        """
        í•œê¸€ í† í°í™” ì˜¤ë¥˜ ë°©ì§€ ë°ì½”ë ˆì´í„°

        Args:
            func: ë˜í•‘í•  í•¨ìˆ˜

        Returns:
            Callable: ë˜í•‘ëœ í•¨ìˆ˜

        Usage:
            @VQAErrorHandler.handle_tokenization_error
            def tokenize_text(text):
                ...
        """
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                error_msg = str(e).lower()

                if "encode" in error_msg or "token" in error_msg or "unicode" in error_msg:
                    logger.error(f"âŒ Tokenization failed: {e}")

                    # í…ìŠ¤íŠ¸ ì •ê·œí™” ì¬ì‹œë„
                    if 'text' in kwargs:
                        logger.info("ğŸ”„ Retrying with normalized text...")

                        # NFKC ì •ê·œí™”
                        kwargs['text'] = unicodedata.normalize('NFKC', kwargs['text'])

                        # ì œì–´ ë¬¸ì ì œê±°
                        import re
                        kwargs['text'] = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', kwargs['text'])

                        try:
                            return func(*args, **kwargs)
                        except Exception as e2:
                            logger.error(f"âŒ Retry failed with normalized text: {e2}")
                            raise
                    else:
                        raise
                else:
                    raise

        return wrapper

    @staticmethod
    def handle_model_load_error(func: Callable) -> Callable:
        """
        ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì²˜ë¦¬ ë°ì½”ë ˆì´í„°

        Args:
            func: ë˜í•‘í•  í•¨ìˆ˜

        Returns:
            Callable: ë˜í•‘ëœ í•¨ìˆ˜

        Usage:
            @VQAErrorHandler.handle_model_load_error
            def load_model(model_path):
                ...
        """
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                error_msg = str(e).lower()

                if "load" in error_msg or "checkpoint" in error_msg or "state_dict" in error_msg:
                    logger.error(f"âŒ Model loading failed: {e}")

                    # ë°±ì—… ì²´í¬í¬ì¸íŠ¸ ì‹œë„
                    if 'model_path' in kwargs:
                        original_path = kwargs['model_path']

                        # fold ë²ˆí˜¸ë¥¼ ë°”ê¿”ì„œ ì‹œë„
                        backup_paths = []
                        for fold in range(3):
                            backup_path = original_path.replace('fold0', f'fold{fold}')
                            if backup_path != original_path:
                                backup_paths.append(backup_path)

                        for backup_path in backup_paths:
                            logger.info(f"ğŸ”„ Trying backup checkpoint: {backup_path}")
                            kwargs['model_path'] = backup_path

                            try:
                                return func(*args, **kwargs)
                            except Exception:
                                continue

                        logger.error("âŒ All backup checkpoints failed")

                    raise
                else:
                    raise

        return wrapper

    @staticmethod
    def safe_inference(
        predictor,
        image_path: str,
        question: str,
        choices: dict,
        max_retries: int = 3
    ) -> dict:
        """
        ì•ˆì „í•œ ì¶”ë¡  (ì¬ì‹œë„ í¬í•¨)

        Args:
            predictor: VQAPredictor ì¸ìŠ¤í„´ìŠ¤
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            question: ì§ˆë¬¸
            choices: ì„ íƒì§€
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            dict: {
                'prediction': str,
                'confidence': float
            }
        """
        for attempt in range(max_retries):
            try:
                result = predictor.predict(image_path, question, choices)

                # ê²°ê³¼ ê²€ì¦
                if 'prediction' not in result:
                    raise ValueError("No prediction in result")

                if result['prediction'] not in ['a', 'b', 'c', 'd']:
                    raise ValueError(f"Invalid prediction: {result['prediction']}")

                return result

            except Exception as e:
                logger.warning(
                    f"âš ï¸  Inference attempt {attempt+1}/{max_retries} failed: {e}"
                )

                if attempt == max_retries - 1:
                    logger.error(f"âŒ All retries failed for image: {image_path}")

                    # í´ë°±: ë¬´ì‘ìœ„ ë‹µ ë°˜í™˜
                    import random
                    fallback = random.choice(['a', 'b', 'c', 'd'])
                    logger.warning(f"ğŸ² Using fallback prediction: {fallback}")

                    return {
                        'prediction': fallback,
                        'confidence': 0.0,
                        'fallback': True
                    }

                # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                time.sleep(1 * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„

        # ë„ë‹¬í•˜ì§€ ì•ŠìŒ
        raise RuntimeError("Unexpected: safe_inference reached end without return")

    @staticmethod
    def clear_gpu_cache():
        """GPU ìºì‹œ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            import gc
            gc.collect()
            logger.info("âœ“ GPU cache cleared")

    @staticmethod
    def get_gpu_memory_stats():
        """GPU ë©”ëª¨ë¦¬ í†µê³„"""
        if not torch.cuda.is_available():
            return {}

        stats = {
            'allocated_gb': torch.cuda.memory_allocated() / 1e9,
            'reserved_gb': torch.cuda.memory_reserved() / 1e9,
            'max_allocated_gb': torch.cuda.max_memory_allocated() / 1e9
        }

        return stats

    @staticmethod
    def print_gpu_memory_stats():
        """GPU ë©”ëª¨ë¦¬ í†µê³„ ì¶œë ¥"""
        stats = VQAErrorHandler.get_gpu_memory_stats()

        if stats:
            print(f"\n{'â”€'*60}")
            print("GPU Memory Stats:")
            print(f"{'â”€'*60}")
            print(f"  Allocated: {stats['allocated_gb']:.2f} GB")
            print(f"  Reserved:  {stats['reserved_gb']:.2f} GB")
            print(f"  Max Used:  {stats['max_allocated_gb']:.2f} GB")
            print(f"{'â”€'*60}\n")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë°ì½”ë ˆì´í„° ì‚¬ìš© ì˜ˆì‹œ

    @VQAErrorHandler.handle_gpu_oom
    @VQAErrorHandler.handle_tokenization_error
    def example_training_step(model, batch, batch_size=4):
        """ì˜ˆì‹œ í•™ìŠµ ìŠ¤í…"""
        # í•™ìŠµ ì½”ë“œ
        print(f"Training with batch_size={batch_size}")
        return "success"

    # í…ŒìŠ¤íŠ¸
    print("="*60)
    print("VQA Error Handler - Examples")
    print("="*60)

    # GPU ë©”ëª¨ë¦¬ í†µê³„
    VQAErrorHandler.print_gpu_memory_stats()

    # GPU ìºì‹œ ì •ë¦¬
    VQAErrorHandler.clear_gpu_cache()

    print("\nâœ“ Error handler loaded successfully")
    print("Usage: Decorate functions with @VQAErrorHandler.handle_gpu_oom, etc.")
