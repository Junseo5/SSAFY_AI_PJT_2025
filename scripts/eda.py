"""
Kaggle VQA Challenge - Exploratory Data Analysis (EDA)

This script performs comprehensive EDA on the VQA dataset including:
- Question type classification
- Answer format analysis
- Distribution visualization
- Data quality checks
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re  # âœ… CRITICAL FIX: Added missing import
from pathlib import Path
from typing import Dict, Tuple
import warnings

warnings.filterwarnings('ignore')


class VQADataAnalyzer:
    """VQA ë°ì´í„° ë¶„ì„ê¸°"""

    def __init__(self, train_csv_path: str = 'data/train.csv'):
        """
        Args:
            train_csv_path: í•™ìŠµ ë°ì´í„° CSV ê²½ë¡œ
        """
        self.train_csv_path = train_csv_path
        self.df = None
        self.type_patterns = {
            'counting': r'ëª‡|ê°œìˆ˜|ìˆ˜|how many|count',
            'color': r'ìƒ‰|ìƒ‰ê¹”|color|ë¬´ìŠ¨ìƒ‰',
            'ocr': r'ê¸€ì|ë¬¸ì|ìˆ«ì|ë²ˆí˜¸|ì½|text|number|write|written',
            'yesno': r'ì¸ê°€|ì…ë‹ˆê¹Œ|\?$|ìˆëŠ”ê°€|ë§ëŠ”ê°€|yes|no',
            'location': r'ì–´ë””|ìœ„ì¹˜|where|ì¥ì†Œ|place',
            'attribute': r'ë¬´ì—‡|what|ì–´ë–¤|kind|which'
        }

    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“ Loading data from {self.train_csv_path}...")
        self.df = pd.read_csv(self.train_csv_path)
        print(f"âœ“ Loaded {len(self.df)} samples")
        print(f"\nColumns: {list(self.df.columns)}")
        print(f"\nFirst 3 rows:")
        print(self.df.head(3))
        return self.df

    def analyze_question_types(self) -> Dict[str, int]:
        """
        ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ë° ë¶„í¬ ë¶„ì„

        Returns:
            dict: {question_type: count}
        """
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("ğŸ“Š Question Type Analysis")
        print("="*60)

        def classify_question(question: str) -> str:
            """ì§ˆë¬¸ì„ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜"""
            if pd.isna(question):
                return 'unknown'

            question_lower = question.lower()

            for qtype, pattern in self.type_patterns.items():
                if re.search(pattern, question_lower, re.I):
                    return qtype
            return 'general'

        self.df['question_type'] = self.df['question'].apply(classify_question)

        type_counts = self.df['question_type'].value_counts().to_dict()

        print("\nQuestion Type Distribution:")
        for qtype, count in sorted(type_counts.items(), key=lambda x: -x[1]):
            percentage = count / len(self.df) * 100
            print(f"  {qtype:12s}: {count:4d} ({percentage:5.1f}%)")

        return type_counts

    def analyze_answer_format(self) -> Dict[str, int]:
        """
        ë³´ê¸° í˜•ì‹ ë¶„ì„

        Returns:
            dict: {format_type: count}
        """
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("ğŸ“ Answer Format Analysis")
        print("="*60)

        def classify_format(row) -> str:
            """ë³´ê¸° í˜•ì‹ ë¶„ë¥˜"""
            # 4ê°œì˜ ë³´ê¸°ë¥¼ ê²°í•©
            choices = f"{row['a']} {row['b']} {row['c']} {row['d']}"

            if pd.isna(choices):
                return 'unknown'

            # í•œê¸€ë§Œ
            if re.match(r'^[ê°€-í£\s]+$', choices):
                return 'pure_korean'
            # ì˜ì–´ë§Œ
            elif re.match(r'^[a-zA-Z\s]+$', choices):
                return 'pure_english'
            # ìˆ«ì í¬í•¨
            elif re.search(r'\d', choices):
                return 'numeric'
            else:
                return 'mixed'

        self.df['answer_format'] = self.df.apply(classify_format, axis=1)

        format_counts = self.df['answer_format'].value_counts().to_dict()

        print("\nAnswer Format Distribution:")
        for fmt, count in sorted(format_counts.items(), key=lambda x: -x[1]):
            percentage = count / len(self.df) * 100
            print(f"  {fmt:15s}: {count:4d} ({percentage:5.1f}%)")

        return format_counts

    def analyze_answer_distribution(self) -> Dict[str, int]:
        """
        ì •ë‹µ ë¶„í¬ ë¶„ì„ (a/b/c/d)

        Returns:
            dict: {answer: count}
        """
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("ğŸ¯ Answer Label Distribution")
        print("="*60)

        answer_counts = self.df['answer'].value_counts().to_dict()

        print("\nAnswer Distribution:")
        for ans, count in sorted(answer_counts.items()):
            percentage = count / len(self.df) * 100
            print(f"  {ans}: {count:4d} ({percentage:5.1f}%)")

        # ê· í˜• ì²´í¬
        expected = len(self.df) / 4
        max_deviation = max(abs(count - expected) for count in answer_counts.values())
        if max_deviation / expected > 0.2:
            print(f"\nâš ï¸  Warning: Answer distribution is imbalanced (max deviation: {max_deviation/expected*100:.1f}%)")
        else:
            print(f"\nâœ“ Answer distribution is balanced")

        return answer_counts

    def analyze_text_lengths(self) -> Tuple[Dict, Dict]:
        """
        í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„

        Returns:
            tuple: (question_lengths, choice_lengths)
        """
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("ğŸ“ Text Length Analysis")
        print("="*60)

        # ì§ˆë¬¸ ê¸¸ì´
        self.df['question_length'] = self.df['question'].str.len()

        q_stats = {
            'min': self.df['question_length'].min(),
            'max': self.df['question_length'].max(),
            'mean': self.df['question_length'].mean(),
            'median': self.df['question_length'].median()
        }

        print(f"\nQuestion Length Statistics:")
        print(f"  Min:    {q_stats['min']:.0f}")
        print(f"  Max:    {q_stats['max']:.0f}")
        print(f"  Mean:   {q_stats['mean']:.1f}")
        print(f"  Median: {q_stats['median']:.0f}")

        # ë³´ê¸° ê¸¸ì´
        self.df['choice_a_length'] = self.df['a'].str.len()
        self.df['choice_b_length'] = self.df['b'].str.len()
        self.df['choice_c_length'] = self.df['c'].str.len()
        self.df['choice_d_length'] = self.df['d'].str.len()

        avg_choice_length = (
            self.df['choice_a_length'] +
            self.df['choice_b_length'] +
            self.df['choice_c_length'] +
            self.df['choice_d_length']
        ) / 4

        c_stats = {
            'min': avg_choice_length.min(),
            'max': avg_choice_length.max(),
            'mean': avg_choice_length.mean(),
            'median': avg_choice_length.median()
        }

        print(f"\nAverage Choice Length Statistics:")
        print(f"  Min:    {c_stats['min']:.1f}")
        print(f"  Max:    {c_stats['max']:.1f}")
        print(f"  Mean:   {c_stats['mean']:.1f}")
        print(f"  Median: {c_stats['median']:.1f}")

        return q_stats, c_stats

    def visualize_distribution(self, output_dir: str = 'outputs'):
        """
        ë¶„í¬ ì‹œê°í™”

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        if self.df is None or 'question_type' not in self.df.columns:
            self.analyze_question_types()

        if 'answer_format' not in self.df.columns:
            self.analyze_answer_format()

        if 'question_length' not in self.df.columns:
            self.analyze_text_lengths()

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(exist_ok=True, parents=True)

        print(f"\nğŸ“Š Creating visualizations...")

        # 4ê°œ ì„œë¸Œí”Œë¡¯
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('VQA Dataset Analysis', fontsize=16, fontweight='bold')

        # 1. ì§ˆë¬¸ ìœ í˜• ë¶„í¬
        type_counts = self.df['question_type'].value_counts()
        axes[0, 0].bar(range(len(type_counts)), type_counts.values, color='skyblue', edgecolor='black')
        axes[0, 0].set_xticks(range(len(type_counts)))
        axes[0, 0].set_xticklabels(type_counts.index, rotation=45, ha='right')
        axes[0, 0].set_title('Question Type Distribution')
        axes[0, 0].set_ylabel('Count')
        axes[0, 0].grid(axis='y', alpha=0.3)

        # 2. ì •ë‹µ ë¶„í¬
        answer_counts = self.df['answer'].value_counts().sort_index()
        axes[0, 1].bar(answer_counts.index, answer_counts.values, color='lightcoral', edgecolor='black')
        axes[0, 1].set_title('Answer Distribution')
        axes[0, 1].set_xlabel('Answer')
        axes[0, 1].set_ylabel('Count')
        axes[0, 1].grid(axis='y', alpha=0.3)

        # 3. ì§ˆë¬¸ ê¸¸ì´ ë¶„í¬
        axes[1, 0].hist(self.df['question_length'], bins=30, color='lightgreen', edgecolor='black')
        axes[1, 0].set_title('Question Length Distribution')
        axes[1, 0].set_xlabel('Length (characters)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].axvline(self.df['question_length'].mean(), color='red', linestyle='--', linewidth=2, label=f"Mean: {self.df['question_length'].mean():.1f}")
        axes[1, 0].legend()
        axes[1, 0].grid(axis='y', alpha=0.3)

        # 4. ë³´ê¸° í˜•ì‹ ë¶„í¬
        format_counts = self.df['answer_format'].value_counts()
        axes[1, 1].bar(range(len(format_counts)), format_counts.values, color='plum', edgecolor='black')
        axes[1, 1].set_xticks(range(len(format_counts)))
        axes[1, 1].set_xticklabels(format_counts.index, rotation=45, ha='right')
        axes[1, 1].set_title('Answer Format Distribution')
        axes[1, 1].set_ylabel('Count')
        axes[1, 1].grid(axis='y', alpha=0.3)

        plt.tight_layout()

        output_path = Path(output_dir) / 'eda_distribution.png'
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"âœ“ Saved visualization to {output_path}")
        plt.close()

    def check_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ ì²´í¬"""
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("ğŸ” Data Quality Check")
        print("="*60)

        # ê²°ì¸¡ì¹˜ í™•ì¸
        print("\nMissing Values:")
        missing = self.df.isnull().sum()
        if missing.sum() == 0:
            print("  âœ“ No missing values")
        else:
            for col, count in missing[missing > 0].items():
                print(f"  âš ï¸  {col}: {count} ({count/len(self.df)*100:.1f}%)")

        # ì¤‘ë³µ í™•ì¸
        print("\nDuplicate Rows:")
        duplicates = self.df.duplicated().sum()
        if duplicates == 0:
            print("  âœ“ No duplicate rows")
        else:
            print(f"  âš ï¸  {duplicates} duplicate rows found")

        # ID ì¤‘ë³µ í™•ì¸
        if 'id' in self.df.columns:
            print("\nID Duplicates:")
            id_duplicates = self.df['id'].duplicated().sum()
            if id_duplicates == 0:
                print("  âœ“ No duplicate IDs")
            else:
                print(f"  âš ï¸  {id_duplicates} duplicate IDs found")

        # ì •ë‹µ ìœ íš¨ì„± í™•ì¸
        print("\nAnswer Validity:")
        valid_answers = self.df['answer'].isin(['a', 'b', 'c', 'd']).all()
        if valid_answers:
            print("  âœ“ All answers are valid (a/b/c/d)")
        else:
            invalid_count = (~self.df['answer'].isin(['a', 'b', 'c', 'd'])).sum()
            print(f"  âš ï¸  {invalid_count} invalid answers found")
            print(f"  Invalid values: {self.df[~self.df['answer'].isin(['a', 'b', 'c', 'd'])]['answer'].unique()}")

    def generate_summary_report(self, output_dir: str = 'outputs'):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        if self.df is None:
            self.load_data()

        output_path = Path(output_dir) / 'eda_summary.txt'

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("VQA Dataset Summary Report\n")
            f.write("="*60 + "\n\n")

            f.write(f"Total Samples: {len(self.df)}\n\n")

            # ì§ˆë¬¸ ìœ í˜•
            if 'question_type' in self.df.columns:
                f.write("Question Type Distribution:\n")
                for qtype, count in self.df['question_type'].value_counts().items():
                    f.write(f"  {qtype:12s}: {count:4d} ({count/len(self.df)*100:5.1f}%)\n")
                f.write("\n")

            # ì •ë‹µ ë¶„í¬
            f.write("Answer Distribution:\n")
            for ans, count in sorted(self.df['answer'].value_counts().items()):
                f.write(f"  {ans}: {count:4d} ({count/len(self.df)*100:5.1f}%)\n")
            f.write("\n")

            # í…ìŠ¤íŠ¸ ê¸¸ì´
            if 'question_length' in self.df.columns:
                f.write("Text Length Statistics:\n")
                f.write(f"  Question Length (mean): {self.df['question_length'].mean():.1f}\n")
                f.write(f"  Question Length (range): {self.df['question_length'].min():.0f} - {self.df['question_length'].max():.0f}\n")
                f.write("\n")

        print(f"\nâœ“ Saved summary report to {output_path}")

    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("\n" + "="*60)
        print("ğŸš€ Running Full VQA Data Analysis")
        print("="*60 + "\n")

        # 1. ë°ì´í„° ë¡œë“œ
        self.load_data()

        # 2. ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        self.analyze_question_types()

        # 3. ë‹µë³€ í˜•ì‹ ë¶„ì„
        self.analyze_answer_format()

        # 4. ì •ë‹µ ë¶„í¬ ë¶„ì„
        self.analyze_answer_distribution()

        # 5. í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
        self.analyze_text_lengths()

        # 6. ë°ì´í„° í’ˆì§ˆ ì²´í¬
        self.check_data_quality()

        # 7. ì‹œê°í™”
        self.visualize_distribution()

        # 8. ìš”ì•½ ë¦¬í¬íŠ¸
        self.generate_summary_report()

        print("\n" + "="*60)
        print("âœ… EDA Complete!")
        print("="*60)
        print("\nGenerated files:")
        print("  - outputs/eda_distribution.png")
        print("  - outputs/eda_summary.txt")
        print("\nEnhanced DataFrame saved with columns:")
        print("  - question_type")
        print("  - answer_format")
        print("  - question_length")

        return self.df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description='VQA Dataset EDA')
    parser.add_argument('--train_csv', default='data/train.csv', help='Path to train.csv')
    parser.add_argument('--output_dir', default='outputs', help='Output directory')

    args = parser.parse_args()

    # ë¶„ì„ ì‹¤í–‰
    analyzer = VQADataAnalyzer(train_csv_path=args.train_csv)
    df_enhanced = analyzer.run_full_analysis()

    # ê°•í™”ëœ DataFrame ì €ì¥ (ì„ íƒì‚¬í•­)
    enhanced_path = 'data/train_with_types.csv'
    df_enhanced.to_csv(enhanced_path, index=False)
    print(f"\nâœ“ Enhanced DataFrame saved to {enhanced_path}")


if __name__ == "__main__":
    main()
