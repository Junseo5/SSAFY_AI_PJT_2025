"""
Kaggle VQA Challenge - Exploratory Data Analysis (EDA)

This script performs comprehensive EDA on the VQA dataset including:
- Question type classification
- Answer format analysis
- Distribution visualization
- Data quality checks
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re  # ‚úÖ CRITICAL FIX: Added missing import
from pathlib import Path
from typing import Dict, Tuple
import warnings

warnings.filterwarnings('ignore')


class VQADataAnalyzer:
    """VQA Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÍ∏∞"""

    def __init__(self, train_csv_path: str = 'data/train.csv'):
        """
        Args:
            train_csv_path: ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ CSV Í≤ΩÎ°ú
        """
        self.train_csv_path = train_csv_path
        self.df = None
        self.type_patterns = {
            'counting': r'Î™á|Í∞úÏàò|Ïàò|how many|count',
            'color': r'ÏÉâ|ÏÉâÍπî|color|Î¨¥Ïä®ÏÉâ',
            'ocr': r'Í∏ÄÏûê|Î¨∏Ïûê|Ïà´Ïûê|Î≤àÌò∏|ÏùΩ|text|number|write|written',
            'yesno': r'Ïù∏Í∞Ä|ÏûÖÎãàÍπå|\?$|ÏûàÎäîÍ∞Ä|ÎßûÎäîÍ∞Ä|yes|no',
            'location': r'Ïñ¥Îîî|ÏúÑÏπò|where|Ïû•ÏÜå|place',
            'attribute': r'Î¨¥Ïóá|what|Ïñ¥Îñ§|kind|which'
        }

    def load_data(self) -> pd.DataFrame:
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        print(f"üìÅ Loading data from {self.train_csv_path}...")
        self.df = pd.read_csv(self.train_csv_path)
        print(f"‚úì Loaded {len(self.df)} samples")
        print(f"\nColumns: {list(self.df.columns)}")
        print(f"\nFirst 3 rows:")
        print(self.df.head(3))
        return self.df

    def analyze_question_types(self) -> Dict[str, int]:
        """
        ÏßàÎ¨∏ Ïú†Ìòï Î∂ÑÎ•ò Î∞è Î∂ÑÌè¨ Î∂ÑÏÑù

        Returns:
            dict: {question_type: count}
        """
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("üìä Question Type Analysis")
        print("="*60)

        def classify_question(question: str) -> str:
            """ÏßàÎ¨∏ÏùÑ Ïú†ÌòïÎ≥ÑÎ°ú Î∂ÑÎ•ò"""
            if pd.isna(question):
                return 'unknown'

            question_lower = question.lower()

            for qtype, pattern in self.type_patterns.items():
                if re.search(pattern, question_lower, re.I):
                    return qtype
            return 'general'

        self.df['question_type'] = self.df['question'].apply(classify_question)

        type_counts = self.df['question_type'].value_counts().to_dict()

        print("\nQuestion Type Distribution:")
        for qtype, count in sorted(type_counts.items(), key=lambda x: -x[1]):
            percentage = count / len(self.df) * 100
            print(f"  {qtype:12s}: {count:4d} ({percentage:5.1f}%)")

        return type_counts

    def analyze_answer_format(self) -> Dict[str, int]:
        """
        Î≥¥Í∏∞ ÌòïÏãù Î∂ÑÏÑù

        Returns:
            dict: {format_type: count}
        """
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("üìù Answer Format Analysis")
        print("="*60)

        def classify_format(row) -> str:
            """Î≥¥Í∏∞ ÌòïÏãù Î∂ÑÎ•ò"""
            # 4Í∞úÏùò Î≥¥Í∏∞Î•º Í≤∞Ìï©
            choices = f"{row['a']} {row['b']} {row['c']} {row['d']}"

            if pd.isna(choices):
                return 'unknown'

            # ÌïúÍ∏ÄÎßå
            if re.match(r'^[Í∞Ä-Ìû£\s]+$', choices):
                return 'pure_korean'
            # ÏòÅÏñ¥Îßå
            elif re.match(r'^[a-zA-Z\s]+$', choices):
                return 'pure_english'
            # Ïà´Ïûê Ìè¨Ìï®
            elif re.search(r'\d', choices):
                return 'numeric'
            else:
                return 'mixed'

        self.df['answer_format'] = self.df.apply(classify_format, axis=1)

        format_counts = self.df['answer_format'].value_counts().to_dict()

        print("\nAnswer Format Distribution:")
        for fmt, count in sorted(format_counts.items(), key=lambda x: -x[1]):
            percentage = count / len(self.df) * 100
            print(f"  {fmt:15s}: {count:4d} ({percentage:5.1f}%)")

        return format_counts

    def analyze_answer_distribution(self) -> Dict[str, int]:
        """
        Ï†ïÎãµ Î∂ÑÌè¨ Î∂ÑÏÑù (a/b/c/d)

        Returns:
            dict: {answer: count}
        """
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("üéØ Answer Label Distribution")
        print("="*60)

        answer_counts = self.df['answer'].value_counts().to_dict()

        print("\nAnswer Distribution:")
        for ans, count in sorted(answer_counts.items()):
            percentage = count / len(self.df) * 100
            print(f"  {ans}: {count:4d} ({percentage:5.1f}%)")

        # Í∑†Ìòï Ï≤¥ÌÅ¨
        expected = len(self.df) / 4
        max_deviation = max(abs(count - expected) for count in answer_counts.values())
        if max_deviation / expected > 0.2:
            print(f"\n‚ö†Ô∏è  Warning: Answer distribution is imbalanced (max deviation: {max_deviation/expected*100:.1f}%)")
        else:
            print(f"\n‚úì Answer distribution is balanced")

        return answer_counts

    def analyze_text_lengths(self) -> Tuple[Dict, Dict]:
        """
        ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Î∂ÑÏÑù

        Returns:
            tuple: (question_lengths, choice_lengths)
        """
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("üìè Text Length Analysis")
        print("="*60)

        # ÏßàÎ¨∏ Í∏∏Ïù¥
        self.df['question_length'] = self.df['question'].str.len()

        q_stats = {
            'min': self.df['question_length'].min(),
            'max': self.df['question_length'].max(),
            'mean': self.df['question_length'].mean(),
            'median': self.df['question_length'].median()
        }

        print(f"\nQuestion Length Statistics:")
        print(f"  Min:    {q_stats['min']:.0f}")
        print(f"  Max:    {q_stats['max']:.0f}")
        print(f"  Mean:   {q_stats['mean']:.1f}")
        print(f"  Median: {q_stats['median']:.0f}")

        # Î≥¥Í∏∞ Í∏∏Ïù¥
        self.df['choice_a_length'] = self.df['a'].str.len()
        self.df['choice_b_length'] = self.df['b'].str.len()
        self.df['choice_c_length'] = self.df['c'].str.len()
        self.df['choice_d_length'] = self.df['d'].str.len()

        avg_choice_length = (
            self.df['choice_a_length'] +
            self.df['choice_b_length'] +
            self.df['choice_c_length'] +
            self.df['choice_d_length']
        ) / 4

        c_stats = {
            'min': avg_choice_length.min(),
            'max': avg_choice_length.max(),
            'mean': avg_choice_length.mean(),
            'median': avg_choice_length.median()
        }

        print(f"\nAverage Choice Length Statistics:")
        print(f"  Min:    {c_stats['min']:.1f}")
        print(f"  Max:    {c_stats['max']:.1f}")
        print(f"  Mean:   {c_stats['mean']:.1f}")
        print(f"  Median: {c_stats['median']:.1f}")

        return q_stats, c_stats

    def visualize_distribution(self, output_dir: str = 'outputs'):
        """
        Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî

        Args:
            output_dir: Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨
        """
        if self.df is None or 'question_type' not in self.df.columns:
            self.analyze_question_types()

        if 'answer_format' not in self.df.columns:
            self.analyze_answer_format()

        if 'question_length' not in self.df.columns:
            self.analyze_text_lengths()

        # Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        Path(output_dir).mkdir(exist_ok=True, parents=True)

        print(f"\nüìä Creating visualizations...")

        # 4Í∞ú ÏÑúÎ∏åÌîåÎ°Ø
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('VQA Dataset Analysis', fontsize=16, fontweight='bold')

        # 1. ÏßàÎ¨∏ Ïú†Ìòï Î∂ÑÌè¨
        type_counts = self.df['question_type'].value_counts()
        axes[0, 0].bar(range(len(type_counts)), type_counts.values, color='skyblue', edgecolor='black')
        axes[0, 0].set_xticks(range(len(type_counts)))
        axes[0, 0].set_xticklabels(type_counts.index, rotation=45, ha='right')
        axes[0, 0].set_title('Question Type Distribution')
        axes[0, 0].set_ylabel('Count')
        axes[0, 0].grid(axis='y', alpha=0.3)

        # 2. Ï†ïÎãµ Î∂ÑÌè¨
        answer_counts = self.df['answer'].value_counts().sort_index()
        axes[0, 1].bar(answer_counts.index, answer_counts.values, color='lightcoral', edgecolor='black')
        axes[0, 1].set_title('Answer Distribution')
        axes[0, 1].set_xlabel('Answer')
        axes[0, 1].set_ylabel('Count')
        axes[0, 1].grid(axis='y', alpha=0.3)

        # 3. ÏßàÎ¨∏ Í∏∏Ïù¥ Î∂ÑÌè¨
        axes[1, 0].hist(self.df['question_length'], bins=30, color='lightgreen', edgecolor='black')
        axes[1, 0].set_title('Question Length Distribution')
        axes[1, 0].set_xlabel('Length (characters)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].axvline(self.df['question_length'].mean(), color='red', linestyle='--', linewidth=2, label=f"Mean: {self.df['question_length'].mean():.1f}")
        axes[1, 0].legend()
        axes[1, 0].grid(axis='y', alpha=0.3)

        # 4. Î≥¥Í∏∞ ÌòïÏãù Î∂ÑÌè¨
        format_counts = self.df['answer_format'].value_counts()
        axes[1, 1].bar(range(len(format_counts)), format_counts.values, color='plum', edgecolor='black')
        axes[1, 1].set_xticks(range(len(format_counts)))
        axes[1, 1].set_xticklabels(format_counts.index, rotation=45, ha='right')
        axes[1, 1].set_title('Answer Format Distribution')
        axes[1, 1].set_ylabel('Count')
        axes[1, 1].grid(axis='y', alpha=0.3)

        plt.tight_layout()

        output_path = Path(output_dir) / 'eda_distribution.png'
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"‚úì Saved visualization to {output_path}")
        plt.close()

    def check_data_quality(self):
        """Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Ï≤¥ÌÅ¨"""
        if self.df is None:
            self.load_data()

        print("\n" + "="*60)
        print("üîç Data Quality Check")
        print("="*60)

        # Í≤∞Ï∏°Ïπò ÌôïÏù∏
        print("\nMissing Values:")
        missing = self.df.isnull().sum()
        if missing.sum() == 0:
            print("  ‚úì No missing values")
        else:
            for col, count in missing[missing > 0].items():
                print(f"  ‚ö†Ô∏è  {col}: {count} ({count/len(self.df)*100:.1f}%)")

        # Ï§ëÎ≥µ ÌôïÏù∏
        print("\nDuplicate Rows:")
        duplicates = self.df.duplicated().sum()
        if duplicates == 0:
            print("  ‚úì No duplicate rows")
        else:
            print(f"  ‚ö†Ô∏è  {duplicates} duplicate rows found")

        # ID Ï§ëÎ≥µ ÌôïÏù∏
        if 'id' in self.df.columns:
            print("\nID Duplicates:")
            id_duplicates = self.df['id'].duplicated().sum()
            if id_duplicates == 0:
                print("  ‚úì No duplicate IDs")
            else:
                print(f"  ‚ö†Ô∏è  {id_duplicates} duplicate IDs found")

        # Ï†ïÎãµ Ïú†Ìö®ÏÑ± ÌôïÏù∏
        print("\nAnswer Validity:")
        valid_answers = self.df['answer'].isin(['a', 'b', 'c', 'd']).all()
        if valid_answers:
            print("  ‚úì All answers are valid (a/b/c/d)")
        else:
            invalid_count = (~self.df['answer'].isin(['a', 'b', 'c', 'd'])).sum()
            print(f"  ‚ö†Ô∏è  {invalid_count} invalid answers found")
            print(f"  Invalid values: {self.df[~self.df['answer'].isin(['a', 'b', 'c', 'd'])]['answer'].unique()}")

    def generate_summary_report(self, output_dir: str = 'outputs'):
        """ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±"""
        if self.df is None:
            self.load_data()

        output_path = Path(output_dir) / 'eda_summary.txt'

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("VQA Dataset Summary Report\n")
            f.write("="*60 + "\n\n")

            f.write(f"Total Samples: {len(self.df)}\n\n")

            # ÏßàÎ¨∏ Ïú†Ìòï
            if 'question_type' in self.df.columns:
                f.write("Question Type Distribution:\n")
                for qtype, count in self.df['question_type'].value_counts().items():
                    f.write(f"  {qtype:12s}: {count:4d} ({count/len(self.df)*100:5.1f}%)\n")
                f.write("\n")

            # Ï†ïÎãµ Î∂ÑÌè¨
            f.write("Answer Distribution:\n")
            for ans, count in sorted(self.df['answer'].value_counts().items()):
                f.write(f"  {ans}: {count:4d} ({count/len(self.df)*100:5.1f}%)\n")
            f.write("\n")

            # ÌÖçÏä§Ìä∏ Í∏∏Ïù¥
            if 'question_length' in self.df.columns:
                f.write("Text Length Statistics:\n")
                f.write(f"  Question Length (mean): {self.df['question_length'].mean():.1f}\n")
                f.write(f"  Question Length (range): {self.df['question_length'].min():.0f} - {self.df['question_length'].max():.0f}\n")
                f.write("\n")

        print(f"\n‚úì Saved summary report to {output_path}")

    def run_full_analysis(self):
        """Ï†ÑÏ≤¥ Î∂ÑÏÑù Ïã§Ìñâ"""
        print("\n" + "="*60)
        print("üöÄ Running Full VQA Data Analysis")
        print("="*60 + "\n")

        # 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        self.load_data()

        # 2. ÏßàÎ¨∏ Ïú†Ìòï Î∂ÑÏÑù
        self.analyze_question_types()

        # 3. ÎãµÎ≥Ä ÌòïÏãù Î∂ÑÏÑù
        self.analyze_answer_format()

        # 4. Ï†ïÎãµ Î∂ÑÌè¨ Î∂ÑÏÑù
        self.analyze_answer_distribution()

        # 5. ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Î∂ÑÏÑù
        self.analyze_text_lengths()

        # 6. Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Ï≤¥ÌÅ¨
        self.check_data_quality()

        # 7. ÏãúÍ∞ÅÌôî
        self.visualize_distribution()

        # 8. ÏöîÏïΩ Î¶¨Ìè¨Ìä∏
        self.generate_summary_report()

        print("\n" + "="*60)
        print("‚úÖ EDA Complete!")
        print("="*60)
        print("\nGenerated files:")
        print("  - outputs/eda_distribution.png")
        print("  - outputs/eda_summary.txt")
        print("\nEnhanced DataFrame saved with columns:")
        print("  - question_type")
        print("  - answer_format")
        print("  - question_length")

        return self.df


def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    import argparse

    parser = argparse.ArgumentParser(description='VQA Dataset EDA')
    parser.add_argument('--train_csv', default='data/train.csv', help='Path to train.csv')
    parser.add_argument('--output_dir', default='outputs', help='Output directory')

    args = parser.parse_args()

    # Î∂ÑÏÑù Ïã§Ìñâ
    analyzer = VQADataAnalyzer(train_csv_path=args.train_csv)
    df_enhanced = analyzer.run_full_analysis()

    # Í∞ïÌôîÎêú DataFrame Ï†ÄÏû• (ÏÑ†ÌÉùÏÇ¨Ìï≠)
    enhanced_path = 'data/train_with_types.csv'
    df_enhanced.to_csv(enhanced_path, index=False)
    print(f"\n‚úì Enhanced DataFrame saved to {enhanced_path}")


if __name__ == "__main__":
    main()
